So I'm curious at a pure architecture level two different applications we see for ASR both often using the same models one is the live text model in which you record and the transcription happens live and usually when you're using APIs these are often actually different endpoints and then you have the I use the term asynchronous although I'm not sure if that's accurate in this case but by that I mean you upload a file much longer recording let's say it could be a one hour zoom meeting or a lengthy dictated note and then the ASR model will infer that my impression has been that there is significantly better performance often when you're just using the second of those two workflows that might just be my imagination or bias but at a sort of pure technical level when the inference is happening. So how does that process look differently under the hood when you're doing live? I understand that there's chunking going on under the hood. So it's actually more like gathering together those little chunks on the go. But if I had two ways of, I could record let's say a 15 minute note in two different ways. One would be speaking, I'm using speech note to record this prompt. One would be just, I speak for 15 minutes. Every 30 seconds it spits out new text, it puts it on the screen. I could also record a 30 minute note in Audacity, save that to an mp3, import that and then transcribe. In those two cases is something different happening with regard to the actual inference that might meaningfully change the accuracy.