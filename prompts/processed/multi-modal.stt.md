Let's say you have your average speech to text transcription app and from what I'm understanding it's going to be using actually a number of different models, kind of in harmony. You have the ASR doing the fundamental transcribing you might have something adding in punctuation You might have voice activity detection and you might even have Wake Word detection so this means I guess that you're what seems like a fairly simple speech to text application you might actually be using four or five different AI models some the ASR model being the big one VAD maybe being a smaller model but my question is when these apps are working how does that whole kind of symphony play together as such how does the app orchestrate the different models such that they don't collide with one another and they follow a predictable sequence from when the user hits start recording through to when they hit save recording and then after that all this kind of magic is expected to just kind of happen in the background does that work on the back end I guess is what I'm asking.