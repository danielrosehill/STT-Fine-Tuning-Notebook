One of the reasons I can think that fine-tuning a ASR model would be useful would be. If you have non-top-of-the-line GPUs on your desktop or on your phone that you can run smaller models with greater accuracy. The trade-off wouldn't be as significant. Let's say from what I'm learning and seeing, it seems to me that even on my desktop I need small is about the biggest I can do to avoid tapping the GPU usage during inference but there's using the stock model which is what I'm using for this prompt transcription, I can definitely see a drop in accuracy. If I were to focus my fine-tuning efforts on actually the smaller models, small and tiny, could I actually bump up the accuracy? Would that be a more sensible strategy for local inference than fine-tuning large, which it seems will actually only be useful to me if I'm running that in the cloud via an API.