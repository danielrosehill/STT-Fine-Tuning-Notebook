So I'm using on my local computer a AMD GPU it is a Radeon 7700 and I think it's eight gigabytes VRAM I think. I was under the impression that I could actually do I'm using a whisper CPP here with the impression that could actually manage all the way up to the large models or the large turbo or at least medium but for the first time I'm looking at a system's monitor when I'm transcribing and I'm watching the GPU and even on medium which is what I am transcribing this voice note with I see this the GPU usage is jumping to 100% whenever the model switches to inference which happens every few seconds because it's whisper so I presume obviously you don't you never want to be maxing out your GPU on but that means the model is too big. I thought that all that's left to try would be small and tiny. But I'm surprised, I thought that... It was easier maybe than it is to run these models. Is there any kind of ballpark for how much VRAM and GPU you really need to get quality results from local STT? I presume if you're even hitting 90% during inference you're not going to be getting the best possible results because that has to also run the displays and so I'm guessing that you need a bigger GPU or a better GPU than I thought to do local STT.