Something i'm curious about in fine tuning is um you have you're taking let's say in the case of whisper. You're taking a very powerful model and you're kind of giving it almost countervailing instructions that this, in this little bit of training data, when I say this word, it should be transcribed as this. Now, there's going to be some overlap in that training data that I say, I use a sentence that might have my custom vocab in it, but it's going to have other normal vocabulary that the model has already learned. So in a sense, you're introducing a conflict that the model has your training data on top of what it knows already in its weights. On an architectural level, under the hood, what's going on when you're doing the fine-tuning that the model is trying to kind of reconcile these divergent, its original training and then what you're adding on top of that? How do you take a small weight and counter it against a big model to get the desired outcome?