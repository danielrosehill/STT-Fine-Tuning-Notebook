So your average um asr model has obviously got some kind of training data that it's based upon um i've seen some of the open source speech data sets that are out there and i'm just curious for when for what let's say whisper base might be trained upon or any of the whisper models how much what kind of quantity we're talking about and where what kind of context did that um transcribing material come from now in the first case to be transcribed i presume when i'm doing my own sort of fine tuning i need to define a source of truth but for the very first of these models that didn't have a previous transcription did someone have to manually review all that training data. To train it and or was this kind of subset trained or correctly annotated and then as the model got progressively better there was actually some machine translation in the training data or is that not possible how would that process work exactly when they were training up the first of these asr models.