<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Speech-to-Text Fine-Tuning Guide - Book 4 - Practical Guide</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="book-style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Speech-to-Text Fine-Tuning Guide - Book 4 - Practical
Guide</h1>
</header>
<h1 id="speech-to-text-fine-tuning-guide">Speech-to-Text Fine-Tuning
Guide</h1>
<h2 id="book-4-practical-guide">Book 4: Practical Guide</h2>
<p><em>Pitfalls, Q&amp;A &amp; Additional Notes</em></p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<p><strong>Part X: Common Pitfalls</strong><br />
Common issues and how to avoid them (3 chapters)</p>
<p><strong>Part XI: Q&amp;A</strong><br />
Frequently asked questions (2 chapters)</p>
<p><strong>Part XII: Additional Notes</strong><br />
Supplementary topics and observations (2 chapters)</p>
<hr />
<h1 id="part-x-common-pitfalls">Part X: Common Pitfalls</h1>
<p><em>Common issues and how to avoid them</em></p>
<hr />
<h2 id="handling-pauses-and-hallucinations">Handling Pauses And
Hallucinations</h2>
<h2 id="the-problem-when-silence-causes-hallucinations">The Problem:
When Silence Causes Hallucinations</h2>
<p>If you’ve used Whisper-based transcription tools while dictating
notes or blog outlines, you’ve likely encountered an annoying
phenomenon: when you pause to think (10-20 seconds), the model sometimes
“hallucinates” and inserts phantom text that you never spoke.</p>
<p><strong>Common hallucinations during silence:</strong> - Repeated
phrases (“Thank you for watching. Thank you for watching.”) - Background
music descriptions (“♪ music playing ♪”) - Generic filler text (“Please
subscribe to my channel”) - Foreign language phrases - Made-up words or
nonsense</p>
<p>This document explains why this happens and how Voice Activity
Detection (VAD) provides a practical solution—without requiring
always-on listening or wake word detection.</p>
<h2 id="why-whisper-hallucinates-during-long-pauses">Why Whisper
Hallucinates During Long Pauses</h2>
<h3 id="the-root-cause-attention-mechanism-behavior">The Root Cause:
Attention Mechanism Behavior</h3>
<p>Whisper (and similar ASR models) uses a transformer architecture with
an attention mechanism. When given long segments of silence:</p>
<ol type="1">
<li><strong>The model expects speech:</strong> Whisper is trained on
audio with speech, not extended silence</li>
<li><strong>Attention seeks patterns:</strong> The attention mechanism
looks for <em>something</em> to focus on</li>
<li><strong>Noise becomes signal:</strong> Background noise, breathing,
ambient sounds get over-interpreted</li>
<li><strong>Decoder generates “plausible” text:</strong> To fulfill its
objective, the model generates text that “could” be there</li>
</ol>
<h3 id="why-long-pauses-are-worse">Why Long Pauses Are Worse</h3>
<p><strong>Short pauses (1-3 seconds):</strong> Generally handled
well—model recognizes natural speech gaps</p>
<p><strong>Medium pauses (5-10 seconds):</strong> Risk zone—model starts
searching for signal in noise</p>
<p><strong>Long pauses (15-30+ seconds):</strong> High hallucination
risk—model “invents” content</p>
<p><strong>The trigger:</strong> It’s not the pause itself, but the
length of silence fed to the model. Whisper processes audio in
~30-second chunks, so a 20-second pause in a 30-second window means 66%
silence—enough to confuse the model.</p>
<h3 id="common-hallucination-patterns">Common Hallucination
Patterns</h3>
<p><strong>1. Training Data Artifacts</strong></p>
<pre><code>&quot;Thank you for watching&quot;
&quot;Please subscribe&quot;
&quot;Don&#39;t forget to like and comment&quot;</code></pre>
<p><em>Why:</em> Whisper was trained on YouTube videos—these phrases are
common in that dataset.</p>
<p><strong>2. Music/Audio Descriptions</strong></p>
<pre><code>&quot;♪ instrumental music ♪&quot;
&quot;[music playing]&quot;
&quot;(upbeat music)&quot;</code></pre>
<p><em>Why:</em> Training data included audio with music; model tries to
describe what it “hears” in noise.</p>
<p><strong>3. Repeated Phrases</strong></p>
<pre><code>&quot;The project timeline. The project timeline. The project timeline.&quot;</code></pre>
<p><em>Why:</em> Attention mechanism gets stuck in a loop when there’s
no new information.</p>
<p><strong>4. Foreign Language Snippets</strong></p>
<pre><code>&quot;Gracias&quot; (Spanish)
&quot;Merci&quot; (French)</code></pre>
<p><em>Why:</em> Multi-lingual training—model sometimes switches
languages to “explain” ambiguous audio.</p>
<h2 id="enter-vad-voice-activity-detection">Enter VAD: Voice Activity
Detection</h2>
<h3 id="what-vad-actually-does">What VAD Actually Does</h3>
<p><strong>Core Function:</strong> VAD detects when speech is present in
audio and when it’s absent.</p>
<p><strong>Key Clarification:</strong> VAD is NOT the same as: -
<strong>Always-on listening</strong> (VAD can be used in push-to-record
apps) - <strong>Wake word detection</strong> (VAD doesn’t trigger on
keywords)</p>
<h3 id="how-vad-solves-the-pause-problem">How VAD Solves the Pause
Problem</h3>
<p><strong>Without VAD (Your Current Experience):</strong></p>
<pre><code>You hit &quot;Record&quot;
    ↓
Audio buffer captures everything (speech + pauses + noise)
    ↓
You hit &quot;Stop&quot;
    ↓
Entire audio (including 20-second pauses) sent to Whisper
    ↓
Whisper tries to transcribe silence → hallucinations</code></pre>
<p><strong>With VAD (Improved Workflow):</strong></p>
<pre><code>You hit &quot;Record&quot;
    ↓
Audio buffer captures everything
    ↓
VAD analyzes audio in real-time or post-recording
    ↓
VAD marks segments: [speech] [silence] [speech] [silence] [speech]
    ↓
Only [speech] segments sent to Whisper
    ↓
Silence is completely removed from what Whisper sees
    ↓
No silence = no hallucinations</code></pre>
<h3 id="vad-in-push-to-record-applications">VAD in Push-to-Record
Applications</h3>
<p>You don’t need always-on listening to benefit from VAD. Here’s how it
works in a typical dictation app:</p>
<p><strong>Use Case 1: Post-Recording VAD Filtering</strong></p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>audio <span class="op">=</span> record_audio()  <span class="co"># Contains speech + 20-second pauses</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>vad <span class="op">=</span> load_vad_model()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>speech_segments <span class="op">=</span> vad.get_speech_timestamps(audio)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>speech_only_audio <span class="op">=</span> extract_segments(audio, speech_segments)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>transcript <span class="op">=</span> whisper_model.transcribe(speech_only_audio)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Use Case 2: Real-time VAD During Recording
(Streaming)</strong></p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>audio_buffer <span class="op">=</span> []</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> audio_chunk <span class="kw">in</span> audio_stream:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># VAD checks each chunk</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> vad.is_speech(audio_chunk):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        audio_buffer.append(audio_chunk)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Silence detected - ignore this chunk</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>transcript <span class="op">=</span> whisper_model.transcribe(audio_buffer)</span></code></pre></div>
<p><strong>Key Point:</strong> In both cases, you still manually control
when recording starts and stops. VAD simply filters out the silent parts
<em>within</em> your recording session.</p>
<h2 id="practical-implementation">Practical Implementation</h2>
<h3 id="solution-1-silero-vad-recommended">Solution 1: Silero VAD
(Recommended)</h3>
<p><strong>Why Silero VAD?</strong> - Lightweight (1.5 MB model) - Fast
(&lt; 5ms per audio chunk) - Highly accurate (&lt; 1% false positive
rate) - Easy to integrate</p>
<p><strong>Installation:</strong></p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchaudio</span></code></pre></div>
<p><strong>Implementation:</strong></p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchaudio</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>model, utils <span class="op">=</span> torch.hub.load(</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    repo_or_dir<span class="op">=</span><span class="st">&#39;snakers4/silero-vad&#39;</span>,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">&#39;silero_vad&#39;</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    force_reload<span class="op">=</span><span class="va">False</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>(get_speech_timestamps, _, read_audio, _, _) <span class="op">=</span> utils</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>audio <span class="op">=</span> read_audio(<span class="st">&#39;your_recording.wav&#39;</span>, sampling_rate<span class="op">=</span><span class="dv">16000</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>speech_timestamps <span class="op">=</span> get_speech_timestamps(</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    audio,</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.5</span>,        <span class="co"># Confidence threshold (0.3-0.7 typical)</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    sampling_rate<span class="op">=</span><span class="dv">16000</span>,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    min_speech_duration_ms<span class="op">=</span><span class="dv">250</span>,  <span class="co"># Ignore very short speech segments</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    min_silence_duration_ms<span class="op">=</span><span class="dv">500</span>  <span class="co"># Minimum silence to trigger segmentation</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>speech_segments <span class="op">=</span> []</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> timestamp <span class="kw">in</span> speech_timestamps:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> timestamp[<span class="st">&#39;start&#39;</span>]</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    end <span class="op">=</span> timestamp[<span class="st">&#39;end&#39;</span>]</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    speech_segments.append(audio[start:end])</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>speech_only <span class="op">=</span> torch.cat(speech_segments)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>torchaudio.save(<span class="st">&#39;speech_only.wav&#39;</span>, speech_only.unsqueeze(<span class="dv">0</span>), <span class="dv">16000</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> whisper</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> whisper.load_model(<span class="st">&quot;base&quot;</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.transcribe(<span class="st">&quot;speech_only.wav&quot;</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">&quot;text&quot;</span>])</span></code></pre></div>
<p><strong>Result:</strong> Your 20-second pauses are completely
removed; Whisper only sees actual speech.</p>
<h3
id="solution-2-whisper-with-vad-pre-filtering-whisper-ctranslate2">Solution
2: Whisper with VAD Pre-filtering (whisper-ctranslate2)</h3>
<p>Some Whisper implementations have VAD built-in:</p>
<p><strong>Installation:</strong></p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install whisper-ctranslate2</span></code></pre></div>
<p><strong>Usage:</strong></p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> whisper_ctranslate2 <span class="im">import</span> Transcribe</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>transcriber <span class="op">=</span> Transcribe(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">&quot;base&quot;</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">&quot;cpu&quot;</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    compute_type<span class="op">=</span><span class="st">&quot;int8&quot;</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    vad_filter<span class="op">=</span><span class="va">True</span>,  <span class="co"># Enable VAD filtering</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    vad_parameters<span class="op">=</span>{</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;threshold&quot;</span>: <span class="fl">0.5</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;min_speech_duration_ms&quot;</span>: <span class="dv">250</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;min_silence_duration_ms&quot;</span>: <span class="dv">2000</span>  <span class="co"># 2 seconds of silence = segment boundary</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> transcriber.transcribe(<span class="st">&quot;your_recording.wav&quot;</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">&quot;text&quot;</span>])</span></code></pre></div>
<p><strong>Advantage:</strong> Single-step process—VAD and transcription
combined.</p>
<h3 id="solution-3-faster-whisper-with-vad">Solution 3: Faster-Whisper
with VAD</h3>
<p><strong>Installation:</strong></p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install faster-whisper</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install silero-vad</span></code></pre></div>
<p><strong>Implementation:</strong></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> faster_whisper <span class="im">import</span> WhisperModel</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>vad_model, utils <span class="op">=</span> torch.hub.load(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    repo_or_dir<span class="op">=</span><span class="st">&#39;snakers4/silero-vad&#39;</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">&#39;silero_vad&#39;</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>get_speech_timestamps <span class="op">=</span> utils[<span class="dv">0</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>read_audio <span class="op">=</span> utils[<span class="dv">2</span>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>audio <span class="op">=</span> read_audio(<span class="st">&#39;your_recording.wav&#39;</span>, sampling_rate<span class="op">=</span><span class="dv">16000</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>speech_timestamps <span class="op">=</span> get_speech_timestamps(</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    audio,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    vad_model,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.5</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>whisper_model <span class="op">=</span> WhisperModel(<span class="st">&quot;base&quot;</span>, device<span class="op">=</span><span class="st">&quot;cpu&quot;</span>, compute_type<span class="op">=</span><span class="st">&quot;int8&quot;</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>full_transcript <span class="op">=</span> []</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> timestamp <span class="kw">in</span> speech_timestamps:</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    start_sample <span class="op">=</span> timestamp[<span class="st">&#39;start&#39;</span>]</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    end_sample <span class="op">=</span> timestamp[<span class="st">&#39;end&#39;</span>]</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert samples to time (for faster-whisper)</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> start_sample <span class="op">/</span> <span class="dv">16000</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> end_sample <span class="op">/</span> <span class="dv">16000</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Transcribe segment (using seek parameter)</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    segments, info <span class="op">=</span> whisper_model.transcribe(</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;your_recording.wav&#39;</span>,</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>        word_timestamps<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        vad_filter<span class="op">=</span><span class="va">False</span>  <span class="co"># We already applied VAD</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> segment <span class="kw">in</span> segments:</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> start_time <span class="op">&lt;=</span> segment.start <span class="op">&lt;=</span> end_time:</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>            full_transcript.append(segment.text)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot; &quot;</span>.join(full_transcript))</span></code></pre></div>
<h2 id="configuration-tuning-vad-for-dictation">Configuration: Tuning
VAD for Dictation</h2>
<h3 id="key-parameters">Key Parameters</h3>
<p><strong>1. Threshold (0.0 - 1.0)</strong> - <strong>Lower
(0.3-0.4):</strong> More sensitive—catches quiet speech, but may include
noise - <strong>Higher (0.6-0.7):</strong> Less sensitive—only clear
speech, but may miss soft speech - <strong>Recommended for
dictation:</strong> 0.5 (balanced)</p>
<p><strong>2. Min Speech Duration (ms)</strong> -
<strong>Purpose:</strong> Ignore very short bursts (likely noise) -
<strong>Too low (&lt; 100ms):</strong> Noise/clicks detected as speech -
<strong>Too high (&gt; 500ms):</strong> Short words/syllables missed -
<strong>Recommended for dictation:</strong> 250ms</p>
<p><strong>3. Min Silence Duration (ms)</strong> -
<strong>Purpose:</strong> Define when a pause is “silence” vs. natural
speech gap - <strong>Lower (100-300ms):</strong> Aggressive
segmentation—splits on brief pauses - <strong>Higher
(1000-2000ms):</strong> Allows longer pauses within same segment -
<strong>Recommended for dictation:</strong> 500-1000ms</p>
<p><strong>For your use case (thinking pauses):</strong></p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>speech_timestamps <span class="op">=</span> get_speech_timestamps(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    audio,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    vad_model,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    threshold<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    min_speech_duration_ms<span class="op">=</span><span class="dv">250</span>,</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    min_silence_duration_ms<span class="op">=</span><span class="dv">1000</span>  <span class="co"># 1 second allows natural pauses</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># But 10-20 second thinking pauses will be filtered out</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<h3 id="testing-your-configuration">Testing Your Configuration</h3>
<p><strong>Validation Script:</strong></p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchaudio</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>model, utils <span class="op">=</span> torch.hub.load(<span class="st">&#39;snakers4/silero-vad&#39;</span>, model<span class="op">=</span><span class="st">&#39;silero_vad&#39;</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>get_speech_timestamps <span class="op">=</span> utils[<span class="dv">0</span>]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>read_audio <span class="op">=</span> utils[<span class="dv">2</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>audio <span class="op">=</span> read_audio(<span class="st">&#39;test_recording.wav&#39;</span>, sampling_rate<span class="op">=</span><span class="dv">16000</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>configs <span class="op">=</span> [</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;threshold&quot;</span>: <span class="fl">0.4</span>, <span class="st">&quot;min_silence_duration_ms&quot;</span>: <span class="dv">500</span>},</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;threshold&quot;</span>: <span class="fl">0.5</span>, <span class="st">&quot;min_silence_duration_ms&quot;</span>: <span class="dv">1000</span>},</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    {<span class="st">&quot;threshold&quot;</span>: <span class="fl">0.6</span>, <span class="st">&quot;min_silence_duration_ms&quot;</span>: <span class="dv">1500</span>},</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> config <span class="kw">in</span> configs:</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Testing: </span><span class="sc">{</span>config<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    speech_timestamps <span class="op">=</span> get_speech_timestamps(</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        audio,</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>        min_speech_duration_ms<span class="op">=</span><span class="dv">250</span>,</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>config</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Analyze results</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    total_speech_time <span class="op">=</span> <span class="bu">sum</span>(</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        (ts[<span class="st">&#39;end&#39;</span>] <span class="op">-</span> ts[<span class="st">&#39;start&#39;</span>]) <span class="op">/</span> <span class="dv">16000</span> <span class="cf">for</span> ts <span class="kw">in</span> speech_timestamps</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    num_segments <span class="op">=</span> <span class="bu">len</span>(speech_timestamps)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;  Segments detected: </span><span class="sc">{</span>num_segments<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;  Total speech time: </span><span class="sc">{</span>total_speech_time<span class="sc">:.2f}</span><span class="ss">s&quot;</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;  First 3 segments:&quot;</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    pprint(speech_timestamps[:<span class="dv">3</span>])</span></code></pre></div>
<p><strong>Run this on a test recording with known pauses to find your
ideal settings.</strong></p>
<h2 id="applications-beyond-always-on-listening">Applications Beyond
Always-On Listening</h2>
<p>You mentioned associating VAD with always-on listening—here’s the
full range of VAD use cases to clarify:</p>
<h3 id="push-to-record-dictation-your-use-case">1. Push-to-Record
Dictation (Your Use Case)</h3>
<ul>
<li><strong>You control:</strong> When recording starts/stops</li>
<li><strong>VAD controls:</strong> Which parts of your recording get
transcribed</li>
<li><strong>Benefit:</strong> Hallucination-free transcripts despite
thinking pauses</li>
</ul>
<h3 id="always-on-listening-virtual-assistants">2. Always-On Listening
(Virtual Assistants)</h3>
<ul>
<li><strong>VAD controls:</strong> When recording starts (speech
detected)</li>
<li><strong>VAD controls:</strong> When recording stops (silence
detected)</li>
<li><strong>You don’t manually trigger anything</strong></li>
</ul>
<h3 id="meetingpodcast-transcription">3. Meeting/Podcast
Transcription</h3>
<ul>
<li><strong>You control:</strong> Load audio file</li>
<li><strong>VAD controls:</strong> Segments sent to ASR (ignores silence
between speakers)</li>
<li><strong>Benefit:</strong> Faster transcription, lower costs</li>
</ul>
<h3 id="real-time-streaming-live-captions">4. Real-time Streaming (Live
Captions)</h3>
<ul>
<li><strong>Audio continuously captured</strong></li>
<li><strong>VAD controls:</strong> When to send chunks to ASR</li>
<li><strong>Benefit:</strong> Lower latency, reduced compute</li>
</ul>
<p><strong>Key Distinction:</strong> VAD is a <em>tool</em> that can be
used in any of these scenarios. It’s not inherently tied to always-on
listening.</p>
<h2 id="alternative-approaches-without-vad">Alternative Approaches
(Without VAD)</h2>
<p>If you can’t or don’t want to use VAD, here are workarounds:</p>
<h3 id="prompt-engineering-limited-effectiveness">1. Prompt Engineering
(Limited Effectiveness)</h3>
<p><strong>Whisper’s <code>initial_prompt</code> parameter:</strong></p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.transcribe(</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;recording.wav&quot;</span>,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    initial_prompt<span class="op">=</span><span class="st">&quot;This is a dictation with natural pauses. Do not add filler text.&quot;</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>Reality:</strong> This helps slightly but doesn’t eliminate
hallucinations during long silence.</p>
<h3 id="temperature-reduction">2. Temperature Reduction</h3>
<p><strong>Lower temperature = less creative (fewer
hallucinations):</strong></p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.transcribe(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;recording.wav&quot;</span>,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.0</span>  <span class="co"># Default is 0.0-1.0</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>Limitation:</strong> Also makes the model less flexible with
accents/vocabulary.</p>
<h3 id="shorter-recording-sessions">3. Shorter Recording Sessions</h3>
<p><strong>Workaround:</strong> Don’t let pauses sit in the recording
buffer. - Manually pause/resume recording during thinking breaks -
Record in shorter bursts (30-60 seconds) - Stitch transcripts together
post-processing</p>
<p><strong>Downside:</strong> Interrupts your workflow; requires manual
management.</p>
<h3 id="post-processing-cleanup">4. Post-Processing Cleanup</h3>
<p><strong>Filter hallucinations with keyword detection:</strong></p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>hallucination_phrases <span class="op">=</span> [</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;thank you for watching&quot;</span>,</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;please subscribe&quot;</span>,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;♪&quot;</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;[music&quot;</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>transcript <span class="op">=</span> result[<span class="st">&quot;text&quot;</span>]</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> phrase <span class="kw">in</span> hallucination_phrases:</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    transcript <span class="op">=</span> transcript.replace(phrase, <span class="st">&quot;&quot;</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(transcript)</span></code></pre></div>
<p><strong>Limitation:</strong> Only catches known hallucinations; won’t
catch all.</p>
<h2 id="recommended-setup-for-dictation">Recommended Setup for
Dictation</h2>
<p><strong>For your specific workflow (blog outlines with thinking
pauses):</strong></p>
<h3 id="option-a-silero-vad-whisper-most-control">Option A: Silero VAD +
Whisper (Most Control)</h3>
<p><strong>Pros:</strong> - Complete control over VAD parameters - Works
with any Whisper backend (faster-whisper, whisper.cpp, etc.) -
Transparent—you can inspect speech segments before transcription</p>
<p><strong>Cons:</strong> - Requires two-step process (VAD → transcribe)
- Slightly more code</p>
<h3 id="option-b-whisper-ctranslate2-with-built-in-vad-easiest">Option
B: Whisper-CTranslate2 with Built-in VAD (Easiest)</h3>
<p><strong>Pros:</strong> - Single command - VAD automatically applied -
Good defaults for dictation</p>
<p><strong>Cons:</strong> - Less control over VAD parameters -
CTranslate2 dependency</p>
<h3 id="option-c-faster-whisper-external-vad-best-performance">Option C:
Faster-Whisper + External VAD (Best Performance)</h3>
<p><strong>Pros:</strong> - Fastest inference (2-4x faster than OpenAI
Whisper) - High-quality VAD with Silero - Good for large volumes of
dictation</p>
<p><strong>Cons:</strong> - More complex setup - GPU recommended for
best speed</p>
<p><strong>Recommendation:</strong> Start with <strong>Option B</strong>
(whisper-ctranslate2) for simplicity. If you need more control, switch
to <strong>Option A</strong> (Silero + Whisper).</p>
<h2 id="real-world-example-before-and-after-vad">Real-World Example:
Before and After VAD</h2>
<h3 id="before-vad-with-hallucinations">Before VAD (With
Hallucinations)</h3>
<p><strong>Your dictation:</strong> &gt; “I want to outline a blog post
about AI transcription tools. (20-second pause thinking) The first
section should cover accuracy metrics.”</p>
<p><strong>Whisper’s transcript (with hallucinations):</strong> &gt; “I
want to outline a blog post about AI transcription tools. Thank you for
watching. Thank you for watching. Please subscribe. The first section
should cover accuracy metrics.”</p>
<h3 id="after-vad-clean">After VAD (Clean)</h3>
<p><strong>VAD detects:</strong> - Speech: 0-5s (“I want to outline…”) -
Silence: 5-25s (pause) - Speech: 25-30s (“The first section…”)</p>
<p><strong>VAD sends to Whisper:</strong> - Segment 1: “I want to
outline…” - Segment 2: “The first section…”</p>
<p><strong>Whisper’s transcript (no hallucinations):</strong> &gt; “I
want to outline a blog post about AI transcription tools. The first
section should cover accuracy metrics.”</p>
<h2 id="performance-impact">Performance Impact</h2>
<p><strong>Overhead of VAD:</strong> - Silero VAD: ~1-5ms per 100ms
audio chunk - For 60 seconds of audio: ~100ms total VAD processing -
<strong>Negligible impact</strong> compared to ASR (which takes
seconds)</p>
<p><strong>Benefit:</strong> - Reduced ASR processing time (only
transcribing speech) - No manual cleanup of hallucinations - Improved
accuracy</p>
<p><strong>Net result:</strong> Faster overall workflow despite extra
VAD step.</p>
<h2 id="conclusion">Conclusion</h2>
<p><strong>The short answer to your question:</strong> Yes, VAD
absolutely solves your pause problem, and no, it doesn’t require
always-on listening.</p>
<p><strong>What VAD does:</strong> - Detects when you’re speaking
vs. pausing - Filters out silent segments before they reach Whisper -
Prevents hallucinations caused by long thinking pauses</p>
<p><strong>How to use it:</strong> 1. Record your dictation as usual
(pauses and all) 2. Apply VAD post-recording to extract speech-only
segments 3. Transcribe speech-only audio with Whisper 4. Get clean
transcripts without phantom text</p>
<p><strong>Recommended starting point:</strong></p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install whisper-ctranslate2</span></code></pre></div>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> whisper_ctranslate2 <span class="im">import</span> Transcribe</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>transcriber <span class="op">=</span> Transcribe(</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    model_path<span class="op">=</span><span class="st">&quot;base&quot;</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    vad_filter<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    vad_parameters<span class="op">=</span>{<span class="st">&quot;min_silence_duration_ms&quot;</span>: <span class="dv">1000</span>}</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> transcriber.transcribe(<span class="st">&quot;your_recording.wav&quot;</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="st">&quot;text&quot;</span>])</span></code></pre></div>
<p><strong>Result:</strong> No more “Thank you for watching”
hallucinations during your coffee-free morning thought pauses.</p>
<hr />
<p><em>This document was generated by Claude Code as part of Daniel
Rosehill’s STT Fine-Tuning Notebook. VAD technology continues to
improve; consult current documentation for the latest models and
parameters.</em></p>
<h2 id="overfitting">Overfitting</h2>
<h2 id="what-is-overfitting">What is Overfitting?</h2>
<p>Overfitting occurs when a machine learning model learns the training
data too well, including its noise and peculiarities, rather than
learning the underlying patterns that generalize to new data. In the
context of STT (Speech-to-Text) fine-tuning, an overfitted model will
perform exceptionally well on training audio but poorly on new, unseen
audio recordings.</p>
<h2 id="signs-of-overfitting">Signs of Overfitting</h2>
<h3 id="training-vs-validation-metrics">Training vs Validation
Metrics</h3>
<ul>
<li><strong>Training loss continues to decrease</strong> while
<strong>validation loss plateaus or increases</strong></li>
<li>High accuracy on training set (&gt;95%) but significantly lower on
validation set</li>
<li>Large gap between training Word Error Rate (WER) and validation
WER</li>
</ul>
<h3 id="behavioral-indicators">Behavioral Indicators</h3>
<ul>
<li>Model memorizes specific phrases from training data</li>
<li>Poor generalization to different speakers, accents, or recording
conditions</li>
<li>Excellent performance on training speakers but degraded performance
on new voices</li>
<li>Model struggles with slight variations in vocabulary or
phrasing</li>
</ul>
<h2 id="common-causes-in-stt-fine-tuning">Common Causes in STT
Fine-Tuning</h2>
<h3 id="insufficient-training-data">1. <strong>Insufficient Training
Data</strong></h3>
<ul>
<li>Small datasets (&lt; 10 hours of audio) increase overfitting
risk</li>
<li>Limited speaker diversity in training set</li>
<li>Narrow range of acoustic conditions</li>
</ul>
<h3 id="too-many-training-epochs">2. <strong>Too Many Training
Epochs</strong></h3>
<ul>
<li>Training for too long allows model to memorize training
examples</li>
<li>Optimal number varies by dataset size and model capacity</li>
</ul>
<h3 id="model-complexity-vs-data-size">3. <strong>Model Complexity vs
Data Size</strong></h3>
<ul>
<li>Large models (like Whisper Large) require more data to avoid
overfitting</li>
<li>Small datasets better suited to smaller models (Whisper
Small/Base)</li>
</ul>
<h3 id="lack-of-data-augmentation">4. <strong>Lack of Data
Augmentation</strong></h3>
<ul>
<li>No acoustic variation (speed, pitch, noise)</li>
<li>Missing diversity in recording conditions</li>
</ul>
<h3 id="improper-regularization">5. <strong>Improper
Regularization</strong></h3>
<ul>
<li>Dropout rates too low or disabled</li>
<li>No weight decay applied</li>
<li>Learning rate too high</li>
</ul>
<h2 id="prevention-strategies">Prevention Strategies</h2>
<h3 id="data-level-solutions">Data-Level Solutions</h3>
<h4 id="increase-dataset-size">Increase Dataset Size</h4>
<ul>
<li>Aim for minimum 20-30 hours of diverse audio</li>
<li>Include multiple speakers (10+ different voices)</li>
<li>Vary recording conditions and environments</li>
</ul>
<h4 id="data-augmentation">Data Augmentation</h4>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Speed perturbation (<span class="fl">0.9</span><span class="er">x</span> <span class="op">-</span> <span class="fl">1.1</span><span class="er">x</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Pitch shifting</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Background noise injection</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Room impulse response simulation</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Volume normalization <span class="kw">and</span> variation</span></code></pre></div>
<h4 id="proper-data-split">Proper Data Split</h4>
<ul>
<li><strong>Training</strong>: 80% of data</li>
<li><strong>Validation</strong>: 10% (for monitoring during
training)</li>
<li><strong>Test</strong>: 10% (for final evaluation)</li>
<li>Ensure speaker diversity across all splits</li>
</ul>
<h3 id="model-configuration">Model Configuration</h3>
<h4 id="choose-appropriate-model-size">Choose Appropriate Model
Size</h4>
<ul>
<li><strong>Small datasets (5-20 hours)</strong>: Whisper Tiny or
Base</li>
<li><strong>Medium datasets (20-100 hours)</strong>: Whisper Small or
Medium</li>
<li><strong>Large datasets (100+ hours)</strong>: Whisper Medium or
Large</li>
</ul>
<h4 id="regularization-techniques">Regularization Techniques</h4>
<p><strong>Dropout</strong></p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>dropout: <span class="fl">0.1</span> <span class="op">-</span> <span class="fl">0.3</span>  <span class="co"># Higher for smaller datasets</span></span></code></pre></div>
<p><strong>Weight Decay</strong></p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>weight_decay: <span class="fl">0.01</span> <span class="op">-</span> <span class="fl">0.1</span></span></code></pre></div>
<p><strong>Gradient Clipping</strong></p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>max_grad_norm: <span class="fl">1.0</span>  <span class="co"># Prevents exploding gradients</span></span></code></pre></div>
<h3 id="training-strategies">Training Strategies</h3>
<h4 id="early-stopping">Early Stopping</h4>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>early_stopping_patience: <span class="dv">3</span><span class="op">-</span><span class="dv">5</span> epochs</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>monitor: <span class="st">&quot;eval_loss&quot;</span></span></code></pre></div>
<h4 id="learning-rate-scheduling">Learning Rate Scheduling</h4>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>lr_scheduler_type: <span class="st">&quot;cosine&quot;</span>  <span class="co"># or &quot;linear&quot;</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>warmup_steps: <span class="dv">500</span></span></code></pre></div>
<h4 id="regular-validation">Regular Validation</h4>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>eval_steps: <span class="dv">500</span>  <span class="co"># Check every 500 steps</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>save_total_limit: <span class="dv">3</span>  <span class="co"># Keep only best 3 checkpoints</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>load_best_model_at_end: <span class="va">True</span></span></code></pre></div>
<h2 id="monitoring-during-training">Monitoring During Training</h2>
<h3 id="key-metrics-to-track">Key Metrics to Track</h3>
<ol type="1">
<li><strong>Loss Curves</strong>
<ul>
<li>Plot training loss and validation loss together</li>
<li>Divergence indicates overfitting</li>
</ul></li>
<li><strong>Word Error Rate (WER)</strong>
<ul>
<li>Calculate on both training and validation sets</li>
<li>Gap &gt; 10-15% suggests overfitting</li>
</ul></li>
<li><strong>Character Error Rate (CER)</strong>
<ul>
<li>More granular metric than WER</li>
<li>Useful for detecting subtle overfitting</li>
</ul></li>
</ol>
<h3 id="visualization-example">Visualization Example</h3>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>plt.plot(train_losses, label<span class="op">=</span><span class="st">&#39;Training Loss&#39;</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>plt.plot(val_losses, label<span class="op">=</span><span class="st">&#39;Validation Loss&#39;</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Training vs Validation Loss&#39;</span>)</span></code></pre></div>
<h2 id="recovery-strategies">Recovery Strategies</h2>
<p>If overfitting is detected during training:</p>
<h3 id="rollback-to-earlier-checkpoint">1. <strong>Rollback to Earlier
Checkpoint</strong></h3>
<ul>
<li>Use checkpoint from before validation loss started increasing</li>
<li>Resume training with adjusted hyperparameters</li>
</ul>
<h3 id="reduce-model-complexity">2. <strong>Reduce Model
Complexity</strong></h3>
<ul>
<li>Switch to smaller model variant</li>
<li>Freeze more layers (only fine-tune final layers)</li>
</ul>
<h3 id="adjust-learning-rate">3. <strong>Adjust Learning
Rate</strong></h3>
<ul>
<li>Lower learning rate by 50-75%</li>
<li>Implement more aggressive learning rate decay</li>
</ul>
<h3 id="increase-regularization">4. <strong>Increase
Regularization</strong></h3>
<ul>
<li>Higher dropout rates</li>
<li>Stronger weight decay</li>
<li>Add more data augmentation</li>
</ul>
<h3 id="add-more-data">5. <strong>Add More Data</strong></h3>
<ul>
<li>Collect additional training samples</li>
<li>Synthesize data if appropriate</li>
<li>Use transfer learning from related domains</li>
</ul>
<h2 id="best-practices-summary">Best Practices Summary</h2>
<ol type="1">
<li><strong>Always split data</strong> into train/validation/test
sets</li>
<li><strong>Monitor both metrics</strong> (training and validation)
throughout training</li>
<li><strong>Use early stopping</strong> to prevent excessive
training</li>
<li><strong>Start small</strong>: Begin with fewer epochs and smaller
models</li>
<li><strong>Validate regularly</strong>: Check performance every few
hundred steps</li>
<li><strong>Keep best checkpoint</strong>: Save model with best
validation performance</li>
<li><strong>Document experiments</strong>: Track hyperparameters and
results</li>
<li><strong>Test on unseen data</strong>: Final evaluation on completely
separate test set</li>
</ol>
<h2 id="trade-offs">Trade-offs</h2>
<ul>
<li><strong>Underfitting vs Overfitting</strong>: Finding the sweet spot
requires experimentation</li>
<li><strong>Training time vs performance</strong>: More epochs isn’t
always better</li>
<li><strong>Model size vs dataset size</strong>: Bigger models need more
data</li>
<li><strong>Generalization vs specialization</strong>: Domain-specific
models may overfit on general speech</li>
</ul>
<h2 id="conclusion-1">Conclusion</h2>
<p>Overfitting is one of the most common challenges in STT fine-tuning.
The key is balanced training with proper regularization, sufficient
diverse data, and careful monitoring of validation metrics. When in
doubt, prefer a model that generalizes well over one that perfectly
memorizes the training set.</p>
<h2 id="repetition-bug-mobile-inference">Repetition Bug Mobile
Inference</h2>
<h2 id="the-problem">The Problem</h2>
<p>When converting fine-tuned Whisper models to GGUF format for use on
mobile devices (specifically FUTO Voice Input), some models—particularly
smaller ones like Whisper Tiny—exhibit a repetition bug where the model
enters an infinite loop, repeating the same transcribed text 20-30 times
instead of stopping after completing the transcription.</p>
<p><strong>Example behavior:</strong> - Input: “I’m going to the shop” -
Expected output: “I’m going to the shop” - Actual output: “I’m going to
the shop I’m going to the shop I’m going to the shop…” (repeating 20-30
times)</p>
<h2 id="what-this-indicates">What This Indicates</h2>
<p>This repetition behavior suggests several possible issues:</p>
<h3 id="end-of-sequence-eos-token-problems">1. <strong>End-of-Sequence
(EOS) Token Problems</strong></h3>
<p>The most likely cause is that the model’s EOS (end-of-sequence) token
mechanism is not functioning correctly:</p>
<ul>
<li><strong>During fine-tuning:</strong> If the training data didn’t
properly include or reinforce EOS token behavior, the model may not have
learned when to stop generating output</li>
<li><strong>During conversion:</strong> The GGUF conversion process may
have incorrectly mapped or lost the EOS token information</li>
<li><strong>During inference:</strong> The mobile inference engine may
not be properly detecting or respecting the EOS token</li>
</ul>
<h3 id="quantization-issues">2. <strong>Quantization
Issues</strong></h3>
<p>Converting to GGUF typically involves quantization (reducing
precision from FP32/FP16 to INT8 or INT4):</p>
<ul>
<li><strong>Threshold sensitivity:</strong> The stopping criteria in
Whisper models rely on probability thresholds. Quantization can alter
these probabilities enough that the stopping condition is never met</li>
<li><strong>Smaller models more affected:</strong> Whisper Tiny has
fewer parameters and less capacity to handle quantization-induced errors
compared to larger variants</li>
<li><strong>Critical parameters affected:</strong> The specific weights
controlling sequence termination may be disproportionately affected by
quantization</li>
</ul>
<h3 id="context-window-or-attention-issues">3. <strong>Context Window or
Attention Issues</strong></h3>
<p>The conversion or mobile inference may have issues with:</p>
<ul>
<li><strong>Max length parameter:</strong> The maximum generation length
may be set incorrectly or ignored</li>
<li><strong>Attention mask:</strong> Problems with the attention
mechanism could cause the model to lose track of what it has already
generated</li>
<li><strong>Memory state:</strong> Issues with the model’s internal
state tracking between chunks</li>
</ul>
<h3 id="fine-tuning-artifacts">4. <strong>Fine-Tuning
Artifacts</strong></h3>
<p>The fine-tuning process itself may have introduced problems:</p>
<ul>
<li><strong>Insufficient training steps:</strong> The model may not have
converged properly during fine-tuning</li>
<li><strong>Learning rate issues:</strong> Too high a learning rate
could have destabilized the model’s stopping behavior</li>
<li><strong>Data imbalance:</strong> If the training data had unusual
characteristics (very short or very long samples), the model may have
learned incorrect stopping patterns</li>
</ul>
<h2 id="diagnostic-steps">Diagnostic Steps</h2>
<p>To narrow down the cause:</p>
<ol type="1">
<li><p><strong>Test the pre-conversion model:</strong> Use the
fine-tuned model on desktop before GGUF conversion. If it works there
but not on mobile, the issue is in conversion/mobile inference</p></li>
<li><p><strong>Test different quantization levels:</strong> Try
converting with different quantization settings (Q8_0 vs Q4_0 vs Q5_1)
to see if precision loss is the culprit</p></li>
<li><p><strong>Test with different model sizes:</strong> If only Tiny
exhibits this behavior, quantization sensitivity is likely the
issue</p></li>
<li><p><strong>Inspect the conversion logs:</strong> Look for warnings
or errors during GGUF conversion, particularly around special
tokens</p></li>
<li><p><strong>Compare tokenizer outputs:</strong> Verify that the
tokenizer is correctly handling special tokens (especially
<code>&lt;|endoftext|&gt;</code>) in both desktop and mobile
environments</p></li>
</ol>
<h2 id="solutions-and-workarounds">Solutions and Workarounds</h2>
<h3 id="short-term-fixes">Short-term fixes:</h3>
<ol type="1">
<li><p><strong>Use a larger model variant:</strong> Try Whisper Base or
Small instead of Tiny—they handle quantization better</p></li>
<li><p><strong>Use higher quantization precision:</strong> If storage
allows, use Q8_0 instead of Q4_0 quantization</p></li>
<li><p><strong>Implement external stopping:</strong> Add inference-time
maximum token limits or timeout mechanisms in the mobile app</p></li>
</ol>
<h3 id="long-term-fixes">Long-term fixes:</h3>
<ol type="1">
<li><p><strong>Improve fine-tuning:</strong> Ensure training data
includes proper sequence boundaries and the model is trained to
convergence</p></li>
<li><p><strong>Add EOS reinforcement:</strong> During fine-tuning, you
can add additional training emphasis on EOS token behavior</p></li>
<li><p><strong>Test conversion tools:</strong> Different GGUF conversion
tools (llama.cpp, ct2-transformers-converter, etc.) may handle the
conversion differently</p></li>
<li><p><strong>Report to FUTO:</strong> This may be a bug in FUTO’s
inference engine that needs fixing</p></li>
</ol>
<h2 id="prevention-in-future-fine-tuning">Prevention in Future
Fine-Tuning</h2>
<p>To avoid this issue in future fine-tuning projects:</p>
<ol type="1">
<li><p><strong>Validate before conversion:</strong> Always test
fine-tuned models thoroughly on desktop before converting to mobile
formats</p></li>
<li><p><strong>Include diverse audio lengths:</strong> Ensure training
data has samples of various lengths to teach proper stopping
behavior</p></li>
<li><p><strong>Monitor validation metrics:</strong> Watch for unusual
patterns in validation that might indicate stopping behavior
issues</p></li>
<li><p><strong>Test multiple model sizes:</strong> Fine-tune both Tiny
and Base variants to ensure the approach works across model
sizes</p></li>
<li><p><strong>Document conversion parameters:</strong> Keep detailed
records of conversion settings so you can iterate if problems
occur</p></li>
</ol>
<h2 id="additional-context">Additional Context</h2>
<ul>
<li><strong>Desktop inference success:</strong> The fact that the model
worked correctly on desktop indicates the fine-tuning itself was likely
successful</li>
<li><strong>Inference was happening:</strong> The model was correctly
transcribing the initial phrase, showing that the core model weights
were intact</li>
<li><strong>Model-specific behavior:</strong> The issue affecting Tiny
but potentially not other sizes points to quantization sensitivity</li>
</ul>
<p>This type of bug is frustrating but common when deploying fine-tuned
models to resource-constrained environments. The good news is that
inference was occurring correctly—the issue is specifically with
sequence termination, which is usually fixable through conversion
parameter adjustments or using slightly larger model variants.</p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
<h1 id="part-xi-qa">Part XI: Q&amp;A</h1>
<p><em>Frequently asked questions</em></p>
<hr />
<h2 id="hardware-stt-keyboard-device">Hardware Stt Keyboard Device</h2>
<h2 id="question-summary">Question Summary</h2>
<p>Daniel proposes an idea for a dedicated hardware device to solve a
specific problem: on Ubuntu Linux with Wayland, voice typing apps often
fail not because of transcription quality, but because of Wayland’s
restrictive security around virtual keyboard input. His idea: a
mini-computer that runs on-device STT and presents itself as a
USB/Bluetooth keyboard to the host computer, bypassing the virtual input
restrictions. This would also allow using a powerful desktop’s STT model
across multiple computers. He asks: Does such a device exist, or should
he build a prototype?</p>
<h2 id="answer">Answer</h2>
<p>This is a brilliant idea that addresses a real problem, particularly
for Linux/Wayland users! Let me break down whether this exists, what’s
close, and whether you should build it.</p>
<h3 id="the-short-answer">The Short Answer</h3>
<p><strong>Products that exist but aren’t quite this:</strong> - Some
voice-to-text USB devices exist, but they’re mostly proprietary closed
systems - No dedicated “STT-as-keyboard” device with modern models
(Whisper, etc.) exists commercially - DIY solutions exist but aren’t
productized</p>
<p><strong>Should you build it?</strong> - <strong>For personal
use:</strong> Absolutely! It’s a fun, achievable project - <strong>As a
product:</strong> Maybe - there’s a niche market but limited -
<strong>Difficulty:</strong> Medium (Raspberry Pi + Whisper + USB HID =
doable)</p>
<p>Let’s explore this in detail.</p>
<h3 id="the-problem-youre-solving">The Problem You’re Solving</h3>
<p><strong>Wayland Security Model:</strong></p>
<pre><code>Issue:
- Wayland doesn&#39;t allow apps to inject keyboard input globally (by design)
- Security feature (prevents keyloggers, input injection attacks)
- Breaks virtual keyboard functionality

Traditional Workarounds:
1. X11 compatibility layer (defeats Wayland security)
2. Accessibility APIs (permission complexity)
3. DE-specific solutions (KDE, GNOME differ)

All are fragile, permission-heavy, or limited.

Your Solution:
- Hardware keyboard = Wayland trusts it implicitly
- No virtual input permissions needed
- Works across any Wayland compositor
- Bonus: Portable across computers!</code></pre>
<h3 id="existing-products-close-but-not-quite">Existing Products (Close
But Not Quite)</h3>
<h4 id="dedicated-voice-recorders-with-transcription"><strong>1.
Dedicated Voice Recorders with Transcription</strong></h4>
<p><strong>Plaud Note, Otter AI Recorder (discontinued),
etc.</strong></p>
<pre><code>What They Do:
- Record audio locally
- Transcribe (usually cloud-based)
- Sync transcripts to app

What They DON&#39;T Do:
- Present as keyboard
- Real-time input to computer
- On-device STT (most use cloud APIs)

Verdict: Not a solution for your use case</code></pre>
<h4 id="voice-typing-dongles-rare-mostly-discontinued"><strong>2. Voice
Typing Dongles (Rare, Mostly Discontinued)</strong></h4>
<p><strong>Nuance PowerMic, SpeechMike</strong></p>
<pre><code>What They Are:
- USB microphones with built-in controls
- Designed for medical dictation
- Work with Dragon NaturallySpeaking

What They DON&#39;T Do:
- Don&#39;t run STT themselves (require host software)
- Not keyboard devices
- Proprietary, expensive ($300-500)

Verdict: Requires host software (same Wayland problem)</code></pre>
<h4 id="bluetooth-voice-to-text-devices-obscure"><strong>3. Bluetooth
Voice-to-Text Devices (Obscure)</strong></h4>
<p><strong>Stenomask, VoiceItt</strong></p>
<pre><code>VoiceItt (now &quot;Talkitt&quot;):
- Bluetooth device for speech input
- Designed for accessibility (speech impairments)
- Translates non-standard speech to text
- Presents as Bluetooth keyboard (on some platforms)

Limitations:
- Focused on accessibility, not general STT
- Proprietary, limited model
- Expensive (~$200-300)
- Not running Whisper or custom models

Verdict: Closest existing product, but not customizable</code></pre>
<h3 id="diy-projects-that-exist">DIY Projects That Exist</h3>
<h4 id="raspberry-pi-voice-typing-keyboards"><strong>Raspberry Pi Voice
Typing Keyboards</strong></h4>
<p><strong>Community Projects (GitHub):</strong></p>
<pre><code>Several developers have built similar prototypes:

1. &quot;whisper-keyboard&quot; (GitHub search)
   - Raspberry Pi Zero W / Pi 4
   - Runs Whisper (tiny/base models)
   - USB HID keyboard emulation
   - Status: Proof-of-concept, not polished

2. &quot;STT-HID-device&quot;
   - Uses Vosk ASR (lighter than Whisper)
   - Pi Zero can handle it
   - Bluetooth or USB-C connection

3. Custom solutions in forums (r/raspberry_pi, r/speechrecognition)
   - Various implementations
   - Mostly one-offs, not documented well</code></pre>
<p><strong>None are productized or turnkey.</strong></p>
<h3 id="your-device-specification-feasibility">Your Device:
Specification &amp; Feasibility</h3>
<p><strong>Proposed Device Concept:</strong></p>
<pre><code>Hardware:
- Raspberry Pi 4 (4GB+ RAM) for Whisper-small/medium
- OR: Raspberry Pi 5 (8GB) for Whisper-large (with optimization)
- OR: Alternative: Orange Pi 5 (16GB, more powerful)
- Microphone: USB mic or Pi-compatible mic (Seeed ReSpeaker)
- Case: 3D printed or off-the-shelf

Software:
- Raspbian/Ubuntu on Pi
- Whisper (faster-whisper for speed)
- USB Gadget mode (Pi presents as USB keyboard)
- OR: Bluetooth HID mode

Features:
- Physical button to trigger STT
- LED indicator (listening, processing, done)
- Optional: Small display (status, recognition preview)
- Battery-powered option (for portability)</code></pre>
<p><strong>Connection Modes:</strong></p>
<pre><code>Option 1: USB-C (USB HID Keyboard)
- Pi Zero W / Pi 4 with USB OTG cable
- Presents as USB keyboard to host
- Host sees: &quot;USB Keyboard (Raspberry Pi)&quot;
- Works with any OS (Linux, Windows, Mac, even Android)

Option 2: Bluetooth (Bluetooth HID)
- Pair as Bluetooth keyboard
- Wireless, portable
- Works across multiple devices (switch pairing)

Option 3: Hybrid (USB charging, Bluetooth operation)
- Best of both worlds</code></pre>
<h3 id="building-it-step-by-step">Building It: Step-by-Step</h3>
<p><strong>Phase 1: Proof of Concept (Weekend Project)</strong></p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Hardware:</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Raspberry Pi 4 <span class="er">(</span><span class="ex">4GB</span><span class="kw">)</span><span class="bu">:</span> <span class="va">$5</span>5</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> USB microphone: <span class="va">$1</span>5-30</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> MicroSD card <span class="er">(</span><span class="ex">64GB</span><span class="kw">)</span><span class="bu">:</span> <span class="va">$1</span>0</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> USB-C cable: <span class="va">$5</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Total:</span> ~<span class="va">$8</span>5-100</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Software</span> Stack:</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> Install Raspbian Lite <span class="er">(</span><span class="ex">headless</span><span class="kw">)</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Install faster-whisper:</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>   <span class="ex">pip</span> install faster-whisper</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> USB HID Setup:</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Enable USB gadget mode (Pi presents as keyboard)</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>   <span class="bu">echo</span> <span class="st">&quot;dtoverlay=dwc2&quot;</span> <span class="op">&gt;&gt;</span> /boot/config.txt</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>   <span class="bu">echo</span> <span class="st">&quot;dwc2&quot;</span> <span class="op">&gt;&gt;</span> /etc/modules</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>   <span class="bu">echo</span> <span class="st">&quot;libcomposite&quot;</span> <span class="op">&gt;&gt;</span> /etc/modules</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="ex">4.</span> HID Keyboard Script:</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Python script to send keystrokes via /dev/hidg0</span></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>   <span class="co"># (Emulate USB keyboard)</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="ex">5.</span> Trigger:</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>   <span class="co"># GPIO button to start/stop recording</span></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Record audio → Whisper → Send as keystrokes</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a><span class="ex">Time:</span> 4-8 hours for basic prototype</span></code></pre></div>
<p><strong>Phase 2: Refinement (1-2 Weekends)</strong></p>
<pre><code>Improvements:
1. Better microphone (noise cancellation)
2. LED feedback (recording, processing, done)
3. Wake word detection (hands-free triggering)
4. Battery power (USB power bank or LiPo battery)
5. 3D printed case

Time: 10-20 hours
Cost: +$30-50 (battery, LEDs, case materials)</code></pre>
<p><strong>Phase 3: Polish (Optional)</strong></p>
<pre><code>Nice-to-Haves:
1. Small OLED display (show recognized text)
2. Multi-device Bluetooth pairing
3. Model selection (switch between Whisper-tiny/small/medium)
4. Language switching
5. Custom wake words
6. Integration with fine-tuned models

Time: 20-40 hours
Cost: +$20-40 (display, connectors, etc.)</code></pre>
<h3 id="technical-challenges-solutions">Technical Challenges &amp;
Solutions</h3>
<p><strong>Challenge 1: Whisper Speed on Pi</strong></p>
<pre><code>Problem:
- Whisper-large is too slow on Raspberry Pi (10-30 seconds per utterance)
- Not suitable for real-time typing

Solutions:
1. Use faster-whisper (optimized, 4-5x faster)
2. Use Whisper-tiny or Whisper-small (near real-time on Pi 4)
3. Use alternative models:
   - Vosk (much faster, lower accuracy)
   - Whisper.cpp (C++ port, faster)
4. Upgrade to Pi 5 or Orange Pi 5 (more powerful)
5. Use external GPU stick (Intel Neural Compute Stick, Google Coral)

Realistic Expectation:
- Whisper-small on Pi 4: ~1-2 seconds per 5-second utterance (acceptable)
- Whisper-medium on Pi 5: ~2-3 seconds per 5-second utterance</code></pre>
<p><strong>Challenge 2: USB HID Keyboard Emulation</strong></p>
<pre><code>Problem:
- Linux USB Gadget mode requires specific Pi models (Pi Zero W, Pi 4 with USB-C)
- Correct configuration tricky

Solution:
- Use CircuitPython libraries (Adafruit HID)
- OR: Use /dev/hidg0 device (ConfigFS USB Gadget)
- Well-documented in Pi community

Example (Python):
import usb_hid
from adafruit_hid.keyboard import Keyboard

keyboard = Keyboard(usb_hid.devices)
keyboard.send(Keycode.H, Keycode.E, Keycode.L, Keycode.L, Keycode.O)


Verdict: Solvable with existing libraries</code></pre>
<p><strong>Challenge 3: Audio Quality &amp; Latency</strong></p>
<pre><code>Problem:
- USB microphone latency
- Background noise
- VAD (Voice Activity Detection) for start/stop

Solution:
- Use VAD to detect speech start/end (Silero VAD, WebRTC VAD)
- Noise suppression (RNNoise, built into some mics)
- Good microphone choice (directional, noise-cancelling)

Recommended Mics:
- Seeed ReSpeaker 2-Mic Hat ($30, fits on Pi GPIO)
- Blue Snowball Ice ($50, USB, excellent quality)
- Samson Go Mic ($40, portable, good quality)</code></pre>
<p><strong>Challenge 4: Power Consumption</strong></p>
<pre><code>Problem:
- Pi 4 draws 3-5W (need decent battery for portability)

Solutions:
1. Pi Zero W (lower power, ~1W) with Vosk or Whisper-tiny
2. External power bank (20,000mAh = 8-10 hours Pi 4 runtime)
3. Efficient model (Whisper-tiny/small, not large)

Portability:
- If USB-tethered to laptop: No battery needed
- If standalone: Battery adds bulk but doable</code></pre>
<h3 id="use-cases-where-this-shines">Use Cases Where This Shines</h3>
<p><strong>1. Wayland/Linux Users (Your Case)</strong></p>
<pre><code>- Bypass virtual keyboard restrictions
- Works across all Wayland compositors
- No permission hassles
- Truly &quot;just works&quot;</code></pre>
<p><strong>2. Multi-Computer Setup</strong></p>
<pre><code>- STT on powerful desktop (Whisper-large)
- Use output on laptop (via Bluetooth/USB)
- One device, multiple clients</code></pre>
<p><strong>3. Privacy-Focused Users</strong></p>
<pre><code>- 100% on-device transcription
- No cloud APIs
- No internet required
- Air-gapped if needed</code></pre>
<p><strong>4. Accessibility</strong></p>
<pre><code>- Physical keyboard bypass for motor impairments
- Portable dictation device
- Works with any computer (even locked-down systems)</code></pre>
<p><strong>5. Field Work / Mobile</strong></p>
<pre><code>- Dictate notes into any device
- Works with tablets, smartphones (Bluetooth keyboard mode)
- Ruggedized enclosure for outdoor use</code></pre>
<h3 id="market-potential-if-you-wanted-to-sell-it">Market Potential (If
You Wanted to Sell It)</h3>
<p><strong>Target Audience:</strong></p>
<pre><code>1. Linux power users (Wayland users especially): Small but passionate
2. Privacy advocates: Growing market
3. Accessibility users: Significant, underserved
4. Field workers (medical, legal, research): Existing market (currently use Dragon)

Market Size: Niche (thousands, not millions)
Price Point: $150-300 (based on components + assembly + margin)

Competition:
- High-end: Nuance PowerMic ($300-500) - but requires software
- Low-end: DIY (free, but technical barrier)
- Your device: Middle ground (plug-and-play, customizable)

Challenges:
- Small market (hard to scale)
- Support burden (different OSes, configurations)
- Certification (FCC, CE for commercial product)

Opportunity:
- Kickstarter potential (tech enthusiast crowd)
- Open-source community could contribute
- Accessibility market underserved</code></pre>
<h3 id="should-you-build-it">Should You Build It?</h3>
<p><strong>For Personal Use: Absolutely Yes</strong></p>
<pre><code>Reasons:
✓ Solves your real problem (Wayland input)
✓ Achievable in a weekend (basic version)
✓ Components are affordable ($100-150)
✓ Learning experience (USB HID, ASR deployment)
✓ Customizable (fine-tuned models, your vocabulary)
✓ Portable (use on multiple machines)

Downsides:
✗ Not as polished as commercial product
✗ Some tinkering required
✗ Limited to quality of Pi-runnable models

Verdict: Go for it! Great weekend project.</code></pre>
<p><strong>As a Commercial Product: Maybe</strong></p>
<pre><code>Reasons to Consider:
✓ Real problem (Wayland, privacy, portability)
✓ No direct competition in this exact form
✓ Could be open-source hardware (community support)
✓ Accessibility angle (grant funding potential)

Reasons to Hesitate:
✗ Small market (niche)
✗ Support burden (many OSes, configurations)
✗ Manufacturing costs (hard to compete with DIY)
✗ Cloud ASR is &quot;good enough&quot; for most users

Verdict: Build prototype, gauge interest, maybe Kickstarter</code></pre>
<h3 id="recommended-approach">Recommended Approach</h3>
<p><strong>Step 1: Build Minimal Prototype (This Weekend)</strong></p>
<div class="sourceCode" id="cb52"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Shopping</span> List:</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Raspberry Pi 4 <span class="er">(</span><span class="ex">4GB</span><span class="kw">)</span> <span class="ex">or</span> Pi 5</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> USB microphone <span class="er">(</span><span class="ex">any</span> decent one<span class="kw">)</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> MicroSD card</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> GPIO button + LED</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Breadboard and wires</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="ex">Goal:</span> Get basic USB keyboard emulation working with Whisper</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Success</span> Criteria:</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Press button</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Speak into mic</span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Text appears on host computer <span class="er">(</span><span class="fu">as</span> if typed<span class="kw">)</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Works on your Ubuntu Wayland system</span></code></pre></div>
<p><strong>Step 2: Refine Based on Use (Next Weekend)</strong></p>
<pre><code>Improvements:
- Better trigger (wake word instead of button?)
- Faster model (faster-whisper, Whisper-small)
- Battery power (if you want portability)
- Better case (3D print or project box)</code></pre>
<p><strong>Step 3: Decide on Next Steps</strong></p>
<pre><code>Option A: Keep it personal
- Use it daily
- Share on GitHub
- Help others build their own

Option B: Gauge interest
- Post on r/raspberry_pi, r/speechrecognition
- Write blog post / YouTube video
- If traction: Consider productizing

Option C: Open-source hardware project
- Design for reproducibility
- Document thoroughly
- Community collaboration (someone might fund/manufacture)</code></pre>
<h3 id="similar-projects-to-reference">Similar Projects to
Reference</h3>
<p><strong>GitHub searches:</strong></p>
<pre><code>- &quot;raspberry pi whisper keyboard&quot;
- &quot;STT USB HID&quot;
- &quot;voice typing pi&quot;
- &quot;speech recognition keyboard emulation&quot;

Expect: 5-10 similar projects, mostly proof-of-concept
Use: Learn from their USB HID implementations, microphone choices</code></pre>
<p><strong>Forums:</strong></p>
<pre><code>- r/raspberry_pi (search &quot;voice typing&quot;)
- Raspberry Pi Forums (speech recognition projects)
- Hackaday (voice-controlled projects)</code></pre>
<h3 id="my-recommendation">My Recommendation</h3>
<p><strong>Build it!</strong> Here’s why:</p>
<ol type="1">
<li><strong>Solves your real problem</strong> - Wayland virtual input is
genuinely annoying</li>
<li><strong>Achievable</strong> - Weekend project for basic version</li>
<li><strong>Affordable</strong> - ~$100 in parts</li>
<li><strong>Educational</strong> - Learn USB HID, on-device ASR
deployment</li>
<li><strong>Useful</strong> - Even if imperfect, better than current
workarounds</li>
<li><strong>Shareable</strong> - If it works, others will want it
(GitHub repo, blog post)</li>
</ol>
<p><strong>Don’t over-engineer initially:</strong> - Start with
Whisper-tiny (fast enough for Pi 4) - USB-tethered first (skip battery
complexity) - Simple button trigger (add wake word later) - Basic case
(project box, not custom 3D print)</p>
<p><strong>If it works well for you:</strong> - Document it thoroughly -
Share on GitHub - Gauge community interest - Decide on next steps
(personal tool vs. product)</p>
<h3 id="conclusion-2">Conclusion</h3>
<p><strong>Does it exist commercially?</strong> Not really - closest is
VoiceItt, but it’s proprietary and limited.</p>
<p><strong>Should you build it?</strong> Yes! It’s a practical,
achievable project that solves a real problem (especially for
Linux/Wayland users).</p>
<p><strong>Difficulty:</strong> Medium - requires some Linux knowledge,
hardware tinkering, but nothing exotic.</p>
<p><strong>Timeline:</strong> Basic prototype in a weekend, polished
version in 2-4 weekends.</p>
<p><strong>Cost:</strong> $100-150 for full setup (can go cheaper with
Pi Zero + Vosk).</p>
<p>This is exactly the kind of project the maker/hacker community loves:
practical, open-source-friendly, solves a niche problem elegantly. Even
if you don’t turn it into a product, you’ll solve your Wayland problem
and probably help dozens of others along the way. Go for it!</p>
<hr />
<p><em>Note: This response was generated by Claude Code as part of
Daniel’s STT Fine-Tuning Notebook project. Hardware specifications and
project suggestions are based on current Raspberry Pi capabilities and
open-source ASR models.</em></p>
<h2 id="punctuation-and-paragraph-spacing-in-asr">Punctuation And
Paragraph Spacing In Asr</h2>
<h2 id="question">Question</h2>
<p>Does Whisper support punctuation natively? Why don’t ASR models
provide paragraph breaks, and are there complementary tools for
paragraph spacing (similar to punctuation restoration models)?</p>
<h2 id="answer-1">Answer</h2>
<p>Great observations! You’ve identified a real gap in ASR
functionality. Let’s break this down.</p>
<hr />
<h2 id="part-1-does-whisper-support-punctuation">Part 1: Does Whisper
Support Punctuation?</h2>
<p><strong>Short answer: Yes, Whisper DOES support punctuation
natively.</strong></p>
<h3 id="why-whisper-has-punctuation-unlike-older-models"><strong>Why
Whisper Has Punctuation (Unlike Older Models)</strong></h3>
<p><strong>Traditional ASR (Wav2Vec2, Kaldi, etc.):</strong> - Used CTC
(Connectionist Temporal Classification) decoding - CTC produces
<strong>character sequences only</strong> (no punctuation) - Required
separate punctuation restoration models</p>
<p><strong>Whisper (Encoder-Decoder Transformer):</strong> - Trained on
<strong>transcripts with punctuation</strong> from subtitles, captions,
etc. - Generates text <strong>autoregressively</strong> (like a language
model) - Learns punctuation as part of the transcription task - Produces
punctuated output naturally</p>
<p><strong>Example:</strong></p>
<pre><code>Audio: &quot;I went to the store but it was closed&quot;

Wav2Vec2 output: &quot;i went to the store but it was closed&quot;
Whisper output: &quot;I went to the store, but it was closed.&quot;</code></pre>
<h3
id="why-speechnote-uses-a-complementary-punctuation-model"><strong>Why
SpeechNote Uses a Complementary Punctuation Model</strong></h3>
<p>There are several possible reasons:</p>
<ol type="1">
<li><p><strong>SpeechNote might support multiple backends</strong>: If
it supports Wav2Vec2 or other models (not just Whisper), it needs a
punctuation restoration fallback.</p></li>
<li><p><strong>Whisper’s punctuation isn’t perfect</strong>: While good,
Whisper can miss commas, semicolons, or use incorrect punctuation. A
dedicated punctuation model can improve accuracy.</p></li>
<li><p><strong>Customization</strong>: Separate punctuation models allow
users to choose different punctuation styles (formal vs. casual, for
example).</p></li>
<li><p><strong>Streaming mode</strong>: Some ASR implementations do
streaming transcription where punctuation is added in
post-processing.</p></li>
</ol>
<p><strong>Bottom line:</strong> With stock Whisper, you get
punctuation—but it’s not always perfect, hence complementary models
exist to refine it.</p>
<hr />
<h2 id="part-2-why-dont-asr-models-support-paragraph-breaks">Part 2: Why
Don’t ASR Models Support Paragraph Breaks?</h2>
<p>This is the more interesting question. <strong>You’re absolutely
right—this is a huge usability gap.</strong></p>
<h3 id="the-core-problem"><strong>The Core Problem</strong></h3>
<p>Paragraph breaks require understanding: 1. <strong>Topic
shifts</strong>: When the speaker changes subjects 2. <strong>Logical
grouping</strong>: Sentences that belong together conceptually 3.
<strong>Discourse structure</strong>: Introduction → body → conclusion
4. <strong>Rhetorical boundaries</strong>: “Now, moving on to…” signals
a break</p>
<p><strong>These are higher-level semantic tasks</strong> that go beyond
what ASR models were traditionally designed for.</p>
<h3 id="why-whisper-doesnt-do-paragraph-breaks"><strong>Why Whisper
Doesn’t Do Paragraph Breaks</strong></h3>
<h4 id="training-data-limitations"><strong>Training Data
Limitations</strong></h4>
<p>Whisper was trained on: - <strong>Subtitles</strong>: Segmented by
time, not logical paragraphs - <strong>Short audio clips</strong>: Most
training samples are &lt;30 seconds - <strong>Flat text</strong>: No
markdown formatting or paragraph structure</p>
<p><strong>Example training data:</strong></p>
<pre><code>[00:00-00:05] &quot;Welcome to today&#39;s lecture on machine learning.&quot;
[00:05-00:10] &quot;We&#39;ll cover three main topics.&quot;
[00:10-00:15] &quot;First, neural networks.&quot;</code></pre>
<p>This teaches Whisper to transcribe and punctuate, but <strong>not
where to insert paragraph breaks</strong> because the training data
doesn’t contain that information.</p>
<h4 id="task-scope"><strong>Task Scope</strong></h4>
<p>Whisper’s objective is: &gt; Audio → Text (transcription + basic
formatting)</p>
<p>Paragraph segmentation is: &gt; Text → Structured Text (discourse
analysis)</p>
<p>These are <strong>different tasks</strong> requiring different
training objectives.</p>
<h4 id="ambiguity"><strong>Ambiguity</strong></h4>
<p>Unlike punctuation (which has audio cues like pauses, intonation),
paragraph breaks are often <strong>subjective</strong>:</p>
<pre><code>Speaker: &quot;I woke up early. I made coffee. I checked my email. Then I started work.&quot;

Could be:
Version A (one paragraph):
I woke up early. I made coffee. I checked my email. Then I started work.

Version B (two paragraphs):
I woke up early. I made coffee. I checked my email.

Then I started work.

Version C (four paragraphs):
I woke up early.

I made coffee.

I checked my email.

Then I started work.</code></pre>
<p><strong>There’s no single “correct” answer</strong>—it depends on
context, audience, and purpose.</p>
<hr />
<h2
id="part-3-why-isnt-there-a-complementary-paragraph-spacing-tool">Part
3: Why Isn’t There a Complementary Paragraph Spacing Tool?</h2>
<p><strong>Great question. The short answer: There are, but they’re not
widely packaged for consumer use.</strong></p>
<h3 id="existing-research-models"><strong>Existing Research &amp;
Models</strong></h3>
<p>Paragraph segmentation (also called “discourse segmentation” or “text
segmentation”) is an active NLP research area:</p>
<p><strong>Academic Models:</strong> - <strong>TextTiling</strong>
(Hearst, 1997): Classic algorithm for topic-based segmentation -
<strong>SECTOR</strong> (Arnold et al., 2019): Neural model for section
segmentation - <strong>Longformer</strong> / <strong>BigBird</strong>:
Long-context transformers used for discourse parsing -
<strong>Sentence-BERT</strong> variants: Used for semantic similarity to
detect topic shifts</p>
<p><strong>Commercial Tools:</strong> - Some meeting transcription
services (Otter.ai, Fireflies) attempt paragraph breaks - Document AI
services (Google, AWS) have text structuring capabilities - Enterprise
ASR platforms (Deepgram, AssemblyAI) are starting to add this</p>
<h3 id="why-not-widely-available"><strong>Why Not Widely
Available?</strong></h3>
<h4 id="complexity">1. <strong>Complexity</strong></h4>
<p>Unlike punctuation (which has clear rules), paragraph segmentation
requires: - Topic modeling - Coreference resolution - Discourse relation
detection - Context understanding</p>
<p><strong>This is significantly harder than punctuation
restoration.</strong></p>
<h4 id="domain-dependence">2. <strong>Domain Dependence</strong></h4>
<p>Good paragraph breaks depend on <strong>genre</strong>: - News
article: Topic-based breaks - Email: Greeting → body → closing - Essay:
Introduction → paragraphs → conclusion - Meeting notes: Speaker turns or
topic shifts</p>
<p>A single model would need to handle all these contexts.</p>
<h4 id="lack-of-training-data">3. <strong>Lack of Training
Data</strong></h4>
<p>Punctuation restoration models were trained on: - Text with
punctuation removed → predict punctuation</p>
<p>But for paragraphs, you need: - <strong>Transcribed speech</strong> →
<strong>paragraph-structured text</strong></p>
<p>This data is rare because: - Most transcription datasets don’t
include paragraph breaks - Paragraph breaks are often added manually by
humans - There’s no standardized format</p>
<h4 id="lower-commercial-priority">4. <strong>Lower Commercial
Priority</strong></h4>
<p>Most ASR users: - Use transcription for
<strong>search/analysis</strong> (structure doesn’t matter) - Manually
edit for <strong>publication</strong> (accept paragraph breaks as
editing step)</p>
<p>So there’s been less commercial pressure to solve this.</p>
<hr />
<h2 id="part-4-solutions-workarounds">Part 4: Solutions &amp;
Workarounds</h2>
<p>Despite the lack of out-of-box tools, there are approaches:</p>
<h3
id="approach-1-post-processing-with-language-models"><strong>Approach 1:
Post-Processing with Language Models</strong></h3>
<p>Modern LLMs (ChatGPT, Claude, etc.) can add paragraph breaks:</p>
<p><strong>Workflow:</strong></p>
<pre><code>1. Get Whisper transcription (no paragraphs)
2. Send to LLM with prompt: &quot;Add paragraph breaks for readability&quot;
3. LLM returns structured text</code></pre>
<p><strong>Pros:</strong> - Works well (LLMs understand discourse
structure) - Can specify style (formal email, casual blog, etc.)</p>
<p><strong>Cons:</strong> - Requires API calls (cost, latency) - Not
integrated into SpeechNote-like apps</p>
<p><strong>Example prompt:</strong></p>
<pre><code>Add appropriate paragraph breaks to this transcription for use as a professional email:

[paste wall-of-text transcription]

Maintain all original text, only add paragraph breaks.</code></pre>
<h3 id="approach-2-rule-based-heuristics"><strong>Approach 2: Rule-Based
Heuristics</strong></h3>
<p>You can implement simple rules:</p>
<p><strong>Heuristic Examples:</strong> - Break on long pauses (&gt;2
seconds) - Break on discourse markers (“Now,” “However,”
“Additionally,”) - Break on speaker turns (if multi-speaker) - Break on
topic shift keywords</p>
<p><strong>Implementation:</strong></p>
<div class="sourceCode" id="cb62"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_paragraph_breaks(text, pause_markers<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple heuristic paragraph breaker</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Break on discourse markers</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>    discourse_markers <span class="op">=</span> [</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;now&#39;</span>, <span class="st">&#39;however&#39;</span>, <span class="st">&#39;additionally&#39;</span>, <span class="st">&#39;furthermore&#39;</span>,</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;on the other hand&#39;</span>, <span class="st">&#39;in conclusion&#39;</span>, <span class="st">&#39;first&#39;</span>,</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">&#39;second&#39;</span>, <span class="st">&#39;third&#39;</span>, <span class="st">&#39;finally&#39;</span></span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Break on long pauses (if available from ASR timestamps)</span></span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pause_markers:</span>
<span id="cb62-16"><a href="#cb62-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Insert breaks at pause locations</span></span>
<span id="cb62-17"><a href="#cb62-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb62-18"><a href="#cb62-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-19"><a href="#cb62-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Break every N sentences (fallback)</span></span>
<span id="cb62-20"><a href="#cb62-20" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> re.split(<span class="vs">r&#39;(?&lt;=[.!?])\s+&#39;</span>, text)</span>
<span id="cb62-21"><a href="#cb62-21" aria-hidden="true" tabindex="-1"></a>    paragraphs <span class="op">=</span> []</span>
<span id="cb62-22"><a href="#cb62-22" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> []</span>
<span id="cb62-23"><a href="#cb62-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-24"><a href="#cb62-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, sent <span class="kw">in</span> <span class="bu">enumerate</span>(sentences):</span>
<span id="cb62-25"><a href="#cb62-25" aria-hidden="true" tabindex="-1"></a>        current.append(sent)</span>
<span id="cb62-26"><a href="#cb62-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for discourse markers</span></span>
<span id="cb62-27"><a href="#cb62-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">any</span>(sent.lower().startswith(marker) <span class="cf">for</span> marker <span class="kw">in</span> discourse_markers):</span>
<span id="cb62-28"><a href="#cb62-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(current) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb62-29"><a href="#cb62-29" aria-hidden="true" tabindex="-1"></a>                paragraphs.append(<span class="st">&#39; &#39;</span>.join(current[:<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb62-30"><a href="#cb62-30" aria-hidden="true" tabindex="-1"></a>                current <span class="op">=</span> [sent]</span>
<span id="cb62-31"><a href="#cb62-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Break every 3-5 sentences</span></span>
<span id="cb62-32"><a href="#cb62-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">len</span>(current) <span class="op">&gt;=</span> <span class="dv">4</span>:</span>
<span id="cb62-33"><a href="#cb62-33" aria-hidden="true" tabindex="-1"></a>            paragraphs.append(<span class="st">&#39; &#39;</span>.join(current))</span>
<span id="cb62-34"><a href="#cb62-34" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> []</span>
<span id="cb62-35"><a href="#cb62-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-36"><a href="#cb62-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb62-37"><a href="#cb62-37" aria-hidden="true" tabindex="-1"></a>        paragraphs.append(<span class="st">&#39; &#39;</span>.join(current))</span>
<span id="cb62-38"><a href="#cb62-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-39"><a href="#cb62-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>.join(paragraphs)</span></code></pre></div>
<p><strong>Pros:</strong> - Fast, no API needed - Can integrate into
SpeechNote-like apps</p>
<p><strong>Cons:</strong> - Crude (not semantically aware) - Won’t work
for all contexts</p>
<h3
id="approach-3-semantic-similarity-texttiling-style"><strong>Approach 3:
Semantic Similarity (TextTiling-style)</strong></h3>
<p>Use embeddings to detect topic shifts:</p>
<p><strong>Concept:</strong></p>
<div class="sourceCode" id="cb63"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">&#39;all-MiniLM-L6-v2&#39;</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> semantic_paragraph_breaks(text, threshold<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Break paragraphs based on semantic similarity</span></span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> text.split(<span class="st">&#39;. &#39;</span>)</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> model.encode(sentences)</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a>    paragraphs <span class="op">=</span> []</span>
<span id="cb63-14"><a href="#cb63-14" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> [sentences[<span class="dv">0</span>]]</span>
<span id="cb63-15"><a href="#cb63-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-16"><a href="#cb63-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(sentences)):</span>
<span id="cb63-17"><a href="#cb63-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compare similarity to previous sentence</span></span>
<span id="cb63-18"><a href="#cb63-18" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> np.dot(embeddings[i], embeddings[i<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb63-19"><a href="#cb63-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-20"><a href="#cb63-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> similarity <span class="op">&lt;</span> threshold:  <span class="co"># Topic shift detected</span></span>
<span id="cb63-21"><a href="#cb63-21" aria-hidden="true" tabindex="-1"></a>            paragraphs.append(<span class="st">&#39;. &#39;</span>.join(current) <span class="op">+</span> <span class="st">&#39;.&#39;</span>)</span>
<span id="cb63-22"><a href="#cb63-22" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> [sentences[i]]</span>
<span id="cb63-23"><a href="#cb63-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb63-24"><a href="#cb63-24" aria-hidden="true" tabindex="-1"></a>            current.append(sentences[i])</span>
<span id="cb63-25"><a href="#cb63-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-26"><a href="#cb63-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb63-27"><a href="#cb63-27" aria-hidden="true" tabindex="-1"></a>        paragraphs.append(<span class="st">&#39;. &#39;</span>.join(current) <span class="op">+</span> <span class="st">&#39;.&#39;</span>)</span>
<span id="cb63-28"><a href="#cb63-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-29"><a href="#cb63-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39;</span><span class="ch">\n\n</span><span class="st">&#39;</span>.join(paragraphs)</span></code></pre></div>
<p><strong>Pros:</strong> - Semantically aware - Better than pure
heuristics</p>
<p><strong>Cons:</strong> - Requires additional model (embeddings) -
Threshold tuning needed</p>
<h3
id="approach-4-fine-tune-a-paragraph-segmentation-model"><strong>Approach
4: Fine-Tune a Paragraph Segmentation Model</strong></h3>
<p>You could fine-tune a model specifically for this:</p>
<p><strong>Data Collection:</strong> 1. Take transcribed
speeches/lectures with paragraph-formatted transcripts 2. Create
training pairs: (flat text, paragraph-structured text) 3. Fine-tune a
seq2seq model (T5, BART) or classification model (BERT)</p>
<p><strong>Task Formulation (Classification):</strong></p>
<pre><code>Input: [sent1] [SEP] [sent2]
Output: 1 (insert break) or 0 (no break)</code></pre>
<p><strong>Pros:</strong> - Can be highly accurate for your use case -
Customizable to your paragraph style</p>
<p><strong>Cons:</strong> - Requires training data - Significant
effort</p>
<hr />
<h2 id="part-5-why-this-matters-and-youre-right-to-care">Part 5: Why
This Matters (And You’re Right to Care)</h2>
<p>Your observation about email usability is spot-on:</p>
<p><strong>Problem:</strong></p>
<pre><code>[Wall-of-text email from ASR]
Hi John I wanted to follow up on our meeting yesterday I thought it went really well and I&#39;m excited about the project I&#39;ve put together a brief summary of the key points we discussed first we agreed to move forward with option B for the deployment strategy second we&#39;ll need to schedule a kickoff meeting with the engineering team by next Friday and third I&#39;ll send you the revised timeline by end of week let me know if you have any questions or if there&#39;s anything else you need from me thanks Daniel</code></pre>
<p><strong>Desired Output:</strong></p>
<pre><code>Hi John,

I wanted to follow up on our meeting yesterday. I thought it went really well and I&#39;m excited about the project. I&#39;ve put together a brief summary of the key points we discussed:

First, we agreed to move forward with option B for the deployment strategy. Second, we&#39;ll need to schedule a kickoff meeting with the engineering team by next Friday. And third, I&#39;ll send you the revised timeline by end of week.

Let me know if you have any questions or if there&#39;s anything else you need from me.

Thanks,
Daniel</code></pre>
<p><strong>The difference is dramatic for usability.</strong></p>
<hr />
<h2 id="practical-recommendations-for-your-workflow">Practical
Recommendations for Your Workflow</h2>
<p>Given your setup (SpeechNote on Linux):</p>
<h3 id="option-1-quick-llm-post-processing-recommended"><strong>Option
1: Quick LLM Post-Processing (Recommended)</strong></h3>
<ol type="1">
<li>Transcribe with SpeechNote (Whisper)</li>
<li>Copy output</li>
<li>Paste into Claude/ChatGPT with: “Add paragraph breaks for email
formatting”</li>
<li>Use result</li>
</ol>
<p><strong>Time cost: 10-20 seconds</strong> <strong>Accuracy: Very
high</strong></p>
<h3 id="option-2-script-a-local-solution"><strong>Option 2: Script a
Local Solution</strong></h3>
<p>Create a simple Python script:</p>
<div class="sourceCode" id="cb67"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/usr/bin/env python3</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> sys.stdin.read()</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> semantic_paragraph_breaks(text)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(result)</span></code></pre></div>
<p><strong>Usage:</strong></p>
<div class="sourceCode" id="cb68"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="ex">speechnote_output.txt</span> <span class="kw">|</span> <span class="ex">python</span> add_paragraphs.py</span></code></pre></div>
<p><strong>Pros</strong>: Local, private, fast <strong>Cons</strong>:
Requires setup, not as good as LLMs</p>
<h3 id="option-3-feature-request-to-speechnote"><strong>Option 3:
Feature Request to SpeechNote</strong></h3>
<p>The SpeechNote developers could add this as a feature: -
Post-processing module for paragraph breaks - Could integrate with local
LLM (Ollama) or cloud API - Configurable (email style vs. essay
style)</p>
<p><strong>Consider opening a feature request on their
GitHub/forum.</strong></p>
<hr />
<h2 id="future-outlook">Future Outlook</h2>
<p><strong>This will improve:</strong></p>
<ol type="1">
<li><p><strong>Longer-context models</strong>: As ASR models get longer
context windows (Whisper can only handle 30s at a time), they’ll learn
discourse structure better.</p></li>
<li><p><strong>Multitask training</strong>: Future ASR models may be
trained on tasks like:</p>
<ul>
<li>Transcription</li>
<li>Punctuation</li>
<li><strong>Paragraph segmentation</strong></li>
<li>Summarization</li>
</ul></li>
<li><p><strong>LLM integration</strong>: As local LLMs become faster,
integrating them into ASR pipelines will become practical.</p></li>
</ol>
<p><strong>Within 1-2 years, expect:</strong> - ASR models with built-in
paragraph segmentation - SpeechNote-like apps with one-click “format for
email” buttons - Fine-tunable paragraph style preferences</p>
<hr />
<h2 id="summary">Summary</h2>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 40%" />
<col style="width: 12%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Whisper Support</th>
<th>Why</th>
<th>Solution</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Punctuation</strong></td>
<td>✅ Yes (native)</td>
<td>Trained on punctuated text</td>
<td>Built-in, or use complementary model for refinement</td>
</tr>
<tr class="even">
<td><strong>Paragraph Breaks</strong></td>
<td>❌ No</td>
<td>Training data lacks structure, task complexity</td>
<td>Post-process with LLM or custom script</td>
</tr>
</tbody>
</table>
<p><strong>Your use case (email formatting) is exactly where the ASR
ecosystem falls short today.</strong></p>
<p>The good news: Post-processing with LLMs (Claude, ChatGPT) works
excellently and takes seconds. The bad news: It’s not integrated into
tools like SpeechNote yet.</p>
<p><strong>Recommended workflow:</strong></p>
<pre><code>Audio → SpeechNote (Whisper) → LLM post-processing → Formatted text</code></pre>
<p>Until ASR models natively support paragraph breaks, this hybrid
approach is your best bet.</p>
<hr />
<p><strong>Note</strong>: This explanation was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Paragraph segmentation is an active research area—expect improvements as
models evolve. For production workflows, combining ASR with LLM
post-processing currently provides the best results for structured text
output.</p>
<h1 id="part-xii-additional-notes">Part XII: Additional Notes</h1>
<p><em>Supplementary topics and observations</em></p>
<hr />
<h2 id="audio-llm-guided-transcription">Audio Llm Guided
Transcription</h2>
<h2 id="question-1">Question</h2>
<p>What is the technology called when you can provide audio + text
prompts (like Gemini) for guided transcription (e.g., “diarize this as A
and B”)? Are there open-source, fine-tunable models that combine ASR
with LLM-style prompt following for audio tasks?</p>
<h2 id="answer-2">Answer</h2>
<p>Excellent question! You’ve identified an emerging and powerful
paradigm. Let’s break this down.</p>
<hr />
<h2 id="what-is-this-technology-called">What Is This Technology
Called?</h2>
<p><strong>The general term is: “Audio-Language Models” (Audio-LLMs) or
“Speech Large Language Models” (Speech-LLMs).</strong></p>
<p>More specifically, when combining: - <strong>Audio input</strong>
(speech, environmental sounds) - <strong>Text prompts</strong>
(instructions, context) - <strong>Text output</strong> (transcription,
diarization, summary, etc.)</p>
<p>…this is often called:</p>
<h3 id="instruction-following-speech-models"><strong>1.
“Instruction-Following Speech Models”</strong></h3>
<p>Models trained to follow text instructions about audio
processing.</p>
<h3 id="audio-conditioned-language-models"><strong>2. “Audio-Conditioned
Language Models”</strong></h3>
<p>LLMs that take audio as input alongside text prompts.</p>
<h3 id="multimodal-audio-text-models"><strong>3. “Multimodal Audio-Text
Models”</strong></h3>
<p>Models that jointly understand audio and text modalities.</p>
<h3 id="prompt-guided-transcription"><strong>4. “Prompt-Guided
Transcription”</strong></h3>
<p>Transcription steered by natural language instructions (your use
case).</p>
<p><strong>Gemini’s audio capability is an example of #2: an
audio-conditioned multimodal LLM.</strong></p>
<p><strong>There’s no single universally-accepted name yet</strong> (the
field is young), but “Audio-Language Models” (Audio-LLMs) is gaining
traction.</p>
<hr />
<h2 id="how-gemini-works-vs.-whisper">How Gemini Works
(vs. Whisper)</h2>
<h3 id="whisper-pure-asr"><strong>Whisper: Pure ASR</strong></h3>
<p><strong>Architecture:</strong></p>
<pre><code>Audio → Encoder → Decoder → Transcription</code></pre>
<p><strong>Capabilities:</strong> - Transcribe audio to text - Detect
language - Add timestamps - (That’s it—no customization beyond model
parameters)</p>
<p><strong>Limitations:</strong> - Can’t follow instructions - Can’t do
speaker diarization - Can’t format output (e.g., “format as Q&amp;A”) -
Can’t incorporate context (e.g., “this is a medical call”)</p>
<hr />
<h3
id="gemini-audio-llm-multimodal-instruction-following"><strong>Gemini
(Audio-LLM): Multimodal Instruction-Following</strong></h3>
<p><strong>Architecture:</strong></p>
<pre><code>Audio → Audio Encoder → Multimodal Transformer (LLM) ← Text Prompt
                                  ↓
                            Text Output</code></pre>
<p><strong>Capabilities:</strong> - Transcribe audio - <strong>Follow
text instructions</strong> (“diarize as A and B”, “summarize this call”)
- <strong>Context-aware</strong> (“this is a phone call between a doctor
and patient”) - <strong>Output formatting</strong> (“format as JSON”,
“use markdown”) - <strong>Reasoning</strong> (“identify the main
complaint”, “what was decided?”)</p>
<p><strong>Key Difference:</strong> Gemini treats audio as
<strong>another input modality to an LLM</strong>, not as a standalone
ASR task.</p>
<p><strong>What Enables This:</strong> 1. <strong>Audio encoder</strong>
converts audio → embeddings (like text tokens) 2. <strong>LLM</strong>
processes both audio embeddings + text prompt together 3.
<strong>Decoder</strong> generates text output following
instructions</p>
<p><strong>Example:</strong></p>
<pre><code>Input (Audio): [30s phone call recording]
Input (Text Prompt): &quot;Transcribe this call. The participants are Alice (caller) and Bob (support agent). Format as Q&amp;A.&quot;

Output:
Q (Alice): Hi, I&#39;m having trouble with my account.
A (Bob): Sure, I can help with that. What&#39;s the issue?
Q (Alice): I can&#39;t log in.
...</code></pre>
<p><strong>Whisper cannot do this</strong> (it would just transcribe
everything without structure or speaker labels).</p>
<hr />
<h2 id="open-source-models-with-audio-llm-capabilities">Open-Source
Models with Audio-LLM Capabilities</h2>
<p><strong>Good news: This field is exploding in 2023-2024.</strong>
Here are the major open-source options:</p>
<hr />
<h3 id="qwen-audio-alibaba-recommended"><strong>1. Qwen-Audio (Alibaba)
⭐ Recommended</strong></h3>
<p><strong>What it is:</strong> - Large-scale audio-language pretrained
model - Understands 30+ audio tasks (ASR, diarization, audio captioning,
etc.) - Follows natural language instructions - <strong>Open-source and
fine-tunable</strong></p>
<p><strong>Hugging Face:</strong> <a
href="https://huggingface.co/Qwen/Qwen-Audio">https://huggingface.co/Qwen/Qwen-Audio</a></p>
<p><strong>Paper:</strong> “Qwen-Audio: Advancing Universal Audio
Understanding via Unified Large-Scale Audio-Language Models” (Nov
2023)</p>
<p><strong>Capabilities:</strong></p>
<pre><code>Prompt: &quot;Transcribe this audio and identify the speakers.&quot;
Prompt: &quot;Summarize the main points of this meeting.&quot;
Prompt: &quot;What sounds do you hear in this audio?&quot;
Prompt: &quot;Translate this Spanish speech to English.&quot;</code></pre>
<p><strong>Architecture:</strong> - Audio encoder (Whisper-like) - Qwen
LLM (7B or 13B parameters) - Multimodal adapter</p>
<p><strong>Fine-tuning:</strong></p>
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">&quot;Qwen/Qwen-Audio&quot;</span>)</span></code></pre></div>
<p><strong>Why this is good for you:</strong> - Open-source (Apache 2.0
license) - Fine-tunable - Supports custom instructions - Active
development</p>
<hr />
<h3 id="speechgpt-fudan-university"><strong>2. SpeechGPT (Fudan
University)</strong></h3>
<p><strong>What it is:</strong> - Enables LLMs to process speech
directly - Can follow instructions for transcription, diarization, etc.
- Uses discrete audio tokens</p>
<p><strong>Hugging Face:</strong> <a
href="https://huggingface.co/fnlp/SpeechGPT">https://huggingface.co/fnlp/SpeechGPT</a></p>
<p><strong>Paper:</strong> “SpeechGPT: Empowering Large Language Models
with Intrinsic Cross-Modal Conversational Abilities” (May 2023)</p>
<p><strong>Architecture:</strong></p>
<pre><code>Audio → HuBERT encoder → Discrete tokens → LLM → Text output</code></pre>
<p><strong>Use Case:</strong> - Conversational speech understanding -
Instruction-following transcription</p>
<p><strong>Limitation:</strong> - Smaller scale than Qwen-Audio - Less
mature ecosystem</p>
<hr />
<h3 id="whisper-llm-pipeline-diy-approach"><strong>3. Whisper + LLM
Pipeline (DIY Approach)</strong></h3>
<p><strong>What it is:</strong> - Combine Whisper (ASR) with an LLM
(Llama, Mistral, etc.) in a pipeline - Whisper transcribes, LLM
processes instructions</p>
<p><strong>Architecture:</strong></p>
<pre><code>Audio → Whisper → Raw transcription → LLM → Formatted output</code></pre>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> faster_whisper <span class="im">import</span> WhisperModel</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>whisper <span class="op">=</span> WhisperModel(<span class="st">&quot;medium&quot;</span>)</span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>segments, info <span class="op">=</span> whisper.transcribe(<span class="st">&quot;audio.wav&quot;</span>)</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>raw_transcription <span class="op">=</span> <span class="st">&quot; &quot;</span>.join([seg.text <span class="cf">for</span> seg <span class="kw">in</span> segments])</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> pipeline(<span class="st">&quot;text-generation&quot;</span>, model<span class="op">=</span><span class="st">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span>)</span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a><span class="ss">You are a transcription assistant.</span></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a><span class="ss">Audio transcription:</span></span>
<span id="cb77-16"><a href="#cb77-16" aria-hidden="true" tabindex="-1"></a><span class="sc">{</span>raw_transcription<span class="sc">}</span></span>
<span id="cb77-17"><a href="#cb77-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-18"><a href="#cb77-18" aria-hidden="true" tabindex="-1"></a><span class="ss">Instructions: This is a phone call between Alice (caller) and Bob (agent).</span></span>
<span id="cb77-19"><a href="#cb77-19" aria-hidden="true" tabindex="-1"></a><span class="ss">Diarize the transcription and format as Q&amp;A.</span></span>
<span id="cb77-20"><a href="#cb77-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-21"><a href="#cb77-21" aria-hidden="true" tabindex="-1"></a><span class="ss">Output:</span></span>
<span id="cb77-22"><a href="#cb77-22" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span></span>
<span id="cb77-23"><a href="#cb77-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-24"><a href="#cb77-24" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> llm(prompt, max_new_tokens<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb77-25"><a href="#cb77-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>][<span class="st">&quot;generated_text&quot;</span>])</span></code></pre></div>
<p><strong>Pros:</strong> - ✅ Works today (no waiting for models) - ✅
Highly customizable (swap components) - ✅ Can use your fine-tuned
Whisper</p>
<p><strong>Cons:</strong> - ❌ Two-stage (not end-to-end) - ❌ Slower
(two inference passes) - ❌ Whisper doesn’t “know” about instructions
during transcription</p>
<p><strong>This is a practical workaround until unified models
mature.</strong></p>
<hr />
<h3 id="ltu-listening-and-talking-understanding-models"><strong>4. LTU
(Listening-and-Talking Understanding) Models</strong></h3>
<p><strong>What it is:</strong> - Recent research on unified speech-text
models - Examples: SALMONN, LLaSM, etc.</p>
<p><strong>SALMONN (ByteDance):</strong> <a
href="https://github.com/bytedance/SALMONN">https://github.com/bytedance/SALMONN</a></p>
<p><strong>Paper:</strong> “SALMONN: Towards Generic Hearing Abilities
for Large Language Models” (Oct 2023)</p>
<p><strong>Capabilities:</strong> - Speech recognition - Audio
captioning (describe sounds) - Speech emotion recognition - Music
understanding - Instruction-following</p>
<p><strong>Status:</strong> - Research code (less production-ready than
Qwen-Audio) - Demonstrates feasibility of unified audio-LLMs</p>
<hr />
<h3 id="gemini-style-open-alternatives-future"><strong>5. Gemini-Style
Open Alternatives (Future)</strong></h3>
<p><strong>What’s coming:</strong> - <strong>OpenAI Whisper v4</strong>
(rumored to have instruction-following) - <strong>Meta’s SeamlessM4T
v3</strong> (multimodal, may add instructions) - <strong>Google’s
USM-v2</strong> (Universal Speech Model, not yet released)</p>
<p><strong>Current state:</strong> Gemini’s audio capabilities are
proprietary—no direct open-source equivalent yet.</p>
<hr />
<h2 id="comparison-table">Comparison Table</h2>
<table>
<colgroup>
<col style="width: 9%" />
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 29%" />
<col style="width: 13%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Open-Source</th>
<th>Fine-Tunable</th>
<th>Instruction-Following</th>
<th>Maturity</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Qwen-Audio</strong></td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>High</td>
<td>Production use, fine-tuning</td>
</tr>
<tr class="even">
<td><strong>SpeechGPT</strong></td>
<td>✅</td>
<td>✅</td>
<td>✅</td>
<td>Medium</td>
<td>Research, experimentation</td>
</tr>
<tr class="odd">
<td><strong>Whisper + LLM</strong></td>
<td>✅</td>
<td>✅ (separately)</td>
<td>✅</td>
<td>High</td>
<td>Immediate practical use</td>
</tr>
<tr class="even">
<td><strong>SALMONN</strong></td>
<td>✅</td>
<td>⚠️ (complex)</td>
<td>✅</td>
<td>Low</td>
<td>Research, demos</td>
</tr>
<tr class="odd">
<td><strong>Gemini</strong></td>
<td>❌</td>
<td>❌</td>
<td>✅</td>
<td>High</td>
<td>Production (if cost OK)</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="fine-tuning-an-audio-llm">Fine-Tuning an Audio-LLM</h2>
<h3 id="qwen-audio-fine-tuning-example"><strong>Qwen-Audio Fine-Tuning
Example</strong></h3>
<p><strong>Goal:</strong> Fine-tune for your specific use case (e.g.,
meeting transcription with diarization).</p>
<p><strong>Data Format:</strong></p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="ot">[</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">{</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;audio&quot;</span><span class="fu">:</span> <span class="st">&quot;path/to/audio1.wav&quot;</span><span class="fu">,</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;prompt&quot;</span><span class="fu">:</span> <span class="st">&quot;Transcribe this meeting. Participants are Alice, Bob, and Charlie. Format with speaker labels.&quot;</span><span class="fu">,</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;response&quot;</span><span class="fu">:</span> <span class="st">&quot;Alice: Let&#39;s start with the budget.</span><span class="ch">\n</span><span class="st">Bob: I think we need to cut costs.</span><span class="ch">\n</span><span class="st">...&quot;</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">{</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;audio&quot;</span><span class="fu">:</span> <span class="st">&quot;path/to/audio2.wav&quot;</span><span class="fu">,</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;prompt&quot;</span><span class="fu">:</span> <span class="st">&quot;Summarize the key decisions from this call.&quot;</span><span class="fu">,</span></span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">&quot;response&quot;</span><span class="fu">:</span> <span class="st">&quot;1. Approved budget of $50k</span><span class="ch">\n</span><span class="st">2. Next meeting on Friday</span><span class="ch">\n</span><span class="st">...&quot;</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">}</span></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="ot">]</span></span></code></pre></div>
<p><strong>Fine-Tuning Code (Conceptual):</strong></p>
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, Trainer</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(<span class="st">&quot;Qwen/Qwen-Audio&quot;</span>)</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">&quot;Qwen/Qwen-Audio&quot;</span>)</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_args,</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>train_dataset,</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>trainer.train()</span></code></pre></div>
<p><strong>Challenges:</strong> - <strong>Data collection</strong>: Need
audio + instruction + desired output triples - <strong>Compute</strong>:
Audio-LLMs are large (7B-13B params) → need GPUs -
<strong>Labeling</strong>: Creating instruction-following data is
labor-intensive</p>
<hr />
<h2 id="practical-recommendations">Practical Recommendations</h2>
<h3 id="immediate-solution-today"><strong>Immediate Solution
(Today):</strong></h3>
<p><strong>Use Whisper + LLM Pipeline</strong></p>
<ol type="1">
<li>Fine-tune Whisper for your audio (if needed)</li>
<li>Use a local LLM (Llama 2, Mistral via Ollama) for
post-processing</li>
<li>Prompt engineering for diarization/formatting</li>
</ol>
<p><strong>Pros:</strong> - Works now - Flexible - Can run locally
(privacy)</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb80"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>whisper_output <span class="op">=</span> whisper.transcribe(<span class="st">&quot;call.wav&quot;</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>llm_prompt <span class="op">=</span> <span class="ss">f&quot;&quot;&quot;</span></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Transcription: </span><span class="sc">{</span>whisper_output<span class="sc">}</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="ss">Task: This is a support call. The caller is the customer, the agent is support.</span></span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="ss">Diarize and format as Q&amp;A.</span></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="ss">&quot;&quot;&quot;</span></span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a>formatted_output <span class="op">=</span> llm(llm_prompt)</span></code></pre></div>
<hr />
<h3 id="short-term-3-6-months"><strong>Short-Term (3-6
Months):</strong></h3>
<p><strong>Experiment with Qwen-Audio</strong></p>
<ol type="1">
<li>Test Qwen-Audio on your audio samples</li>
<li>Evaluate instruction-following quality</li>
<li>If promising, fine-tune on your specific tasks</li>
</ol>
<p><strong>Why:</strong> - Most mature open-source Audio-LLM - Active
development - Fine-tunable</p>
<hr />
<h3 id="long-term-1-2-years"><strong>Long-Term (1-2
Years):</strong></h3>
<p><strong>Wait for Specialized Models</strong></p>
<p>The field is moving fast. Within 1-2 years, expect: - More
open-source Audio-LLMs - Better fine-tuning tools - Purpose-built models
for transcription + instructions</p>
<hr />
<h2 id="why-isnt-this-standard-yet">Why Isn’t This Standard Yet?</h2>
<p><strong>Good question. Several reasons:</strong></p>
<h3 id="technical-complexity"><strong>1. Technical
Complexity</strong></h3>
<p>Combining ASR + LLM requires: - Large-scale multimodal pretraining
(expensive) - Careful architecture design (modality fusion) -
Instruction-following data (labor-intensive)</p>
<h3 id="compute-requirements"><strong>2. Compute
Requirements</strong></h3>
<p>Audio-LLMs are <strong>huge</strong>: - Qwen-Audio: 7B-13B parameters
- Gemini: Likely 100B+ parameters</p>
<p><strong>Training/fine-tuning needs serious compute.</strong></p>
<h3 id="data-scarcity"><strong>3. Data Scarcity</strong></h3>
<p>Unlike text LLMs (trained on internet text), Audio-LLMs need: - Audio
recordings + transcriptions + instructions + desired outputs - This data
barely exists at scale</p>
<h3 id="commercial-incentives"><strong>4. Commercial
Incentives</strong></h3>
<p>Google (Gemini), OpenAI (GPT-4 multimodal) have invested heavily but
kept models proprietary.</p>
<p>Open-source is catching up, but slowly.</p>
<hr />
<h2 id="does-it-have-a-name-terminology-summary">Does It Have a Name?
(Terminology Summary)</h2>
<p><strong>The capability you’re describing doesn’t have ONE universally
accepted name, but here are the terms used:</strong></p>
<table>
<colgroup>
<col style="width: 46%" />
<col style="width: 53%" />
</colgroup>
<thead>
<tr class="header">
<th>Term</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Audio-Language Models (Audio-LLMs)</strong></td>
<td>Most common in research</td>
</tr>
<tr class="even">
<td><strong>Speech Large Language Models (Speech-LLMs)</strong></td>
<td>Emphasizes speech focus</td>
</tr>
<tr class="odd">
<td><strong>Instruction-Following Transcription</strong></td>
<td>Task-specific description</td>
</tr>
<tr class="even">
<td><strong>Multimodal Audio Understanding</strong></td>
<td>Broader term (includes non-speech audio)</td>
</tr>
<tr class="odd">
<td><strong>Prompt-Guided Speech Processing</strong></td>
<td>Emphasizes prompting aspect</td>
</tr>
</tbody>
</table>
<p><strong>If you need to search for papers/models, use “Audio-Language
Models” or “Audio-LLM”.</strong></p>
<hr />
<h2 id="future-outlook-1">Future Outlook</h2>
<p><strong>This is an active research area. Expect rapid
progress:</strong></p>
<p><strong>2024:</strong> - More open-source Audio-LLMs (Qwen-Audio
scale) - Better fine-tuning recipes - Specialized models (e.g., meeting
transcription)</p>
<p><strong>2025:</strong> - On-device Audio-LLMs (mobile-optimized) -
Real-time instruction-following transcription - Fine-tuning accessible
to individuals (not just labs)</p>
<p><strong>2026:</strong> - Whisper-level ubiquity for Audio-LLMs -
Standardized fine-tuning pipelines - Integration into consumer apps</p>
<p><strong>The trajectory is clear: Audio-LLMs will become
standard.</strong></p>
<hr />
<h2 id="summary-1">Summary</h2>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>What’s it called?</strong></td>
<td>Audio-Language Models (Audio-LLMs) or Instruction-Following Speech
Models</td>
</tr>
<tr class="even">
<td><strong>Open-source equivalent?</strong></td>
<td><strong>Qwen-Audio</strong> (best option), SpeechGPT, or Whisper +
LLM pipeline</td>
</tr>
<tr class="odd">
<td><strong>Fine-tunable?</strong></td>
<td>Yes—Qwen-Audio is fine-tunable, but requires substantial
compute</td>
</tr>
<tr class="even">
<td><strong>Why not standard?</strong></td>
<td>Technical complexity, compute cost, data scarcity, commercial
incentives</td>
</tr>
<tr class="odd">
<td><strong>Recommended approach?</strong></td>
<td><strong>Near-term:</strong> Whisper + LLM pipeline.
<strong>Future:</strong> Fine-tune Qwen-Audio.</td>
</tr>
</tbody>
</table>
<p><strong>Gemini’s audio capabilities represent the future of
transcription—combining ASR with LLM reasoning. Open-source is catching
up, with Qwen-Audio as the current leader.</strong></p>
<p><strong>For your use case (diarization, formatting, context-aware
transcription), start with a Whisper + LLM pipeline today, and monitor
Qwen-Audio for future fine-tuning.</strong></p>
<hr />
<p><strong>Note</strong>: This explanation was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Audio-Language Models are a rapidly evolving field—check Hugging Face’s
audio tasks page and arXiv for latest models. The Whisper + LLM pipeline
is a practical interim solution until unified fine-tunable Audio-LLMs
mature. For production use, evaluate Qwen-Audio’s instruction-following
quality on your specific audio domain before committing to
fine-tuning.</p>
<h2 id="why-asr-finetuning-lacks-ease-of-use">Why Asr Finetuning Lacks
Ease Of Use</h2>
<h2 id="question-2">Question</h2>
<p>Why isn’t there a streamlined, GUI-based way to fine-tune ASR models
like Whisper? Why do we still need notebooks, Google Colab, and
technical knowledge? Why hasn’t a cloud service or simple tool emerged
for this?</p>
<h2 id="answer-3">Answer</h2>
<p>Excellent and insightful question! You’ve identified a real gap in
the ecosystem. Let’s explore why ASR fine-tuning remains technical and
what explains the lack of user-friendly tools.</p>
<hr />
<h2 id="the-current-state-why-its-still-hard">The Current State: Why
It’s Still “Hard”</h2>
<p>You’re absolutely right: - <strong>Fine-tuning LLMs (text)</strong>:
Relatively streamlined (OpenAI fine-tuning API, Hugging Face AutoTrain,
etc.) - <strong>Fine-tuning ASR (audio)</strong>: Still requires
notebooks, code, technical setup</p>
<p><strong>Why the disparity?</strong></p>
<hr />
<h2 id="reason-1-market-size-commercial-incentives">Reason 1: Market
Size &amp; Commercial Incentives</h2>
<h3 id="llm-fine-tuning-huge-market"><strong>LLM Fine-Tuning: Huge
Market</strong></h3>
<p><strong>Who needs it:</strong> - Enterprises (customer support,
document processing, etc.) - Startups (custom chatbots, domain-specific
assistants) - Researchers (academic use) - Individuals (personal
assistants, creative writing)</p>
<p><strong>Result:</strong> - OpenAI launched fine-tuning API (GPT-3.5,
GPT-4) - Hugging Face created AutoTrain (one-click fine-tuning) -
Numerous startups (Anyscale, Together AI, etc.) - <strong>Commercial
incentive is massive</strong></p>
<hr />
<h3 id="asr-fine-tuning-niche-market-so-far"><strong>ASR Fine-Tuning:
Niche Market (So Far)</strong></h3>
<p><strong>Who needs it:</strong> - Enterprises with <strong>very
specific</strong> audio domains (medical, legal, call centers) -
Researchers (academia, speech labs) - Niche use cases (low-resource
languages, specialized vocabulary)</p>
<p><strong>Why smaller:</strong> 1. <strong>Good-enough
baseline</strong>: Whisper, Google Speech, AWS Transcribe already handle
80-90% of use cases 2. <strong>Domain overlap</strong>: Most business
audio (meetings, calls) is covered by general models 3. <strong>Data
scarcity</strong>: Collecting high-quality audio data is harder than
text 4. <strong>Compute cost</strong>: Audio fine-tuning is expensive
(GPUs, storage for audio files)</p>
<p><strong>Result:</strong> - Less commercial pressure to build
consumer-friendly tools - Market not yet big enough to justify polished
GUIs - Tools exist for enterprise (see below) but not for
individuals</p>
<hr />
<h2 id="reason-2-technical-complexity-of-audio-data">Reason 2: Technical
Complexity of Audio Data</h2>
<h3 id="text-fine-tuning-simple-data"><strong>Text Fine-Tuning: Simple
Data</strong></h3>
<p><strong>Input:</strong></p>
<pre><code>{&quot;prompt&quot;: &quot;Translate to French: Hello&quot;, &quot;completion&quot;: &quot;Bonjour&quot;}</code></pre>
<ul>
<li>Text files are small (KB per example)</li>
<li>Easy to upload (CSV, JSON)</li>
<li>No special processing needed</li>
<li>Validation is straightforward</li>
</ul>
<p><strong>Result:</strong> Easy to build a web UI where you upload a
CSV and click “Train.”</p>
<hr />
<h3 id="audio-fine-tuning-complex-data"><strong>Audio Fine-Tuning:
Complex Data</strong></h3>
<p><strong>Input:</strong></p>
<pre><code>Audio file: 30-second WAV (4.8 MB)
Transcription: &quot;This is the transcription&quot;
Metadata: Speaker ID, sampling rate, duration, etc.</code></pre>
<p><strong>Challenges:</strong></p>
<h4 id="file-size"><strong>1. File Size</strong></h4>
<ul>
<li>1 hour of audio (16kHz WAV) = ~115 MB</li>
<li>10 hours = 1.15 GB</li>
<li>100 hours = 11.5 GB</li>
</ul>
<p><strong>Uploading 10+ GB to a web UI is slow and
error-prone.</strong></p>
<h4 id="format-diversity"><strong>2. Format Diversity</strong></h4>
<ul>
<li>WAV, MP3, FLAC, OGG, M4A, etc.</li>
<li>Different sample rates (8kHz, 16kHz, 44.1kHz, 48kHz)</li>
<li>Mono vs. stereo</li>
<li>Different bit depths (16-bit, 24-bit, 32-bit float)</li>
</ul>
<p><strong>A GUI needs to handle all these formats and convert
them.</strong></p>
<h4 id="validation-complexity"><strong>3. Validation
Complexity</strong></h4>
<ul>
<li>Is the audio file corrupt?</li>
<li>Does the transcription match the audio duration?</li>
<li>Are there missing/mismatched files?</li>
<li>Is the sample rate appropriate?</li>
</ul>
<p><strong>Requires sophisticated validation, unlike simple
text.</strong></p>
<h4 id="preprocessing"><strong>4. Preprocessing</strong></h4>
<ul>
<li>Audio normalization (volume leveling)</li>
<li>Resampling (convert to 16kHz for Whisper)</li>
<li>Silence trimming</li>
<li>Augmentation (speed, pitch, noise)</li>
</ul>
<p><strong>Notebooks let users customize; GUIs would need to expose
these options (complex UI).</strong></p>
<hr />
<h2 id="reason-3-computational-requirements-cost">Reason 3:
Computational Requirements &amp; Cost</h2>
<h3 id="llm-fine-tuning-small-models"><strong>LLM Fine-Tuning (Small
Models)</strong></h3>
<ul>
<li><strong>GPT-3.5 fine-tuning</strong>: $0.008/1k tokens (training) +
$0.012/1k tokens (inference)</li>
<li><strong>Run on modest GPUs</strong>: Many models &lt;7B params can
fine-tune on consumer GPUs</li>
</ul>
<p><strong>Result:</strong> Cheap and accessible → commercial services
viable.</p>
<hr />
<h3 id="asr-fine-tuning-large-models"><strong>ASR Fine-Tuning (Large
Models)</strong></h3>
<ul>
<li><strong>Whisper Medium</strong>: 769M parameters</li>
<li><strong>Whisper Large</strong>: 1.5B parameters</li>
<li><strong>Training time</strong>: Hours to days on high-end GPUs</li>
<li><strong>GPU requirements</strong>: 16-40 GB VRAM (A100, H100)</li>
<li><strong>Storage</strong>: Audio data is 10-100x larger than text
data</li>
</ul>
<p><strong>Cost Estimate (Cloud GPU):</strong></p>
<pre><code>10 hours of audio, Whisper Medium, 5 epochs:
- GPU: A100 40GB for 8 hours = $20-40
- Storage: 1 GB audio + checkpoints = $5
Total: ~$25-50 per fine-tune</code></pre>
<p><strong>For a cloud service:</strong> - Need to provision GPUs
(expensive idle time if not batching users) - Need large storage (audio
files) - Need to manage uploads/downloads (bandwidth costs)</p>
<p><strong>This is why most tools direct you to bring-your-own-GPU
(Colab, notebooks).</strong></p>
<hr />
<h2 id="reason-4-fragmented-ecosystem">Reason 4: Fragmented
Ecosystem</h2>
<h3 id="llm-fine-tuning-convergence"><strong>LLM Fine-Tuning:
Convergence</strong></h3>
<p><strong>Standard Stack:</strong> - Hugging Face Transformers (de
facto standard) - Standard datasets format (JSON/CSV) - Common training
APIs (Trainer, SFTTrainer)</p>
<p><strong>Result:</strong> Easy to build unified tools (AutoTrain,
OpenAI API).</p>
<hr />
<h3 id="asr-fine-tuning-fragmented"><strong>ASR Fine-Tuning:
Fragmented</strong></h3>
<p><strong>Multiple frameworks:</strong> - Hugging Face Transformers
(Whisper, Wav2Vec2) - ESPnet (research-oriented, complex) - Kaldi (old
but still used) - NeMo (NVIDIA-specific) - Fairseq (Meta, less
maintained)</p>
<p><strong>Multiple model families:</strong> - Whisper (encoder-decoder)
- Wav2Vec2 (encoder-only, CTC) - HuBERT (different training paradigm) -
Conformer (different architecture)</p>
<p><strong>Multiple preprocessing approaches:</strong> -
Mel-spectrograms vs. raw audio - Different augmentation techniques - VAD
(Voice Activity Detection) vs. no VAD</p>
<p><strong>Result:</strong> Harder to build one-size-fits-all GUI.</p>
<hr />
<h2 id="reason-5-lag-behind-llm-tooling">Reason 5: Lag Behind LLM
Tooling</h2>
<h3 id="timeline"><strong>Timeline:</strong></h3>
<p><strong>2020-2022: LLM boom</strong> - GPT-3, ChatGPT → massive
commercial interest - Fine-tuning tools emerge rapidly</p>
<p><strong>2022-2024: ASR catches up</strong> - Whisper released (Sept
2022) - Only recently became clear that fine-tuning Whisper is practical
for consumers - Tooling is still maturing</p>
<p><strong>ASR fine-tuning is ~2 years behind LLM fine-tuning in terms
of UX.</strong></p>
<hr />
<h2 id="what-exists-today-you-might-have-missed">What Exists Today (You
Might Have Missed)</h2>
<p><strong>You said there’s “no streamlined way,” but some tools
exist—they’re just not widely known:</strong></p>
<h3 id="hugging-face-autotrain-audio-support"><strong>1. Hugging Face
AutoTrain (Audio Support)</strong></h3>
<p><strong>What it is:</strong> - Web UI for fine-tuning models
(including ASR) - Upload audio dataset → select model → train - Runs on
Hugging Face’s infrastructure</p>
<p><strong>How to use:</strong> 1. Go to <a
href="https://ui.autotrain.huggingface.co/">https://ui.autotrain.huggingface.co/</a>
2. Create a new project (select “Speech Recognition”) 3. Upload audio
dataset (audiofolder format) 4. Select base model (Whisper, Wav2Vec2) 5.
Configure hyperparameters 6. Pay for compute time (via Hugging Face
credits)</p>
<p><strong>Limitations:</strong> - Still requires understanding of
dataset formats - Not as polished as LLM fine-tuning UI - Compute costs
can add up</p>
<p><strong>But it exists!</strong> This is closest to what you’re asking
for.</p>
<hr />
<h3 id="unsloth-notebook-first-but-easier"><strong>2. Unsloth
(Notebook-First, But Easier)</strong></h3>
<p><strong>What it is:</strong> - Optimized fine-tuning library (2-4x
faster than standard) - Notebooks, but with minimal code</p>
<p><strong>Why notebooks:</strong> - Reproducibility (share exact setup)
- Flexibility (customize easily) - Cost (use free Colab GPUs)</p>
<p><strong>Why not GUI:</strong> - Unsloth is a small team (can’t build
polished GUI) - Notebooks reach technical audience (their target market)
- Monetization harder for GUI tools (who pays?)</p>
<hr />
<h3 id="assemblyai-custom-models-commercial"><strong>3. AssemblyAI
Custom Models (Commercial)</strong></h3>
<p><strong>What it is:</strong> - Enterprise ASR service with custom
model fine-tuning - Upload audio, they fine-tune for you - No code
needed (API-based)</p>
<p><strong>How it works:</strong> 1. Upload audio dataset (via their
dashboard) 2. They fine-tune Whisper (or their own models) 3. Deploy as
custom API endpoint</p>
<p><strong>Cost:</strong> - Enterprise pricing (not public, likely
$$$)</p>
<p><strong>Target:</strong> - Businesses with budgets (call centers,
legal firms, etc.)</p>
<p><strong>Not for individuals</strong> (no self-service, no public
pricing).</p>
<hr />
<h3 id="deepgram-custom-models-commercial"><strong>4. Deepgram Custom
Models (Commercial)</strong></h3>
<p><strong>Similar to AssemblyAI:</strong> - Enterprise service - Upload
audio → they fine-tune - API deployment</p>
<p><strong>Again, not for individuals.</strong></p>
<hr />
<h2 id="why-no-consumer-friendly-tool-yet">Why No Consumer-Friendly Tool
Yet?</h2>
<p><strong>Synthesizing the reasons:</strong></p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor</th>
<th>Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Market size</strong></td>
<td>Small (niche use cases) vs. LLMs (universal)</td>
</tr>
<tr class="even">
<td><strong>Data complexity</strong></td>
<td>Audio files large, hard to upload/validate</td>
</tr>
<tr class="odd">
<td><strong>Compute cost</strong></td>
<td>Expensive (GPUs, storage) → hard to offer free tier</td>
</tr>
<tr class="even">
<td><strong>Fragmentation</strong></td>
<td>Multiple frameworks/models → hard to unify</td>
</tr>
<tr class="odd">
<td><strong>Timeline</strong></td>
<td>ASR fine-tuning only recently practical (post-Whisper 2022)</td>
</tr>
<tr class="even">
<td><strong>Commercial incentive</strong></td>
<td>Enterprise tools exist, consumer market unproven</td>
</tr>
</tbody>
</table>
<p><strong>Bottom line: The consumer market for ASR fine-tuning isn’t
big enough (yet) to justify a polished, affordable GUI
tool.</strong></p>
<hr />
<h2 id="whats-coming-predictions">What’s Coming (Predictions)</h2>
<p><strong>The landscape is changing. Here’s what to
expect:</strong></p>
<h3 id="short-term-2024-2025"><strong>Short-Term
(2024-2025):</strong></h3>
<ol type="1">
<li><strong>Hugging Face AutoTrain improvements</strong>
<ul>
<li>Better audio UX (drag-and-drop, format auto-detection)</li>
<li>Cheaper compute options</li>
<li>More tutorials/guides</li>
</ul></li>
<li><strong>Startup entrants</strong>
<ul>
<li>Someone will build “Replicate for ASR” (one-click fine-tuning)</li>
<li>Likely API-based (upload audio via API, poll for completion)</li>
<li>Pricing: $10-50 per fine-tune</li>
</ul></li>
<li><strong>Open-source CLI tools</strong>
<ul>
<li>Simpler wrappers around Transformers</li>
<li><code>finetune-whisper --audio-dir ./data --model medium</code> (one
command)</li>
<li>Already starting to appear (e.g.,
<code>whisper-finetune</code>)</li>
</ul></li>
</ol>
<hr />
<h3 id="long-term-2025-2027"><strong>Long-Term
(2025-2027):</strong></h3>
<ol type="1">
<li><strong>Cloud services mature</strong>
<ul>
<li>Google Cloud AI / AWS SageMaker add ASR fine-tuning</li>
<li>GUI + pay-as-you-go pricing</li>
<li>Integrated with their transcription APIs</li>
</ul></li>
<li><strong>Local fine-tuning tools (GUI)</strong>
<ul>
<li>Desktop apps (think “Whisper Studio”)</li>
<li>Drag-and-drop audio files</li>
<li>One-click fine-tune (uses your GPU)</li>
<li>Open-source (likely community-built)</li>
</ul></li>
<li><strong>Consumer AI assistants</strong>
<ul>
<li>Smartphone apps that fine-tune on-device</li>
<li>“Train your phone’s STT on your voice” (tap to train)</li>
<li>Powered by quantized models (INT4/INT8)</li>
</ul></li>
</ol>
<hr />
<h2 id="explaining-to-a-non-technical-friend">Explaining to a
Non-Technical Friend</h2>
<p><strong>Your observation:</strong> &gt; “By the time I start talking
about Python notebooks and Google Colab, they’re going to be already
confused.”</p>
<p><strong>This is the exact problem.</strong> Here’s how to explain
it:</p>
<p><strong>Current state:</strong> &gt; “Right now, fine-tuning
speech-to-text is like baking a cake from scratch. You need to know the
recipe (code), have the right tools (GPU, Python), and follow detailed
steps (notebook). There’s no Betty Crocker box mix yet.”</p>
<p><strong>Why:</strong> &gt; “Speech data is big and messy (like
ingredients that go bad quickly). It’s expensive to train (like needing
a commercial oven). And there aren’t enough people doing it yet for
someone to build an easy ‘box mix’ version.”</p>
<p><strong>Future:</strong> &gt; “Within a year or two, you’ll probably
be able to upload audio files to a website, click ‘Train,’ and get your
custom model. Like uploading photos to Google Photos. But we’re not
quite there yet.”</p>
<hr />
<h2 id="what-you-can-do-today">What You Can Do Today</h2>
<h3
id="option-1-use-hugging-face-autotrain-closest-to-gui"><strong>Option
1: Use Hugging Face AutoTrain (Closest to GUI)</strong></h3>
<ul>
<li>Go to <a
href="https://ui.autotrain.huggingface.co">ui.autotrain.huggingface.co</a></li>
<li>Upload audio dataset</li>
<li>Select Whisper</li>
<li>Train (pay for compute)</li>
</ul>
<p><strong>Pros:</strong> Closest to “just click and train”
<strong>Cons:</strong> Still requires understanding dataset format,
costs add up</p>
<hr />
<h3
id="option-2-use-a-notebook-template-easier-than-it-looks"><strong>Option
2: Use a Notebook Template (Easier Than It Looks)</strong></h3>
<p><strong>Reality: Notebooks aren’t as scary as they seem.</strong></p>
<p><strong>What you do:</strong> 1. Copy a template (Unsloth, Hugging
Face) 2. Change 3 variables: - Path to your audio - Model size (small,
medium, large) - Number of training steps 3. Click “Run All” 4. Wait</p>
<p><strong>It’s more “fill in the blanks” than “write
code.”</strong></p>
<p><strong>Template example:</strong></p>
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>dataset_path <span class="op">=</span> <span class="st">&quot;/content/my_audio_dataset&quot;</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">&quot;openai/whisper-medium&quot;</span></span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Most notebooks are ~80% boilerplate you never
touch.</strong></p>
<hr />
<h3 id="option-3-wait-for-better-tools-6-12-months"><strong>Option 3:
Wait for Better Tools (6-12 Months)</strong></h3>
<p><strong>If you’re not in a rush:</strong> - Market is clearly moving
toward easier tools - Hugging Face will likely improve AutoTrain
significantly - Startups are entering the space</p>
<p><strong>By mid-2025, expect much friendlier options.</strong></p>
<hr />
<h2 id="the-irony-fine-tuning-is-getting-easier-but-perception-lags">The
Irony: Fine-Tuning Is Getting Easier, But Perception Lags</h2>
<p><strong>Technical reality:</strong> - Fine-tuning Whisper is
<strong>dramatically easier</strong> than it was 2 years ago - Unsloth,
LoRA, QLoRA make it 4x faster and cheaper - Notebooks abstract away most
complexity</p>
<p><strong>Perception:</strong> - Still seen as “expert-only” - Lack of
GUI reinforces this - Tech-savvy users share notebooks, but
non-technical users don’t discover them</p>
<p><strong>The gap between capability and accessibility is closing, but
not closed.</strong></p>
<hr />
<h2 id="summary-2">Summary</h2>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Why no GUI?</strong></td>
<td>Small market, high compute cost, technical complexity, recent (2022)
viability</td>
</tr>
<tr class="even">
<td><strong>What exists?</strong></td>
<td>Hugging Face AutoTrain (closest to GUI), enterprise services
(AssemblyAI, Deepgram)</td>
</tr>
<tr class="odd">
<td><strong>Why notebooks?</strong></td>
<td>Flexible, reproducible, free (Colab), reach technical audience</td>
</tr>
<tr class="even">
<td><strong>When will it improve?</strong></td>
<td>6-12 months for better web UIs, 1-2 years for mature consumer
tools</td>
</tr>
<tr class="odd">
<td><strong>What to do now?</strong></td>
<td>Use AutoTrain (GUI), or use notebook templates (easier than it
looks)</td>
</tr>
</tbody>
</table>
<p><strong>Your frustration is valid—ASR fine-tuning lags LLM
fine-tuning in UX by ~2 years.</strong></p>
<p><strong>But the trajectory is clear: This will get much easier very
soon.</strong></p>
<p><strong>In 2-3 years, explaining ASR fine-tuning to a non-technical
friend will be:</strong> &gt; “Upload your audio files to this website,
click ‘Train,’ wait an hour, and you’re done. Like ordering food
delivery.”</p>
<p><strong>We’re not there yet, but we’re getting close.</strong></p>
<hr />
<p><strong>Note</strong>: This explanation was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook. The
ASR fine-tuning ecosystem is evolving rapidly—check Hugging Face
AutoTrain, emerging startups, and open-source projects for latest
developments. For non-technical users, templated notebooks are currently
the best compromise between ease of use and flexibility. Expect
significant UX improvements in 2024-2025 as market demand grows and
tooling matures.</p>
</body>
</html>
