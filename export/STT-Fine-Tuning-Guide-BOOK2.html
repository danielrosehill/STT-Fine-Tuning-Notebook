<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Speech-to-Text Fine-Tuning Guide - Book 2 - Implementation</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="book-style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Speech-to-Text Fine-Tuning Guide - Book 2 -
Implementation</h1>
</header>
<h1 id="speech-to-text-fine-tuning-guide">Speech-to-Text Fine-Tuning
Guide</h1>
<h2 id="book-2-implementation">Book 2: Implementation</h2>
<p><em>Fine-Tuning, Inference &amp; Hardware Optimization</em></p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<p><strong>Part IV: Fine-Tuning</strong><br />
Fine-tuning strategies and techniques (7 chapters)</p>
<p><strong>Part V: Inference &amp; Deployment</strong><br />
Running and deploying ASR models (4 chapters)</p>
<p><strong>Part VI: AMD GPU Optimization</strong><br />
AMD-specific hardware considerations (3 chapters)</p>
<hr />
<h1 id="part-iv-fine-tuning">Part IV: Fine-Tuning</h1>
<p><em>Fine-tuning strategies and techniques</em></p>
<hr />
<h2 id="fine-tuning-small-models-strategy">Fine Tuning Small Models
Strategy</h2>
<h2 id="the-strategic-question">The Strategic Question</h2>
<p>If your desktop GPU can comfortably run Whisper Small but struggles
with Medium/Large, and you notice accuracy drops with stock Small
compared to larger models:</p>
<p><strong>Would fine-tuning Small or Tiny models be a more practical
strategy than fine-tuning Large models that you can only run in the
cloud?</strong></p>
<h2 id="short-answer">Short Answer</h2>
<p><strong>Yes! Fine-tuning smaller models (Small/Tiny) for local
inference is an excellent and often overlooked strategy.</strong></p>
<p>The accuracy improvements from fine-tuning can be <strong>more
significant</strong> for smaller models than larger ones, and the
practical benefits for daily use are substantial:</p>
<ul>
<li><strong>Fine-tuned Whisper Small can approach or match stock Whisper
Medium accuracy</strong> for your specific voice/vocabulary</li>
<li><strong>Fine-tuned Whisper Tiny can approach stock Small
accuracy</strong></li>
<li>You get these benefits with fast, local inference on modest
hardware</li>
<li>More practical than fine-tuning Large models you can only use via
expensive API calls</li>
</ul>
<h2 id="the-math-fine-tuning-gains-vs-model-size">The Math: Fine-Tuning
Gains vs Model Size</h2>
<h3 id="baseline-accuracy-stock-models-general-speech">Baseline Accuracy
(Stock Models, General Speech)</h3>
<p>Typical Word Error Rates (WER) on diverse audio:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>WER (clean)</th>
<th>WER (noisy)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Large-v3</td>
<td>1550M</td>
<td>3-5%</td>
<td>8-12%</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>769M</td>
<td>4-7%</td>
<td>10-15%</td>
</tr>
<tr class="odd">
<td>Small</td>
<td>244M</td>
<td>8-12%</td>
<td>15-25%</td>
</tr>
<tr class="even">
<td>Base</td>
<td>74M</td>
<td>12-18%</td>
<td>25-35%</td>
</tr>
<tr class="odd">
<td>Tiny</td>
<td>39M</td>
<td>15-25%</td>
<td>30-45%</td>
</tr>
</tbody>
</table>
<p><strong>Observation:</strong> Each size tier represents roughly
1.5-2× more errors</p>
<h3 id="fine-tuning-improvements-typical-gains">Fine-Tuning Improvements
(Typical Gains)</h3>
<p>When fine-tuned on 5-10 hours of personal data:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Baseline WER</th>
<th>Fine-tuned WER</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Large-v3</td>
<td>5%</td>
<td>3-4%</td>
<td>1-2% absolute (20-40% relative)</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>6%</td>
<td>4-5%</td>
<td>1-2% absolute (17-33% relative)</td>
</tr>
<tr class="odd">
<td>Small</td>
<td>10%</td>
<td>5-7%</td>
<td>3-5% absolute (30-50% relative)</td>
</tr>
<tr class="even">
<td>Base</td>
<td>15%</td>
<td>8-11%</td>
<td>4-7% absolute (27-47% relative)</td>
</tr>
<tr class="odd">
<td>Tiny</td>
<td>20%</td>
<td>10-14%</td>
<td>6-10% absolute (30-50% relative)</td>
</tr>
</tbody>
</table>
<p><strong>Key insight:</strong> Smaller models have <strong>more room
to improve</strong> because:</p>
<ol type="1">
<li>They start with higher error rates</li>
<li>Fine-tuning teaches specific patterns they initially missed</li>
<li>Domain specialization matters more when base capacity is
limited</li>
</ol>
<h3 id="the-crossover-effect">The Crossover Effect</h3>
<p><strong>Fine-tuned Small can match or beat stock Medium for your
specific use case:</strong></p>
<pre><code>Stock Medium (general speech): 6% WER
Fine-tuned Small (your voice): 5-7% WER

Result: Fine-tuned Small ≈ Stock Medium for YOUR audio</code></pre>
<p><strong>Fine-tuned Tiny can match or beat stock Base:</strong></p>
<pre><code>Stock Base (general speech): 15% WER
Fine-tuned Tiny (your voice): 10-14% WER

Result: Fine-tuned Tiny approaches Stock Small</code></pre>
<p>This is the <strong>fine-tuning sweet spot</strong> for
resource-constrained scenarios.</p>
<h2 id="why-smaller-models-benefit-more-from-fine-tuning">Why Smaller
Models Benefit More from Fine-Tuning</h2>
<h3 id="capacity-limitation-vs-specialization">1. Capacity Limitation vs
Specialization</h3>
<p><strong>Large models:</strong> Have capacity to handle diverse
scenarios</p>
<ul>
<li>Already perform well on your voice (within their general
capability)</li>
<li>Fine-tuning refines edges, adds vocabulary</li>
<li>Gains are incremental</li>
</ul>
<p><strong>Small models:</strong> Limited capacity forces
generalization</p>
<ul>
<li>Must compress 680,000 hours of training into fewer parameters</li>
<li>Sacrifice some accuracy for breadth</li>
<li>Fine-tuning says: “Forget broad coverage, focus on THIS”</li>
</ul>
<p><strong>Analogy:</strong></p>
<ul>
<li>Large model: Expert who knows 10,000 topics, fine-tuning adds 10
more</li>
<li>Small model: Generalist who knows 1,000 topics, fine-tuning replaces
100 irrelevant ones with your specific needs</li>
</ul>
<h3 id="target-vocabulary-impact">2. Target Vocabulary Impact</h3>
<p>For rare vocabulary (Hebrew words, technical terms, proper
nouns):</p>
<p><strong>Large models:</strong></p>
<pre><code>&quot;Mekolet&quot; (unfamiliar word)
Large model: &quot;makaleh&quot; (best guess from phonetics)
Fine-tuned Large: &quot;Mekolet&quot; (learned from your data)

Error reduction: 1 word per sentence</code></pre>
<p><strong>Small models:</strong></p>
<pre><code>&quot;Mekolet&quot; (unfamiliar word)
Small model: &quot;the color&quot; (worse phonetic guess, more confusion)
Fine-tuned Small: &quot;Mekolet&quot; (learned from your data)

Error reduction: 1 word per sentence + fewer cascading errors</code></pre>
<p><strong>Impact:</strong> Same vocabulary learning, but starts from
worse baseline = bigger improvement</p>
<h3 id="voice-adaptation">3. Voice Adaptation</h3>
<p><strong>Your unique voice characteristics</strong> (accent, pace,
prosody) matter more for smaller models:</p>
<p><strong>Large models:</strong> Robust to accent variations</p>
<ul>
<li>Trained on such diverse data that your accent is likely covered</li>
<li>Fine-tuning adjusts, but marginally</li>
</ul>
<p><strong>Small models:</strong> Less accent diversity in effective
training</p>
<ul>
<li>Fewer parameters = less capacity to memorize accent patterns</li>
<li>Your accent may not be well-represented</li>
<li>Fine-tuning teaches: “This is what speech sounds like”</li>
</ul>
<p><strong>Result:</strong> Bigger gains for smaller models</p>
<h2 id="real-world-example-your-use-case">Real-World Example: Your Use
Case</h2>
<p>Based on your described scenario:</p>
<h3 id="current-state-stock-whisper-small">Current State: Stock Whisper
Small</h3>
<p><strong>Performance:</strong></p>
<ul>
<li>Runs well on your GPU (no throttling)</li>
<li>Noticeable accuracy drop vs larger models</li>
<li>Struggles with:
<ul>
<li>Hebrew vocabulary (Mekolet, etc.)</li>
<li>Your specific accent/speaking patterns</li>
<li>Technical terms you use frequently</li>
</ul></li>
</ul>
<p><strong>Estimated WER:</strong> 12-15% on your audio</p>
<h3 id="after-fine-tuning-fine-tuned-whisper-small">After Fine-Tuning:
Fine-Tuned Whisper Small</h3>
<p><strong>Expected improvements:</strong></p>
<ol type="1">
<li><strong>Hebrew vocabulary:</strong> 90-95% accuracy on trained
words</li>
<li><strong>Your voice:</strong> 20-40% error reduction</li>
<li><strong>Domain terms:</strong> 70-90% accuracy on your specific
terminology</li>
</ol>
<p><strong>Estimated WER:</strong> 6-8% on your audio</p>
<p><strong>Comparison:</strong></p>
<ul>
<li>Stock Medium: ~7-9% WER on your audio</li>
<li>Fine-tuned Small: ~6-8% WER on your audio</li>
<li><strong>Practical equivalence!</strong></li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>✓ Runs locally on your GPU</li>
<li>✓ Faster inference (Small = 2× speed of Medium)</li>
<li>✓ No API costs</li>
<li>✓ Privacy (all local)</li>
<li>✓ Offline capability</li>
</ul>
<h3 id="alternative-fine-tuning-large-api-only">Alternative: Fine-Tuning
Large (API Only)</h3>
<p><strong>If you fine-tuned Whisper Large but can only use it via cloud
API:</strong></p>
<p><strong>Expected accuracy:</strong> ~3-4% WER (excellent!)</p>
<p><strong>Practical drawbacks:</strong></p>
<ul>
<li>✗ Requires internet connection</li>
<li>✗ API costs ($0.006/minute = $3.60/hour = ~$50-100/month for heavy
use)</li>
<li>✗ Latency (network round-trip adds 200-500ms)</li>
<li>✗ Privacy concerns (audio sent to cloud)</li>
<li>✗ Dependency on API availability</li>
</ul>
<p><strong>Trade-off question:</strong> Is 3-5% absolute WER improvement
worth the practical costs?</p>
<p>For many users: <strong>No.</strong> Daily usability matters more
than ultimate accuracy.</p>
<h2 id="fine-tuning-tiny-the-ultra-efficient-option">Fine-Tuning Tiny:
The Ultra-Efficient Option</h2>
<h3 id="why-fine-tune-tiny">Why Fine-Tune Tiny?</h3>
<p><strong>Use case:</strong> Phone, embedded devices, ultra-fast
inference</p>
<p><strong>Stock Tiny problems:</strong></p>
<ul>
<li>20-25% WER on general speech</li>
<li>Struggles significantly with uncommon vocabulary</li>
<li>Limited robustness to noise and accents</li>
</ul>
<p><strong>Fine-tuned Tiny potential:</strong></p>
<ul>
<li>10-14% WER on your specific voice/domain</li>
<li>Excellent on trained vocabulary</li>
<li>Matches or exceeds stock Base model</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>✓ Runs on phones smoothly</li>
<li>✓ Extremely fast inference (10-20× real-time)</li>
<li>✓ Minimal battery impact</li>
<li>✓ &lt;100MB model size (even quantized to ~40MB)</li>
</ul>
<p><strong>Practical value:</strong> A fine-tuned Tiny on your phone
beats any cloud API in:</p>
<ul>
<li>Speed (instant)</li>
<li>Privacy (local)</li>
<li>Offline capability</li>
<li>Cost ($0)</li>
</ul>
<h2 id="recommended-strategy-for-local-inference">Recommended Strategy
for Local Inference</h2>
<h3 id="three-tier-approach">Three-Tier Approach</h3>
<h4 id="tier-1-desktop-fine-tuned-small"><strong>Tier 1: Desktop
(Fine-tuned Small)</strong></h4>
<p><strong>Target device:</strong> Your desktop with 8GB GPU</p>
<p><strong>Model:</strong> Fine-tuned Whisper Small</p>
<p><strong>Training data:</strong> 5-10 hours, diverse scenarios</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Fast inference on your GPU</li>
<li>Accuracy approaching Medium</li>
<li>Fully local</li>
</ul>
<p><strong>Use for:</strong></p>
<ul>
<li>Desktop dictation</li>
<li>Long-form transcription</li>
<li>Primary STT workstation</li>
</ul>
<h4 id="tier-2-phone-fine-tuned-tiny"><strong>Tier 2: Phone (Fine-tuned
Tiny)</strong></h4>
<p><strong>Target device:</strong> Your phone</p>
<p><strong>Model:</strong> Fine-tuned Whisper Tiny (GGUF Q4/Q5)</p>
<p><strong>Training data:</strong> Same 5-10 hours (reuse from desktop
training!)</p>
<p><strong>Benefits:</strong></p>
<ul>
<li>Smooth phone performance</li>
<li>Accuracy approaching Base/Small</li>
<li>On-device inference</li>
</ul>
<p><strong>Use for:</strong></p>
<ul>
<li>Mobile dictation</li>
<li>Voice notes</li>
<li>Offline transcription</li>
</ul>
<h4
id="tier-3-cloud-api-fine-tuned-large-or-turbo---optional"><strong>Tier
3: Cloud API (Fine-tuned Large or Turbo) - Optional</strong></h4>
<p><strong>Target:</strong> Occasions requiring maximum accuracy</p>
<p><strong>Model:</strong> Fine-tuned Large-v3 or Turbo via API</p>
<p><strong>Training data:</strong> Same data set</p>
<p><strong>Use for:</strong></p>
<ul>
<li>Critical transcriptions (legal, medical)</li>
<li>Difficult audio (poor quality, heavy noise)</li>
<li>When connected and accuracy is paramount</li>
</ul>
<p><strong>Cost:</strong> ~$0.006/min = $0.36/hour (affordable for
occasional use)</p>
<h3 id="training-efficiency-one-dataset-multiple-models">Training
Efficiency: One Dataset, Multiple Models</h3>
<p><strong>You can fine-tune all three models with the same training
data:</strong></p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> train.py <span class="at">--model</span> small <span class="at">--data</span> dataset/ <span class="at">--epochs</span> 3</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> train.py <span class="at">--model</span> tiny <span class="at">--data</span> dataset/ <span class="at">--epochs</span> 3</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> train.py <span class="at">--model</span> large-v3 <span class="at">--data</span> dataset/ <span class="at">--epochs</span> 2</span></code></pre></div>
<p><strong>Time investment:</strong></p>
<ul>
<li>Data collection: 5-10 hours (one-time)</li>
<li>Training Small: 2-6 hours</li>
<li>Training Tiny: 1-3 hours</li>
<li>Training Large: 6-12 hours</li>
</ul>
<p><strong>Result:</strong> Three fine-tuned models optimized for
different deployment scenarios, all from one data collection effort.</p>
<h2 id="expected-accuracy-comparison">Expected Accuracy Comparison</h2>
<p>Based on your specific voice and vocabulary:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Baseline (Stock)</th>
<th>Fine-tuned</th>
<th>Inference Speed</th>
<th>Deployment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tiny</td>
<td>20% WER</td>
<td>12% WER</td>
<td>10-20× RT</td>
<td>Phone</td>
</tr>
<tr class="even">
<td>Small</td>
<td>12% WER</td>
<td>7% WER</td>
<td>3-5× RT</td>
<td>Desktop</td>
</tr>
<tr class="odd">
<td>Medium</td>
<td>8% WER</td>
<td>6% WER</td>
<td>1.5-2.5× RT</td>
<td>Desktop (heavy)</td>
</tr>
<tr class="even">
<td>Large-v3</td>
<td>5% WER</td>
<td>3% WER</td>
<td>1× RT</td>
<td>Cloud API</td>
</tr>
<tr class="odd">
<td>Large-turbo</td>
<td>6% WER</td>
<td>4% WER</td>
<td>1.5× RT</td>
<td>Cloud API</td>
</tr>
</tbody>
</table>
<p><strong>Key observation:</strong></p>
<ul>
<li>Fine-tuned Small (7% WER) ≈ Stock Medium (8% WER)</li>
<li>Fine-tuned Tiny (12% WER) ≈ Stock Small (12% WER)</li>
</ul>
<p><strong>Practical winner:</strong> Fine-tuned Small for desktop,
Fine-tuned Tiny for mobile</p>
<h2 id="addressing-the-gpu-concern">Addressing the GPU Concern</h2>
<p>Your observation: “Even on my desktop I need Small is about the
biggest I can do to avoid tapping the GPU usage during inference.”</p>
<p><strong>Two clarifications:</strong></p>
<h3 id="gpu-during-inference-is-normal">1. 100% GPU During Inference is
Normal</h3>
<p>As covered in the GPU requirements document:</p>
<ul>
<li>GPU hitting 100% during inference bursts is
<strong>optimal</strong></li>
<li>This is NOT a bottleneck or problem</li>
<li>You WANT full GPU utilization during processing</li>
<li>Between bursts, GPU returns to idle</li>
</ul>
<p><strong>You can likely run Medium just fine on your GPU</strong> if
RTF (real-time factor) is still &lt;1.0</p>
<h3 id="fine-tuned-small-is-still-excellent">2. Fine-Tuned Small is
Still Excellent</h3>
<p>Even if you prefer to run Small to avoid heavy GPU load:</p>
<p><strong>Fine-tuning Small is a great strategy:</strong></p>
<ul>
<li>Gets you to Medium-level accuracy</li>
<li>Faster inference = more responsive experience</li>
<li>Lower power consumption</li>
<li>Reduces thermal/noise concerns</li>
</ul>
<p><strong>This is a valid optimization choice,</strong> not a
limitation.</p>
<h2 id="practical-implementation-steps">Practical Implementation
Steps</h2>
<h3 id="step-1-collect-training-data">Step 1: Collect Training Data</h3>
<p><strong>Target:</strong> 5-10 hours of your voice</p>
<p><strong>Content:</strong></p>
<ul>
<li>60% target vocabulary in natural sentences</li>
<li>30% typical dictation (sentences you’d actually dictate)</li>
<li>10% challenging scenarios (fast speech, technical content)</li>
</ul>
<p><strong>Recording:</strong></p>
<ul>
<li>Quality USB mic in quiet room</li>
<li>16kHz+, WAV format</li>
<li>Natural speaking pace</li>
</ul>
<h3 id="step-2-prepare-data">Step 2: Prepare Data</h3>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">dataset/</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> train/</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>   ├── audio001.wav</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>   ├── audio001.txt</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>   ├── audio002.wav</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>   ├── audio002.txt</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>   ...</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> validation/</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> audio_val001.wav</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> audio_val001.txt</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="ex">...</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="step-3-fine-tune-small-model">Step 3: Fine-Tune Small Model</h3>
<div class="sourceCode" id="cb7"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> finetune_whisper.py <span class="dt">\</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_name</span> openai/whisper-small <span class="dt">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--train_data</span> dataset/train <span class="dt">\</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--val_data</span> dataset/validation <span class="dt">\</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--epochs</span> 3 <span class="dt">\</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 8 <span class="dt">\</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--learning_rate</span> 1e-5</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="step-4-fine-tune-tiny-model">Step 4: Fine-Tune Tiny Model</h3>
<div class="sourceCode" id="cb8"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> finetune_whisper.py <span class="dt">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model_name</span> openai/whisper-tiny <span class="dt">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--train_data</span> dataset/train <span class="dt">\</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--val_data</span> dataset/validation <span class="dt">\</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--epochs</span> 3 <span class="dt">\</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--batch_size</span> 16 <span class="dt">\</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--learning_rate</span> 1e-5</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="step-5-convert-for-deployment">Step 5: Convert for
Deployment</h3>
<p><strong>Desktop (whisper.cpp):</strong></p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> convert-hf-to-gguf.py models/whisper-small-finetuned <span class="dt">\</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--outfile</span> whisper-small-finetuned-q5.gguf <span class="dt">\</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quant</span> q5_0</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="ex">whisper.cpp</span> <span class="at">--model</span> whisper-small-finetuned-q5.gguf</span></code></pre></div>
<p><strong>Phone (FUTO, WhisperKit, etc):</strong></p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> convert-hf-to-gguf.py models/whisper-tiny-finetuned <span class="dt">\</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--outfile</span> whisper-tiny-finetuned-q4.gguf <span class="dt">\</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quant</span> q4_0</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="step-6-compare-and-validate">Step 6: Compare and Validate</h3>
<p><strong>Test on held-out audio</strong> (not in training set):</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">whisper.cpp</span> <span class="at">--model</span> small test_audio.wav <span class="op">&gt;</span> stock_small.txt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="ex">wer</span> stock_small.txt test_audio_reference.txt</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ex">whisper.cpp</span> <span class="at">--model</span> small-finetuned test_audio.wav <span class="op">&gt;</span> finetuned_small.txt</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">wer</span> finetuned_small.txt test_audio_reference.txt</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Expected:</strong> 30-50% WER reduction</p>
<h2 id="cost-benefit-analysis">Cost-Benefit Analysis</h2>
<h3 id="option-a-fine-tune-small-use-locally">Option A: Fine-Tune Small,
Use Locally</h3>
<p><strong>Costs:</strong></p>
<ul>
<li>Training compute: $10-50 (cloud GPU) or free (your GPU)</li>
<li>Development time: 1-2 days</li>
<li>Ongoing: $0</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Local inference (fast, private, offline)</li>
<li>Medium-level accuracy</li>
<li>No per-use costs</li>
</ul>
<p><strong>Best for:</strong> Daily use, privacy-conscious users,
offline needs</p>
<h3 id="option-b-fine-tune-large-use-via-api">Option B: Fine-Tune Large,
Use via API</h3>
<p><strong>Costs:</strong></p>
<ul>
<li>Training compute: $50-200 (requires better GPU/longer training)</li>
<li>Development time: 2-3 days</li>
<li>Ongoing: $0.006/min = $50-100/month (heavy user)</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>Best accuracy (3-4% WER)</li>
<li>No local GPU needed</li>
<li>Access from any device</li>
</ul>
<p><strong>Best for:</strong> Users who prioritize ultimate accuracy
over cost/privacy</p>
<h3 id="option-c-use-stock-large-via-api">Option C: Use Stock Large via
API</h3>
<p><strong>Costs:</strong></p>
<ul>
<li>Training: $0</li>
<li>Ongoing: $0.006/min = $50-100/month</li>
</ul>
<p><strong>Benefits:</strong></p>
<ul>
<li>No training effort</li>
<li>Good general accuracy</li>
<li>Immediate availability</li>
</ul>
<p><strong>Drawback:</strong></p>
<ul>
<li>Not optimized for your voice/vocabulary</li>
<li>Higher WER than fine-tuned (5% vs 3%)</li>
</ul>
<h2 id="when-each-strategy-makes-sense">When Each Strategy Makes
Sense</h2>
<h3 id="fine-tune-smalltiny-recommended-for-you">Fine-Tune Small/Tiny
(Recommended for you)</h3>
<p><strong>Choose when:</strong></p>
<ul>
<li>✓ You use STT frequently (daily)</li>
<li>✓ You value privacy/offline capability</li>
<li>✓ Your GPU can handle Small comfortably</li>
<li>✓ You can invest 1-2 days in training</li>
<li>✓ 6-8% WER is acceptable for your use case</li>
</ul>
<h3 id="fine-tune-medium">Fine-Tune Medium</h3>
<p><strong>Choose when:</strong></p>
<ul>
<li>✓ Your GPU can handle Medium well</li>
<li>✓ You want balance of accuracy and local inference</li>
<li>✓ Slightly slower inference is acceptable</li>
</ul>
<h3 id="fine-tune-large-api-deployment">Fine-Tune Large (API
deployment)</h3>
<p><strong>Choose when:</strong></p>
<ul>
<li>✗ You rarely use STT but need maximum accuracy when you do</li>
<li>✗ You’re okay with $50-100/month in API costs</li>
<li>✗ Privacy/offline not critical</li>
<li>✗ You need absolute best results</li>
</ul>
<p><strong>For your stated use case, Fine-Tune Small/Tiny is the
winner.</strong></p>
<h2 id="conclusion">Conclusion</h2>
<p><strong>Fine-tuning smaller models (Small/Tiny) for local inference
is a highly effective and practical strategy,</strong> especially
when:</p>
<ol type="1">
<li>Your GPU is limited (can’t comfortably run Large locally)</li>
<li>You use STT frequently (daily dictation, notes)</li>
<li>You value privacy and offline capability</li>
<li>You have specific vocabulary needs (Hebrew words, technical
terms)</li>
</ol>
<p><strong>Expected results:</strong></p>
<ul>
<li><strong>Fine-tuned Small:</strong> Matches or beats stock Medium
accuracy for YOUR voice</li>
<li><strong>Fine-tuned Tiny:</strong> Matches or beats stock Base/Small
for YOUR voice</li>
<li><strong>Practical benefits:</strong> Fast, local, private, offline,
cost-free ongoing use</li>
</ul>
<p><strong>This is MORE sensible than fine-tuning Large</strong> if you
can only run Large via API, because:</p>
<ul>
<li>Daily usability &gt; ultimate accuracy</li>
<li>Fine-tuned Small gets you “close enough” (6-8% vs 3-4% WER)</li>
<li>Local benefits (speed, privacy, offline, cost) outweigh marginal
accuracy gains</li>
</ul>
<p><strong>Recommendation:</strong> Fine-tune both Small (desktop) and
Tiny (phone) with the same training data, giving you optimized models
for each deployment target.</p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
<h2 id="how-fine-tuning-works-architecturally">How Fine Tuning Works
Architecturally</h2>
<h2 id="question-summary">Question Summary</h2>
<p>Daniel asks about the apparent conflict in fine-tuning: When you
fine-tune a large model like Whisper on a small custom dataset, you’re
essentially giving it “countervailing instructions” - telling it to
transcribe custom vocabulary differently while still maintaining its
general knowledge. The question is: How does the model architecturally
reconcile this conflict? How can a small amount of training data
meaningfully update a massive pre-trained model without destroying what
it already knows?</p>
<h2 id="answer">Answer</h2>
<p>Excellent question that gets to the heart of what makes modern
transfer learning work! You’ve correctly identified what seems like a
paradox: how can a tiny dataset (hours) meaningfully update a model
trained on massive data (thousands of hours) without either (a) being
completely overwhelmed or (b) destroying the original knowledge?</p>
<p>The answer involves several clever mechanisms happening at the
mathematical and architectural level. Let’s break it down.</p>
<h3
id="the-fundamental-tension-catastrophic-forgetting-vs.-effective-learning">The
Fundamental Tension: Catastrophic Forgetting vs. Effective Learning</h3>
<p><strong>The Problem You Identified:</strong></p>
<pre><code>Pre-trained Model (Whisper):
- 680,000 hours of training data
- Billions of parameters
- General knowledge: &quot;recognize&quot; → &quot;recognize&quot;

Your Fine-Tuning:
- 10 hours of training data
- Same parameters
- Specific knowledge: &quot;recognise&quot; → &quot;recognise&quot; (British spelling)

Conflict: How does the model remember both?</code></pre>
<p><strong>What Could Go Wrong:</strong></p>
<ol type="1">
<li><strong>Catastrophic Forgetting:</strong>
<ul>
<li>New training completely overwrites old knowledge</li>
<li>Model forgets how to transcribe normal speech</li>
<li>Only works on your specific data</li>
</ul></li>
<li><strong>No Learning:</strong>
<ul>
<li>Original weights too strong</li>
<li>Fine-tuning data too small to make a difference</li>
<li>No improvement on custom vocabulary</li>
</ul></li>
</ol>
<p><strong>What Actually Happens:</strong> Through careful tuning of
learning rates, freezing strategies, and mathematical properties of
gradient descent, the model finds a sweet spot where it: -
<strong>Preserves</strong> general knowledge in most parameters -
<strong>Adapts</strong> specific parameters for your domain -
<strong>Balances</strong> old and new knowledge</p>
<h3 id="the-mathematical-mechanics-how-fine-tuning-actually-works">The
Mathematical Mechanics: How Fine-Tuning Actually Works</h3>
<h4 id="level-1-gradient-descent-and-learning-rates"><strong>Level 1:
Gradient Descent and Learning Rates</strong></h4>
<p>At the most fundamental level, fine-tuning uses <strong>much smaller
learning rates</strong> than pre-training:</p>
<pre><code>Pre-training:
- Learning rate: 1e-3 to 1e-4 (0.001 to 0.0001)
- Large updates to weights
- Model parameters change significantly each batch

Fine-tuning:
- Learning rate: 1e-5 to 1e-6 (0.00001 to 0.000001)
- Tiny updates to weights (10-100x smaller)
- Model parameters change slightly</code></pre>
<p><strong>What This Means Mathematically:</strong></p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> old_weight <span class="op">-</span> (learning_rate × gradient)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>Pre<span class="op">-</span>training example:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> <span class="fl">0.5</span> <span class="op">-</span> (<span class="fl">0.001</span> × <span class="fl">2.0</span>) <span class="op">=</span> <span class="fl">0.498</span>  <span class="co"># 0.4% change</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>Fine<span class="op">-</span>tuning example:</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>new_weight <span class="op">=</span> <span class="fl">0.5</span> <span class="op">-</span> (<span class="fl">0.00001</span> × <span class="fl">2.0</span>) <span class="op">=</span> <span class="fl">0.49998</span>  <span class="co"># 0.004% change</span></span></code></pre></div>
<p><strong>Key Insight:</strong> Small learning rates mean your
fine-tuning makes <strong>small adjustments</strong> to existing weights
rather than replacing them. It’s like turning a dial slightly rather
than resetting it.</p>
<h4 id="level-2-loss-function-landscape"><strong>Level 2: Loss Function
Landscape</strong></h4>
<p>The pre-trained model has already found a “good valley” in the loss
landscape. Fine-tuning nudges it toward a nearby valley that’s even
better for your specific data.</p>
<pre><code>Visualizing Loss Landscape:

Before Fine-Tuning:
                    ╱╲
                   ╱  ╲
    ╱╲            ╱    ╲           ╱╲
   ╱  ╲          ╱  ●   ╲         ╱  ╲
  ╱    ╲________╱        ╲_______╱    ╲
              Pre-trained
              model position
              (good for general speech)

After Fine-Tuning:
                    ╱╲
                   ╱  ╲
    ╱╲            ╱ ●  ╲           ╱╲
   ╱  ╲          ╱      ╲         ╱  ╲
  ╱    ╲________╱        ╲_______╱    ╲
              Fine-tuned
              model position
              (great for your domain + good for general)

Model moves slightly within the valley, doesn&#39;t jump to a different valley</code></pre>
<p><strong>Why This Works:</strong> - Pre-training has done the “hard
work” of finding good representations - Fine-tuning just adjusts within
the same general region - Small dataset is sufficient for local
adjustment - Large dataset was needed to find the region in the first
place</p>
<h3 id="the-architectural-mechanisms-where-does-learning-happen">The
Architectural Mechanisms: Where Does Learning Happen?</h3>
<p>Not all parts of the model are equally affected by fine-tuning.
Here’s what happens in transformer models like Whisper:</p>
<h4 id="layer-wise-learning-dynamics"><strong>Layer-Wise Learning
Dynamics</strong></h4>
<pre><code>Whisper Architecture (Simplified):

Audio Input
    ↓
┌─────────────────────┐
│ Encoder Layers 1-4  │ ← Learn: Low-level audio features
│ (Early Layers)      │    (Phonemes, acoustics)
│                     │    Status: Mostly frozen by fine-tuning
└─────────────────────┘
    ↓
┌─────────────────────┐
│ Encoder Layers 5-24 │ ← Learn: Mid-level patterns
│ (Middle Layers)     │    (Words, prosody)
│                     │    Status: Slightly adjusted
└─────────────────────┘
    ↓
┌─────────────────────┐
│ Decoder Layers 1-24 │ ← Learn: Language patterns
│ (Decoder)           │    (Vocabulary, grammar, context)
│                     │    Status: Most fine-tuning happens here
└─────────────────────┘
    ↓
┌─────────────────────┐
│ Output Head         │ ← Learn: Token probabilities
│ (Final Layer)       │    Status: Heavy fine-tuning
└─────────────────────┘
    ↓
Text Output</code></pre>
<p><strong>What Gets Updated During Fine-Tuning:</strong></p>
<ol type="1">
<li><strong>Early encoder layers (1-4):</strong>
<ul>
<li>Learn basic audio features (spectral patterns, phonemes)</li>
<li>These are universal across languages/speakers</li>
<li><strong>Fine-tuning effect:</strong> Minimal (maybe 0.1-1% weight
change)</li>
<li><strong>Why:</strong> Your audio isn’t fundamentally different from
training audio</li>
</ul></li>
<li><strong>Middle encoder layers (5-24):</strong>
<ul>
<li>Learn word-level patterns, speaker characteristics</li>
<li>Some domain specificity</li>
<li><strong>Fine-tuning effect:</strong> Moderate (1-5% weight
change)</li>
<li><strong>Why:</strong> Your speaking style, vocabulary patterns
differ somewhat</li>
</ul></li>
<li><strong>Decoder layers (all):</strong>
<ul>
<li>Learn language model, vocabulary, context</li>
<li>Highly domain-specific</li>
<li><strong>Fine-tuning effect:</strong> Significant (5-15% weight
change)</li>
<li><strong>Why:</strong> This is where custom vocabulary lives</li>
</ul></li>
<li><strong>Output projection layer:</strong>
<ul>
<li>Maps to specific tokens/words</li>
<li>Most domain-specific</li>
<li><strong>Fine-tuning effect:</strong> Heavy (10-30% weight
change)</li>
<li><strong>Why:</strong> Direct mapping to your custom vocabulary</li>
</ul></li>
</ol>
<p><strong>Key Insight:</strong> Fine-tuning doesn’t update all
parameters equally. It makes large changes to task-specific parts
(decoder, output) and small changes to universal parts (early
encoder).</p>
<h3 id="advanced-technique-1-layer-freezing">Advanced Technique #1:
Layer Freezing</h3>
<p>Many fine-tuning approaches explicitly freeze early layers:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.encoder.layers[:<span class="dv">8</span>]:  <span class="co"># First 8 encoder layers</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> layer.parameters():</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span>  <span class="co"># Don&#39;t update these</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.encoder.layers[<span class="dv">8</span>:]:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> layer.parameters():</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># Update these</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> model.decoder.layers:</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> layer.parameters():</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># Update these</span></span></code></pre></div>
<p><strong>Effect:</strong> - ~50% of model parameters don’t change at
all - Remaining 50% get small updates (low learning rate) - Catastrophic
forgetting becomes nearly impossible - Your custom data only affects
relevant layers</p>
<h3 id="advanced-technique-2-lora-low-rank-adaptation">Advanced
Technique #2: LoRA (Low-Rank Adaptation)</h3>
<p>This is the cutting-edge approach for efficient fine-tuning:</p>
<p><strong>The Core Idea:</strong> Instead of updating all weights, add
small “adapter” matrices that capture your domain-specific
knowledge.</p>
<pre><code>Original Weight Matrix (W): [1024 × 1024]
- Pre-trained weights (frozen, never updated)

LoRA Adapter Matrices:
- A: [1024 × 8] (small rank)
- B: [8 × 1024]
- Product A×B: [1024 × 1024] (same size as W)

Final Computation:
output = (W + α × A × B) × input

Where:
- W remains frozen (original knowledge preserved)
- Only A and B are trained (tiny fraction of parameters)
- α is a scaling factor (typically 0.01-0.1)</code></pre>
<p><strong>The Math:</strong></p>
<pre><code>Parameters in Full Fine-Tuning:
- Original matrix W: 1024 × 1024 = 1,048,576 parameters
- All must be updated

Parameters in LoRA:
- Matrix A: 1024 × 8 = 8,192 parameters
- Matrix B: 8 × 1024 = 8,192 parameters
- Total: 16,384 parameters (1.5% of original!)

Result: 98.5% of parameters stay frozen, 1.5% capture your domain knowledge</code></pre>
<p><strong>How This Solves Your Question:</strong></p>
<pre><code>Original Knowledge (W):
&quot;recognize&quot; → &quot;recognize&quot; (American spelling)

LoRA Adapter (A×B):
Adds slight bias: &quot;recognise&quot; → &quot;recognise&quot; (British spelling)

Combined (W + A×B):
- Still recognizes American spelling (W unchanged)
- Also handles British spelling (A×B adds this capability)
- No conflict, additive knowledge!</code></pre>
<p><strong>Why LoRA Works So Well:</strong></p>
<ol type="1">
<li><strong>Mathematically elegant:</strong>
<ul>
<li>Additions don’t destroy original weights</li>
<li>Small rank (8-16) is sufficient for most domain adaptations</li>
<li>α scaling factor controls how much domain knowledge influences
output</li>
</ul></li>
<li><strong>Preserves original knowledge:</strong>
<ul>
<li>W never changes → general knowledge intact</li>
<li>A×B is small → can’t overwhelm original model</li>
</ul></li>
<li><strong>Efficient:</strong>
<ul>
<li>100x fewer trainable parameters</li>
<li>Faster training, less memory</li>
<li>Can store multiple LoRA adapters for different domains</li>
</ul></li>
</ol>
<h3 id="how-the-model-reconciles-conflicting-information">How the Model
Reconciles Conflicting Information</h3>
<p>Let’s trace through a specific example:</p>
<p><strong>Scenario:</strong> You’re fine-tuning Whisper on British
English with medical terminology.</p>
<pre><code>Input Audio: &quot;The patient recognises colorectal abnormalities&quot;

Pre-trained Whisper (Before Fine-Tuning):
- Would transcribe: &quot;The patient recognizes colorectal abnormalities&quot;
- Issue: &quot;recognizes&quot; (American) vs &quot;recognises&quot; (British)

What Happens During Fine-Tuning:

1. Encoder processes audio → acoustic features (unchanged by fine-tuning)

2. Decoder generates tokens:

   Token: &quot;recognizes&quot; vs &quot;recognises&quot;

   Pre-trained weight says:
   P(&quot;recognizes&quot;) = 0.85
   P(&quot;recognises&quot;) = 0.15

   Fine-tuning gradient pushes:
   P(&quot;recognizes&quot;) = 0.85 → 0.40 (decreased)
   P(&quot;recognises&quot;) = 0.15 → 0.60 (increased)

   After fine-tuning:
   P(&quot;recognizes&quot;) = 0.40 (still possible!)
   P(&quot;recognises&quot;) = 0.60 (now preferred)

3. With LoRA:

   W says: P(&quot;recognizes&quot;) = 0.85
   A×B adds: +0.45 to P(&quot;recognises&quot;)

   Combined:
   P(&quot;recognizes&quot;) = 0.85 (from W)
   P(&quot;recognises&quot;) = 0.15 + 0.45 = 0.60 (from W + A×B)

   Model chooses &quot;recognises&quot; but hasn&#39;t &quot;forgotten&quot; &quot;recognizes&quot;!</code></pre>
<p><strong>Key Insight:</strong> The model doesn’t replace knowledge, it
adds context-dependent preferences.</p>
<h3 id="the-role-of-batch-normalization-and-layer-normalization">The
Role of Batch Normalization and Layer Normalization</h3>
<p>Another architectural component that helps:</p>
<pre><code>Each transformer layer has normalization:

Input → Attention → LayerNorm → FeedForward → LayerNorm → Output

LayerNorm parameters:
- Scale (γ): learned multiplier
- Shift (β): learned offset

During fine-tuning:
- Main weights (attention, feedforward) change slightly
- Normalization parameters (γ, β) change more significantly
- These small normalization parameters can &quot;steer&quot; the model&#39;s behavior
- Without changing fundamental representations</code></pre>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>normalized <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> std  <span class="co"># Normalize to mean=0, std=1</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> γ × normalized <span class="op">+</span> β     <span class="co"># Scale and shift</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>Pre<span class="op">-</span>training: γ <span class="op">=</span> <span class="fl">1.0</span>, β <span class="op">=</span> <span class="fl">0.0</span> (no transformation)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>Fine<span class="op">-</span>tuning: γ <span class="op">=</span> <span class="fl">1.2</span>, β <span class="op">=</span> <span class="fl">0.3</span> (slight transformation)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>Effect: Amplifies certain features (via γ) <span class="kw">and</span> shifts baseline (via β)</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>Without changing the features themselves<span class="op">!</span></span></code></pre></div>
<h3
id="regularization-preventing-catastrophic-forgetting">Regularization:
Preventing Catastrophic Forgetting</h3>
<p>Several techniques explicitly prevent the model from diverging too
much:</p>
<h4 id="weight-decay-l2-regularization"><strong>1. Weight Decay (L2
Regularization)</strong></h4>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>total_loss <span class="op">=</span> task_loss <span class="op">+</span> λ × weight_decay_term</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>weight_decay_term <span class="op">=</span> Σ (w <span class="op">-</span> w_pretrained)²</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>Effect:</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Penalizes weights that drift far <span class="im">from</span> pre<span class="op">-</span>trained values</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Keeps model <span class="st">&quot;anchored&quot;</span> to original knowledge</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> λ controls how strongly (typically λ <span class="op">=</span> <span class="fl">0.01</span>)</span></code></pre></div>
<h4 id="elastic-weight-consolidation-ewc"><strong>2. Elastic Weight
Consolidation (EWC)</strong></h4>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>ewc_loss <span class="op">=</span> Σ F_i × (w_i <span class="op">-</span> w_pretrained_i)²</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>Where F_i <span class="op">=</span> importance of weight i (<span class="im">from</span> pre<span class="op">-</span>training)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>Effect:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Weights important <span class="cf">for</span> general task: heavily penalized (don<span class="st">&#39;t change much)</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="er">- Weights less important: can change more freely</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> Model preserves critical knowledge, adapts peripheral knowledge</span></code></pre></div>
<h3
id="practical-example-fine-tuning-whisper-on-technical-vocabulary">Practical
Example: Fine-Tuning Whisper on Technical Vocabulary</h3>
<p>Let’s walk through what actually happens:</p>
<pre><code>Your Dataset:
- 10 hours of you speaking about machine learning
- Technical terms: &quot;PyTorch&quot;, &quot;CUDA&quot;, &quot;embeddings&quot;, &quot;hyperparameters&quot;

Whisper Pre-trained:
- Might transcribe: &quot;pie torch&quot;, &quot;CUDA&quot; ✓, &quot;embeddings&quot; ✓, &quot;hyper parameters&quot;
- Issues with: PyTorch (not in training data), hyperparameters (splits it)

Fine-Tuning Process:

Epoch 1:
- Model sees &quot;PyTorch&quot; in your audio + transcript
- Gradient: Increase P(&quot;PyTorch&quot;), decrease P(&quot;pie torch&quot;)
- Learning rate tiny (1e-6), so weights change by ~0.01%
- 100 examples of &quot;PyTorch&quot; → cumulative change ~1%

Epoch 5:
- Model has seen &quot;PyTorch&quot; 500 times
- Cumulative weight changes ~5%
- Now: P(&quot;PyTorch&quot;) = 0.90, P(&quot;pie torch&quot;) = 0.10

Final Model:
- In ML context: confidently transcribes &quot;PyTorch&quot;
- In baking context: might still transcribe &quot;pie torch&quot;!
- Context matters, model learns conditional preferences

Regular vocabulary:
- Words like &quot;the&quot;, &quot;and&quot;, &quot;is&quot; seen 10,000 times in your data
- But also seen 10,000,000 times in pre-training
- Fine-tuning is 0.1% of total exposure
- These weights barely change</code></pre>
<h3 id="the-final-picture-how-it-all-fits-together">The Final Picture:
How It All Fits Together</h3>
<pre><code>┌─────────────────────────────────────────────┐
│ Pre-trained Model (Whisper)                 │
│ - 680,000 hours of training                 │
│ - Billions of parameters                     │
│ - General knowledge encoded in weights      │
└─────────────────────────────────────────────┘
                    │
                    │ Fine-tuning with:
                    │ - Low learning rate (1e-5)
                    │ - Layer freezing (early layers)
                    │ - LoRA adapters (optional)
                    │ - Regularization (weight decay)
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ Fine-tuned Model                            │
│                                             │
│ Early Layers: ~99% unchanged                │
│ - Still recognize basic audio features     │
│                                             │
│ Middle Layers: ~95% unchanged               │
│ - Slight adjustments for your voice/style  │
│                                             │
│ Late Layers: ~85-90% unchanged              │
│ - Learned your vocabulary patterns         │
│                                             │
│ Output Layer: ~70-80% unchanged             │
│ - Custom vocabulary probabilities updated  │
│                                             │
│ Result: General knowledge + Domain expertise│
└─────────────────────────────────────────────┘</code></pre>
<h3 id="answering-your-core-question">Answering Your Core Question</h3>
<p><strong>“How do you take a small weight and counter it against a big
model to get the desired outcome?”</strong></p>
<p>The answer has several layers:</p>
<ol type="1">
<li><p><strong>Small learning rates:</strong> Updates are tiny (0.01-1%
per weight), so small data can’t overwrite large model</p></li>
<li><p><strong>Layer freezing:</strong> 50-70% of model doesn’t train at
all, only domain-relevant parts update</p></li>
<li><p><strong>LoRA adapters:</strong> Add small correction matrices
instead of changing original weights</p></li>
<li><p><strong>Regularization:</strong> Explicitly penalize divergence
from pre-trained weights</p></li>
<li><p><strong>Selective updating:</strong> Layers closer to output
(where domain knowledge matters) change more than early layers
(universal features)</p></li>
<li><p><strong>Additive learning:</strong> New knowledge is added to
existing knowledge, not replacing it</p></li>
<li><p><strong>Context-dependent behavior:</strong> Model learns
<em>when</em> to use custom vocabulary (in your context) vs. general
vocabulary</p></li>
</ol>
<p><strong>The Metaphor:</strong> Think of the pre-trained model as a
master chef with 40 years of experience. Fine-tuning is like teaching
them a new regional cuisine: - They don’t forget how to cook (general
knowledge intact) - They learn new spices and techniques
(domain-specific knowledge added) - They adjust their intuition slightly
(small weight updates) - They know when to use new vs. traditional
techniques (context-dependent) - 40 years of experience isn’t
overwritten by 2 weeks of training!</p>
<h3 id="conclusion-1">Conclusion</h3>
<p>Your intuition about conflict is correct, but the model doesn’t
experience it as conflict - it experiences it as
<em>refinement</em>:</p>
<ul>
<li><strong>Pre-training:</strong> Learn the general structure of
language and speech (broad, shallow knowledge)</li>
<li><strong>Fine-tuning:</strong> Deepen knowledge in specific domain
(narrow, deep knowledge)</li>
<li><strong>Result:</strong> Model that’s expert in your domain but
competent everywhere else</li>
</ul>
<p>The “small weight countering big model” works because: 1. It’s not
really counter - it’s additive/corrective 2. Learning rates are tuned to
prevent overwriting 3. Architecture separates general features (early)
from specific features (late) 4. Regularization explicitly preserves
pre-trained knowledge 5. Mathematical properties of gradient descent
favor local minima near pre-trained weights</p>
<p>This is why transfer learning is so powerful: you get the best of
both worlds with remarkably little data!</p>
<hr />
<p><em>Note: This response was generated by Claude Code as part of
Daniel’s STT Fine-Tuning Notebook project. Technical explanations are
based on deep learning research, transformer architecture design, and
fine-tuning best practices.</em></p>
<h2 id="mission-critical-enterprise-asr-implementation">Mission Critical
Enterprise Asr Implementation</h2>
<h2 id="question-summary-1">Question Summary</h2>
<p>Daniel asks about enterprise-level ASR implementation in
mission-critical contexts (air traffic control, medical transcription,
etc.) where accuracy is paramount and budgets are essentially unlimited.
The question explores: what do these organizations actually do to
achieve the absolute best ASR performance? Do they fine-tune models or
use pre-existing specialist datasets? What does the implementation
process look like, where do they turn for help, and what timelines are
involved?</p>
<h2 id="answer-1">Answer</h2>
<p>Excellent question that gets at the difference between
hobbyist/individual ASR fine-tuning and enterprise mission-critical
deployments. The approach for organizations where errors can have
life-or-death consequences is fundamentally different from typical
implementations.</p>
<h3 id="what-organizations-actually-do-the-enterprise-reality">What
Organizations Actually Do: The Enterprise Reality</h3>
<p><strong>Short Answer:</strong> They almost always build heavily
customized, domain-specific ASR systems through a combination of: 1.
Custom data collection and curation 2. Fine-tuning (or full training) on
domain-specific data 3. Extensive human-in-the-loop verification 4.
Multi-model ensemble approaches 5. Continuous monitoring and
retraining</p>
<p><strong>They do NOT:</strong> Simply use off-the-shelf Whisper or
commercial APIs and call it done.</p>
<h3 id="mission-critical-asr-use-cases">Mission-Critical ASR Use
Cases</h3>
<p>Let’s examine specific examples:</p>
<h4 id="air-traffic-control-atc"><strong>Air Traffic Control
(ATC)</strong></h4>
<ul>
<li><strong>Error tolerance:</strong> Effectively zero</li>
<li><strong>Challenges:</strong>
<ul>
<li>Highly specialized vocabulary (aviation phraseology)</li>
<li>Critical proper nouns (airport codes, callsigns)</li>
<li>Background noise (radio static, cockpit noise)</li>
<li>Multilingual speakers with varied accents</li>
<li>Life-or-death consequences for errors</li>
</ul></li>
<li><strong>What they do:</strong>
<ul>
<li>Custom datasets recorded from actual ATC communications</li>
<li>Fine-tune on specific controller voices and regional accents</li>
<li>Domain-specific language models (aviation phraseology)</li>
<li>Real-time confidence scoring with human override</li>
<li>Regulatory certification requirements (FAA, EASA)</li>
</ul></li>
<li><strong>Providers:</strong>
<ul>
<li><strong>Saab Sensis</strong> (specialized ATC ASR systems)</li>
<li><strong>Thales</strong> (aviation communication systems)</li>
<li><strong>Raytheon</strong> (integrated ATC solutions)</li>
<li>Custom in-house systems with research partnerships (NASA, MIT
Lincoln Labs)</li>
</ul></li>
</ul>
<h4 id="medical-transcription"><strong>Medical
Transcription</strong></h4>
<ul>
<li><strong>Error tolerance:</strong> Very low (HIPAA, patient
safety)</li>
<li><strong>Challenges:</strong>
<ul>
<li>Extensive medical terminology</li>
<li>Drug names (sound-alikes are dangerous: “Celebrex” vs
“Cerebyx”)</li>
<li>Anatomical terms, procedures, diagnoses</li>
<li>Physician accents and speaking styles</li>
<li>Integration with EHR systems</li>
</ul></li>
<li><strong>What they do:</strong>
<ul>
<li>Specialty-specific models (radiology, cardiology, pathology)</li>
<li>Custom vocabularies for institutions</li>
<li>Human transcriptionist review (ASR-assisted workflow)</li>
<li>Continuous learning from corrections</li>
<li>HIPAA-compliant on-premise deployment</li>
</ul></li>
<li><strong>Providers:</strong>
<ul>
<li><strong>Nuance Dragon Medical</strong> (market leader, recently
acquired by Microsoft)</li>
<li>**3M M*Modal** (competitor to Nuance)</li>
<li><strong>Suki.ai</strong> (newer AI-first approach)</li>
<li><strong>Amazon Transcribe Medical</strong></li>
<li>In-house systems at major health systems (Mayo Clinic, Cleveland
Clinic)</li>
</ul></li>
</ul>
<h4 id="legal-transcription-court-reporting"><strong>Legal Transcription
(Court Reporting)</strong></h4>
<ul>
<li><strong>Error tolerance:</strong> Low (legal record accuracy)</li>
<li><strong>Challenges:</strong>
<ul>
<li>Legal terminology</li>
<li>Multiple speakers with overlapping speech</li>
<li>Proper nouns (names, locations, organizations)</li>
<li>Verbatim accuracy requirements (including fillers, pauses)</li>
</ul></li>
<li><strong>What they do:</strong>
<ul>
<li>Specialized court reporting ASR systems</li>
<li>Real-time stenographer augmentation (not replacement)</li>
<li>Speaker diarization critical</li>
<li>Verbatim transcription (can’t clean up grammar)</li>
</ul></li>
<li><strong>Providers:</strong>
<ul>
<li><strong>Verbit</strong> (AI court reporting)</li>
<li><strong>Rev.ai</strong> (professional transcription with high
accuracy)</li>
<li>Traditional court reporters with ASR assistance</li>
</ul></li>
</ul>
<h3 id="the-typical-implementation-process-for-mission-critical-asr">The
Typical Implementation Process for Mission-Critical ASR</h3>
<p>Here’s what an organization with “unlimited budget” and paramount
accuracy requirements actually does:</p>
<h4 id="phase-1-requirements-planning-3-6-months"><strong>Phase 1:
Requirements &amp; Planning (3-6 months)</strong></h4>
<p><strong>Step 1: Define Requirements</strong></p>
<pre><code>- Target WER: Usually &lt;2-5% for mission-critical (vs. 10-15% for general use)
- Domain scope: Specific terminology, vocabulary size
- Speaker demographics: Accents, languages, voice types
- Environmental conditions: Noise profiles, channel characteristics
- Latency requirements: Real-time vs. batch processing
- Regulatory requirements: HIPAA, FAA certification, ISO compliance
- Integration requirements: EHR, ATC systems, etc.</code></pre>
<p><strong>Step 2: Feasibility Study</strong></p>
<pre><code>- Benchmark existing solutions (commercial APIs, open-source models)
- Test with domain-specific data samples
- Establish baseline WER on realistic test cases
- Identify gap between current SOTA and requirements
- Budget allocation: $500K-5M+ for initial development</code></pre>
<p><strong>Step 3: Build vs. Buy Decision</strong></p>
<pre><code>Option A: Commercial Specialist Provider
- Nuance, 3M, Saab (domain-specific solutions)
- Pro: Faster deployment, regulatory compliance built-in
- Con: Less customization, ongoing licensing costs
- Timeline: 6-12 months to full deployment

Option B: Custom Development
- Partner with research institution or specialized consultancy
- Pro: Maximum customization, IP ownership
- Con: Longer timeline, higher risk
- Timeline: 18-36 months to full deployment

Option C: Hybrid
- Start with commercial solution
- Supplement with custom fine-tuning
- Most common for large organizations
- Timeline: 12-18 months</code></pre>
<h4 id="phase-2-data-collection-curation-6-18-months"><strong>Phase 2:
Data Collection &amp; Curation (6-18 months)</strong></h4>
<p>This is where mission-critical differs dramatically from typical
ASR:</p>
<p><strong>Step 1: Data Collection Strategy</strong></p>
<p>Organizations do NOT rely on public datasets. They collect
proprietary data:</p>
<pre><code>Medical Transcription Example:

Data Sources:
- Recorded physician dictations (with consent)
- De-identified patient encounters
- Simulated clinical scenarios (actors)
- Partnerships with medical schools
- Purchased specialty-specific datasets

Target Volume:
- Minimum: 500-1,000 hours per specialty
- Optimal: 5,000+ hours
- Distribution: Balanced across specialties, physician demographics

Data Characteristics:
- Real-world audio quality (office noise, phone quality)
- Diverse accents and speaking styles
- Full coverage of medical vocabulary
- Varied patient scenarios</code></pre>
<p><strong>Step 2: Transcript Quality</strong></p>
<p>Mission-critical applications require gold-standard transcripts:</p>
<pre><code>Transcription Process:
1. Professional transcriptionists create initial transcript
2. Domain expert review (e.g., physician reviews medical transcripts)
3. Second-pass QA for consistency
4. Triple-check on medical terminology, drug names
5. Final validation: &lt;0.5% error rate on ground truth

Cost: $1-3 per audio minute (vs. $0.10-0.25 for standard transcription)
Timeline: 2-3x longer than standard transcription</code></pre>
<p><strong>Step 3: Data Augmentation</strong></p>
<pre><code>Techniques:
- Noise injection (specific to target environment)
- Speed perturbation
- Channel simulation (phone, radio, microphone types)
- Accent augmentation
- Synthetic data generation (TTS with domain vocabulary)

Purpose: Increase robustness without collecting more real data</code></pre>
<h4 id="phase-3-model-development-6-12-months"><strong>Phase 3: Model
Development (6-12 months)</strong></h4>
<p><strong>Approach 1: Fine-Tuning SOTA Models (Most
Common)</strong></p>
<pre><code>Starting Point:
- Whisper-large-v3 (current SOTA for many domains)
- Wav2Vec 2.0 (for low-latency requirements)
- Canary (NVIDIA, good for specialized domains)

Fine-Tuning Process:
1. Start with multilingual/general model
2. Continue pre-training on domain-specific audio (no transcripts needed)
3. Fine-tune on curated domain-specific dataset
4. Optimize for specific acoustic conditions
5. Integration with domain-specific language model

Timeline: 3-6 months
Compute Cost: $50K-200K (using cloud GPU clusters)</code></pre>
<p><strong>Approach 2: Custom Model Architecture (Less
Common)</strong></p>
<pre><code>When Used:
- Existing models fundamentally unsuited (e.g., extreme latency requirements)
- Unique acoustic characteristics
- Regulatory requirements mandate explainability

Process:
- Custom architecture design
- Training from scratch on proprietary data
- Extensive validation and testing

Timeline: 12-18 months
Cost: $500K-2M+
Examples: Proprietary ATC systems, military applications</code></pre>
<p><strong>Approach 3: Ensemble Systems (High-End Approach)</strong></p>
<pre><code>Architecture:
- Multiple models running in parallel
  - Whisper-large-v3 (general robustness)
  - Domain-specific fine-tuned model
  - Specialty-focused model (e.g., drug names for medical)
- Confidence-weighted voting
- Fallback to human review when models disagree

Advantages:
- Higher accuracy (1-2% WER improvement)
- Robustness to edge cases
- Better uncertainty quantification

Disadvantages:
- 3-5x inference cost
- More complex deployment

Used by: Top-tier medical institutions, critical ATC systems</code></pre>
<h4 id="phase-4-language-model-integration-2-4-months"><strong>Phase 4:
Language Model Integration (2-4 months)</strong></h4>
<p>Mission-critical systems don’t just use acoustic models; they heavily
leverage language models:</p>
<pre><code>Domain-Specific Language Model:

Medical Example:
- Custom vocabulary (100K+ medical terms)
- Contextual priors:
  - &quot;Celebrex&quot; much more likely than &quot;Cerebyx&quot; in arthritis context
  - &quot;2 milligrams&quot; vs. &quot;too many grams&quot; (catastrophic if wrong)
- Institution-specific terminology
- Physician-specific patterns (Dr. Smith always says &quot;unremarkable&quot; not &quot;normal&quot;)

Implementation:
- Custom language model trained on domain text
  - Medical journals, textbooks, clinical notes
  - 10M-100M domain-specific words
- Integration with ASR decoder
- Contextual biasing for current case (patient history, current diagnosis)

WER Improvement: 20-40% relative reduction (e.g., 10% → 6% WER)</code></pre>
<h4 id="phase-5-testing-validation-6-12-months"><strong>Phase 5: Testing
&amp; Validation (6-12 months)</strong></h4>
<p>Mission-critical systems undergo exhaustive testing:</p>
<pre><code>Testing Phases:

1. Lab Testing (2-3 months)
   - Controlled environment
   - Test suite: 100+ hours representative data
   - Target: &lt;3% WER on test set

2. Pilot Deployment (3-6 months)
   - Limited users in real environment
   - Human-in-the-loop verification
   - Collect error cases and retrain
   - Iterative improvement

3. Shadow Deployment (3-6 months)
   - Run in parallel with existing system
   - Compare outputs, identify discrepancies
   - Build confidence in system reliability

4. Staged Rollout (6-12 months)
   - 10% of users → 50% → 100%
   - Continuous monitoring
   - Rapid response to issues

Total Testing Timeline: 12-24 months (overlaps with development)</code></pre>
<h4 id="phase-6-deployment-integration-4-8-months"><strong>Phase 6:
Deployment &amp; Integration (4-8 months)</strong></h4>
<p><strong>Infrastructure Requirements:</strong></p>
<pre><code>On-Premise Deployment (Typical for HIPAA/Sensitive Data):
- GPU clusters for inference
  - Medical center: 10-50 GPUs
  - Major hospital network: 100+ GPUs
- Redundancy and failover
- HIPAA-compliant data handling
- Integration with existing systems (EHR, PACS, etc.)

Cost: $500K-2M for hardware + infrastructure
Ongoing: $200K-500K/year for maintenance, updates</code></pre>
<p><strong>Cloud Deployment (Where Permissible):</strong></p>
<pre><code>- AWS, Azure, or GCP with compliance certifications
- Dedicated tenancy for security
- Auto-scaling for load
- Global deployment for multi-site organizations

Cost: $50K-300K/year depending on volume</code></pre>
<h4 id="phase-7-continuous-improvement-ongoing"><strong>Phase 7:
Continuous Improvement (Ongoing)</strong></h4>
<p>Mission-critical systems are never “done”:</p>
<pre><code>Ongoing Activities:

1. Error Monitoring (Daily)
   - Track WER on production data
   - Flag unusual errors for review
   - Identify drift in performance

2. Retraining (Quarterly/Annually)
   - Incorporate corrected transcripts
   - Add new vocabulary (e.g., new drugs)
   - Adapt to new speakers
   - Update for new procedures/terminology

3. Model Updates (Annually)
   - Retrain on expanded dataset
   - Incorporate new SOTA techniques
   - Benchmark against latest commercial offerings

4. User Feedback Loop
   - Clinicians/controllers report errors
   - Domain experts review and correct
   - Corrections fed back into training

Annual Cost: $100K-500K for continuous improvement</code></pre>
<h3 id="where-organizations-turn-for-implementation">Where Organizations
Turn for Implementation</h3>
<p><strong>Tier 1: Commercial Specialists (Most Common)</strong></p>
<p>Medical: - <strong>Nuance Dragon Medical One</strong> (market leader)
- Cost: $1,500-3,000 per user/year - Includes specialty vocabularies,
continuous updates - HIPAA-compliant cloud or on-premise - **3M M*Modal
Fluency Direct** - Competitor to Nuance - Similar pricing and
capabilities</p>
<p>Legal: - <strong>Verbit</strong> - <strong>Rev.ai
Professional</strong></p>
<p>Aviation/ATC: - <strong>Saab Sensis</strong> -
<strong>Thales</strong></p>
<p><strong>Tier 2: Specialized Consultancies &amp; Research
Partners</strong></p>
<p>For custom development: - <strong>SoapBox Labs</strong> (specialized
in difficult acoustic conditions) - <strong>AssemblyAI</strong> (custom
model development) - <strong>Deepgram</strong> (custom voice AI
solutions) - University research partnerships (CMU, MIT, Stanford speech
labs) - Defense contractors (for government/military applications)</p>
<p>Cost: $500K-5M for custom development project</p>
<p><strong>Tier 3: In-House with Cloud Provider APIs</strong></p>
<p>Large tech-forward organizations: - Start with AWS Transcribe
Medical, Google Medical LM - Heavily customize with fine-tuning - Build
internal ML teams (10-50 people) - Examples: Cleveland Clinic, Kaiser
Permanente, large EHR vendors</p>
<p><strong>Tier 4: Full Custom (Rare)</strong></p>
<p>Only for: - Government/military (national security requirements) -
Unique requirements not met by commercial options - Organizations with
&gt;$10M budgets for speech systems</p>
<p>Partner with: - DARPA research programs - National labs (Lincoln
Labs, etc.) - Top-tier university research groups</p>
<h3 id="timeline-summary">Timeline Summary</h3>
<p><strong>Fast Track (Commercial Solution):</strong></p>
<pre><code>Month 0-3:    Requirements, vendor selection
Month 3-6:    Pilot deployment, initial testing
Month 6-12:   Integration, training, rollout
Month 12-18:  Full deployment, optimization

Total: 18 months to full deployment</code></pre>
<p><strong>Custom Development (Typical):</strong></p>
<pre><code>Month 0-6:    Planning, feasibility, data collection start
Month 6-18:   Data curation, initial model development
Month 18-24:  Model fine-tuning, language model integration
Month 24-36:  Testing, validation, pilot deployment
Month 36-48:  Staged rollout, continuous improvement

Total: 3-4 years to mature deployment</code></pre>
<p><strong>Hybrid Approach (Recommended for Most):</strong></p>
<pre><code>Month 0-6:    Deploy commercial solution as baseline
Month 6-12:   Collect domain-specific data
Month 12-24:  Develop custom fine-tuned models
Month 24-30:  A/B test custom vs. commercial
Month 30-36:  Migrate to hybrid system (custom + commercial fallback)

Total: 2-3 years to optimized deployment</code></pre>
<h3 id="cost-breakdown-example-large-hospital-system">Cost Breakdown
Example: Large Hospital System</h3>
<p>Implementing mission-critical medical transcription ASR:</p>
<pre><code>Year 1 (Planning &amp; Initial Deployment):
- Commercial solution licensing (500 physicians): $750K
- Integration with EHR systems: $300K
- Training and change management: $200K
- Infrastructure (servers, support): $150K
Total: $1.4M

Year 2-3 (Custom Development):
- Data collection and curation: $500K
- Model development (consultancy): $800K
- Testing and validation: $400K
- Additional compute/infrastructure: $200K
Total: $1.9M

Ongoing (Annual):
- Commercial licensing: $750K
- Maintenance and updates: $300K
- Continuous improvement: $200K
- Infrastructure: $150K
Total: $1.4M/year

Total 5-Year Cost: ~$8.5M
Cost per Physician: ~$17K over 5 years ($3.4K/year)

ROI:
- Physician time saved: 30 min/day
- Value: ~$50K/physician/year
- Break-even: ~1 year</code></pre>
<h3
id="do-organizations-fine-tune-or-use-pre-existing-specialist-datasets">Do
Organizations Fine-Tune or Use Pre-Existing Specialist Datasets?</h3>
<p><strong>The answer: Both, sequentially</strong></p>
<ol type="1">
<li><strong>Start with pre-existing specialist datasets</strong> (if
available):
<ul>
<li>Medical: CommonVoice Medical, medical podcast datasets</li>
<li>Legal: Court transcription datasets</li>
<li>Limited availability for most domains</li>
</ul></li>
<li><strong>Rapidly collect custom data:</strong>
<ul>
<li>Pre-existing datasets provide starting point</li>
<li>Custom data essential for achieving &lt;5% WER</li>
<li>Typical: 70% custom data, 30% public/specialist data</li>
</ul></li>
<li><strong>Fine-tune progressively:</strong>
<ul>
<li>Stage 1: General model → domain fine-tune (public data)</li>
<li>Stage 2: Domain model → institution-specific fine-tune (custom
data)</li>
<li>Stage 3: Continuous fine-tuning with production corrections</li>
</ul></li>
</ol>
<p><strong>Key Insight:</strong> Pre-existing specialist datasets are
insufficient for mission-critical applications. Custom data collection
is non-negotiable for achieving required accuracy.</p>
<h3 id="why-not-just-use-openai-whisper-or-commercial-apis">Why Not Just
Use OpenAI Whisper or Commercial APIs?</h3>
<p>Organizations with unlimited budgets don’t just use off-the-shelf
solutions because:</p>
<ol type="1">
<li><strong>Accuracy Gap:</strong>
<ul>
<li>Whisper on medical: 15-20% WER</li>
<li>Custom fine-tuned: 3-5% WER</li>
<li>Required: &lt;3% WER</li>
<li>Gap too large for mission-critical use</li>
</ul></li>
<li><strong>Domain Vocabulary:</strong>
<ul>
<li>General models lack comprehensive medical/aviation/legal
terminology</li>
<li>Drug names, airport codes, legal terms require specialized
training</li>
</ul></li>
<li><strong>Data Privacy:</strong>
<ul>
<li>HIPAA prohibits sending patient data to external APIs</li>
<li>ATC communications are sensitive</li>
<li>Must be on-premise or private cloud</li>
</ul></li>
<li><strong>Latency Requirements:</strong>
<ul>
<li>Commercial APIs: 2-5 second latency</li>
<li>Real-time requirements: &lt;500ms</li>
<li>Requires local deployment</li>
</ul></li>
<li><strong>Regulatory Compliance:</strong>
<ul>
<li>FAA certification for ATC systems</li>
<li>FDA clearance for medical devices</li>
<li>Commercial APIs don’t meet regulatory requirements</li>
</ul></li>
<li><strong>Cost at Scale:</strong>
<ul>
<li>Large hospital: 10M+ minutes/year</li>
<li>Commercial API: $0.006/minute = $60K/year (cheap!)</li>
<li>But: accuracy insufficient, privacy concerns override cost</li>
</ul></li>
</ol>
<h3 id="conclusion-the-mission-critical-asr-reality">Conclusion: The
Mission-Critical ASR Reality</h3>
<p>For organizations where accuracy is paramount:</p>
<ol type="1">
<li><strong>They almost always fine-tune</strong>, and extensively</li>
<li><strong>Custom data collection is mandatory</strong> (not
optional)</li>
<li><strong>Implementation takes 2-4 years</strong> (not months)</li>
<li><strong>Costs range $2M-10M+</strong> for initial deployment</li>
<li><strong>Continuous improvement is ongoing</strong>
($200K-500K/year)</li>
<li><strong>They use specialist providers</strong> (Nuance, 3M) or large
consultancies</li>
<li><strong>Pre-existing datasets are starting points</strong>, not
solutions</li>
<li><strong>Human-in-the-loop remains essential</strong>, even with best
ASR</li>
</ol>
<p><strong>The process is:</strong> Commercial baseline → Custom data →
Fine-tuning → Testing → Deployment → Continuous improvement</p>
<p><strong>Key Differentiator:</strong> Mission-critical organizations
treat ASR as a long-term platform investment, not a one-time
implementation. They build continuous improvement pipelines and treat
&lt;5% WER as the starting point, not the goal.</p>
<hr />
<p><em>Note: This response was generated by Claude Code as part of
Daniel’s STT Fine-Tuning Notebook project. Information is based on
industry practices, published case studies, and vendor documentation.
Specific costs and timelines vary significantly by organization size and
requirements.</em></p>
<h2 id="personal-voice-finetuning-rationale">Personal Voice Finetuning
Rationale</h2>
<h2 id="question">Question</h2>
<p>Is fine-tuning an ASR model on your unique voice, accent, and
mixed-language patterns (Hebrew/English code-switching, technical
vocabulary) a legitimate reason for fine-tuning, even if the accuracy
improvement is modest?</p>
<h2 id="answer-2">Answer</h2>
<p><strong>Short answer: Yes, absolutely—and it’s probably more valuable
than you think.</strong></p>
<p>Your use case is not only legitimate but represents an emerging and
increasingly common fine-tuning pattern: <strong>personalized ASR
adaptation</strong>. Let’s break down why this matters.</p>
<hr />
<h2 id="why-personal-voice-fine-tuning-is-valuable">Why Personal Voice
Fine-Tuning Is Valuable</h2>
<h3 id="code-switching-and-mixed-language-use">1. <strong>Code-Switching
and Mixed-Language Use</strong></h3>
<p>Your scenario (English with Hebrew words) is exactly where
general-purpose models struggle:</p>
<p><strong>Whisper’s Challenge:</strong> - Trained on separate language
corpuses - Switches between English/Hebrew detection based on dominant
language - Can’t handle mid-sentence language switching gracefully -
Hebrew words get either: - Mistranscribed as phonetically similar
English words - Forced into Hebrew transcription mode (breaking English
flow)</p>
<p><strong>Fine-Tuning Solution:</strong></p>
<pre><code>Before: &quot;I need to go to the misrad [office] for the te&#39;udat zehut [ID card]&quot;
Whisper: &quot;I need to go to the miss rod for the to that say hoot&quot;

After Fine-Tuning:
Whisper: &quot;I need to go to the misrad for the te&#39;udat zehut&quot;</code></pre>
<p><strong>Why This Works:</strong> - You’re teaching the model your
specific code-switching patterns - The model learns which Hebrew words
you use in English contexts - It stops trying to “correct” these words
into English phonetics</p>
<p><strong>Data Requirements:</strong> - 2-5 hours of your speech with
code-switching - Transcriptions that preserve your Hebrew words in
English sentences - The model learns this as a valid pattern, not an
error</p>
<hr />
<h3 id="technical-vocabulary-adaptation">2. <strong>Technical Vocabulary
Adaptation</strong></h3>
<p>Tech/AI/dev terminology is where even excellent models like Whisper
fail:</p>
<p><strong>Common Whisper Failures:</strong></p>
<pre><code>You say: &quot;PyTorch tensor quantization&quot;
Whisper: &quot;pie torch tensor quantisation&quot; (wrong spelling, UK English)

You say: &quot;Kubernetes pod affinity&quot;
Whisper: &quot;communities pod affinity&quot;

You say: &quot;Hugging Face transformers API&quot;
Whisper: &quot;hugging face transform as API&quot;

You say: &quot;CUDA kernels&quot;
Whisper: &quot;cooler kernels&quot;</code></pre>
<p><strong>Why Technical Terms Fail:</strong> 1. Many technical terms
are <strong>rare in general training data</strong> 2. They’re often
<strong>homophones</strong> with common words (CUDA/cooler, cache/cash)
3. They follow <strong>uncommon capitalization</strong> (PyTorch, gRPC)
4. They’re <strong>product names</strong> that didn’t exist during
training</p>
<p><strong>Fine-Tuning Impact:</strong> - Teaches the model your
frequently-used technical vocabulary - Learns proper
capitalization/spelling conventions - Understands context (e.g., “CUDA”
in tech discussion vs. “cooler” in general speech) - Recognizes acronyms
and proper nouns</p>
<hr />
<h3 id="personal-voice-and-accent-adaptation">3. <strong>Personal Voice
and Accent Adaptation</strong></h3>
<p>This is where you might be underestimating the value:</p>
<p><strong>What Makes Your Voice Unique:</strong> - Accent patterns
(Israeli English has distinct phonological features) - Speaking pace and
rhythm - Prosody (stress patterns) - Coarticulation (how you blend
sounds between words) - Individual pronunciation quirks</p>
<p><strong>Whisper’s Training Data Distribution:</strong> While Whisper
saw diverse accents, Israeli English specifically: - Is a minority
accent in the training data - Often grouped with “Middle Eastern”
accents (broad category) - May not have enough examples to capture
individual variation</p>
<p><strong>Fine-Tuning Benefits:</strong> -
<strong>Personalization</strong>: Model learns YOUR specific
pronunciation patterns - <strong>Accuracy gains</strong>: Even 2-3% WER
(Word Error Rate) improvement matters - <strong>Consistency</strong>:
Fewer random errors on words you say frequently -
<strong>Confidence</strong>: Model is more “certain” about your speech
patterns</p>
<p><strong>Real-World Impact Example:</strong></p>
<pre><code>General Whisper WER on your speech: 8%
Fine-tuned Whisper WER on your speech: 5%

That&#39;s 37.5% error reduction!

In a 1000-word document:
- Before: 80 errors → time spent correcting
- After: 50 errors → 30 fewer corrections

Over time: hours saved, reduced cognitive load</code></pre>
<hr />
<h2 id="is-modest-improvement-worth-it">Is “Modest” Improvement Worth
It?</h2>
<p>You mentioned “even if the accuracy improvement is modest”—let’s
reframe this:</p>
<h3 id="what-counts-as-modest"><strong>What Counts as
“Modest”?</strong></h3>
<table>
<colgroup>
<col style="width: 47%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="header">
<th>WER Improvement</th>
<th>Practical Impact</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1-2% reduction</td>
<td>Noticeable in daily use, fewer frustrating errors</td>
</tr>
<tr class="even">
<td>2-5% reduction</td>
<td><strong>Significant</strong>: substantially fewer corrections</td>
</tr>
<tr class="odd">
<td>5-10% reduction</td>
<td><strong>Major</strong>: transforms usability for specific tasks</td>
</tr>
<tr class="even">
<td>10%+ reduction</td>
<td><strong>Dramatic</strong>: only achievable in very narrow
domains</td>
</tr>
</tbody>
</table>
<p><strong>For personal fine-tuning, 2-5% WER reduction is realistic and
highly valuable.</strong></p>
<h3 id="the-usability-cliff"><strong>The “Usability Cliff”</strong></h3>
<p>There’s a non-linear relationship between WER and usability:</p>
<pre><code>WER 15%: Barely usable, constant corrections needed
WER 10%: Usable but frustrating
WER 7%: Acceptable for drafts
WER 5%: Reliable for production use
WER 3%: Excellent, minimal intervention
WER 1%: Near-human parity</code></pre>
<p><strong>Going from 8% → 5% crosses a usability threshold</strong>: it
moves from “acceptable” to “reliable.”</p>
<hr />
<h2 id="your-specific-use-case-analysis">Your Specific Use Case
Analysis</h2>
<p>Let’s assess your drivers:</p>
<h3 id="hebrew-code-switching"><strong>1. Hebrew
Code-Switching</strong></h3>
<p><strong>Legitimacy</strong>: ⭐⭐⭐⭐⭐ (Critical for bilingual
users) <strong>Expected Improvement</strong>: High (this is where
general models fail hardest) <strong>Data Requirement</strong>: Moderate
(2-5 hours with mixed-language speech)</p>
<h3 id="technical-vocabulary"><strong>2. Technical
Vocabulary</strong></h3>
<p><strong>Legitimacy</strong>: ⭐⭐⭐⭐⭐ (Essential for professional
use) <strong>Expected Improvement</strong>: High (technical terms are
underrepresented) <strong>Data Requirement</strong>: Moderate (2-5 hours
of domain-specific speech)</p>
<h3 id="personal-voiceaccent"><strong>3. Personal
Voice/Accent</strong></h3>
<p><strong>Legitimacy</strong>: ⭐⭐⭐⭐ (Valuable, though benefits are
subtler) <strong>Expected Improvement</strong>: Moderate (2-5% WER
reduction likely) <strong>Data Requirement</strong>: Moderate (5-10
hours of your speech)</p>
<hr />
<h2 id="comparative-legitimacy">Comparative Legitimacy</h2>
<p>Let’s compare your use case to “traditional” fine-tuning
scenarios:</p>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 25%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Your Case</th>
<th>Traditional Comparison</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Domain Specificity</strong></td>
<td>AI/tech/dev</td>
<td>✅ Similar to medical/legal fine-tuning</td>
</tr>
<tr class="even">
<td><strong>Language Adaptation</strong></td>
<td>Hebrew-English code-switching</td>
<td>✅ Similar to regional dialect adaptation</td>
</tr>
<tr class="odd">
<td><strong>Underrepresented Data</strong></td>
<td>Israeli English, your voice</td>
<td>✅ Similar to low-resource language work</td>
</tr>
<tr class="even">
<td><strong>Personalization</strong></td>
<td>Your unique patterns</td>
<td>⭐ Novel, but increasingly common</td>
</tr>
</tbody>
</table>
<p><strong>Your use case combines multiple legitimate fine-tuning
drivers.</strong></p>
<hr />
<h2 id="the-emerging-trend-personal-asr-fine-tuning">The Emerging Trend:
Personal ASR Fine-Tuning</h2>
<p>You’re actually ahead of a curve:</p>
<p><strong>Why Personal Fine-Tuning Is Growing:</strong></p>
<ol type="1">
<li><strong>Consumer hardware enables it</strong>: You can fine-tune
Whisper on a consumer GPU</li>
<li><strong>Tooling has matured</strong>: Hugging Face + notebooks make
it accessible</li>
<li><strong>Privacy concerns</strong>: On-device, personal models avoid
cloud inference</li>
<li><strong>Productivity gains</strong>: Even small improvements
compound over thousands of hours of use</li>
<li><strong>Code-switching normalization</strong>: Multilingual life is
increasingly common</li>
</ol>
<p><strong>Analogy:</strong> - 10 years ago: “Why would I need a
personalized keyboard autocorrect?” - Today: Everyone benefits from
personalized keyboards learning their vocabulary</p>
<p><strong>Personal ASR is following the same trajectory.</strong></p>
<hr />
<h2 id="practical-considerations-for-your-case">Practical Considerations
for Your Case</h2>
<h3 id="data-collection-strategy"><strong>Data Collection
Strategy</strong></h3>
<p>For your specific needs:</p>
<pre><code>1. Hebrew Code-Switching Corpus (2-5 hours):
   - Record yourself speaking naturally in English with Hebrew words
   - Ensure variety: conversations, monologues, different topics
   - Transcribe with Hebrew words as you say them (transliterated)

2. Technical Vocabulary Corpus (2-5 hours):
   - Record yourself discussing AI/ML/dev topics
   - Include terminology you use daily: libraries, tools, concepts
   - Transcribe with proper technical spelling/capitalization

3. General Speech Corpus (5-10 hours):
   - Diverse topics, speaking styles
   - Includes your accent/pronunciation patterns
   - Can overlap with above categories</code></pre>
<p><strong>Total: 5-10 hours of transcribed audio</strong> (allowing for
overlap)</p>
<h3 id="expected-outcomes"><strong>Expected Outcomes</strong></h3>
<p><strong>Realistic Expectations:</strong> - <strong>Hebrew
words</strong>: 70-90% accuracy improvement on specific terms you use -
<strong>Technical vocabulary</strong>: 50-80% reduction in
mis-transcriptions - <strong>Overall WER</strong>: 2-5% reduction
(37-62% error reduction) - <strong>Subjective usability</strong>:
Noticeable improvement in daily use</p>
<p><strong>Bonus Benefits:</strong> - Model learns your speaking
pace/rhythm - Fewer errors on names (people, products, companies) -
Better handling of acronyms you use - Reduced need for post-editing</p>
<hr />
<h2 id="is-it-worth-the-effort">Is It Worth the Effort?</h2>
<p><strong>Time Investment:</strong> - Data collection: 10-15 hours
(including transcription) - Fine-tuning: 2-8 hours (mostly automated) -
Validation/iteration: 2-5 hours</p>
<p><strong>Total: ~20-30 hours one-time investment</strong></p>
<p><strong>Return on Investment:</strong> If you use STT for 2+
hours/week: - Assume 5 minutes/hour saved on corrections (conservative)
- = 10 minutes/week = ~9 hours/year saved - <strong>Payback period: ~2-3
years</strong></p>
<p><strong>But the real value isn’t just time saved:</strong> -
<strong>Reduced cognitive load</strong>: Less frustrating to use -
<strong>Increased trust</strong>: More willing to rely on STT -
<strong>Professional quality</strong>: Output closer to publishable</p>
<hr />
<h2 id="recommendations-for-your-project">Recommendations for Your
Project</h2>
<h3 id="yes-proceed-with-fine-tuning.-heres-how"><strong>Yes, Proceed
with Fine-Tuning. Here’s How:</strong></h3>
<h4 id="phase-1-pilot-validate-approach"><strong>Phase 1: Pilot
(Validate Approach)</strong></h4>
<ol type="1">
<li>Collect 2 hours of mixed-language, technical speech</li>
<li>Transcribe carefully (preserve Hebrew words, technical terms)</li>
<li>Fine-tune Whisper Medium (balance of size/performance)</li>
<li>Benchmark: compare WER before/after on held-out test set</li>
</ol>
<p><strong>If improvement ≥2% WER reduction → proceed to Phase
2</strong></p>
<h4 id="phase-2-full-fine-tuning"><strong>Phase 2: Full
Fine-Tuning</strong></h4>
<ol type="1">
<li>Collect 5-10 hours total (including Phase 1 data)</li>
<li>Ensure diversity: topics, speaking styles, contexts</li>
<li>Fine-tune with data augmentation (speed/pitch variations)</li>
<li>Validate on real-world usage over 1-2 weeks</li>
</ol>
<h4 id="phase-3-iterative-improvement"><strong>Phase 3: Iterative
Improvement</strong></h4>
<ol type="1">
<li>Collect “error cases” during daily use</li>
<li>Add targeted data for persistent errors</li>
<li>Periodic re-training (every 3-6 months)</li>
</ol>
<hr />
<h2 id="bottom-line">Bottom Line</h2>
<p><strong>Your reasons for fine-tuning are not only legitimate but
represent a valuable and growing use case.</strong></p>
<p>The combination of: - Mixed-language patterns (Hebrew/English) -
Domain-specific vocabulary (AI/tech) - Personal voice/accent
adaptation</p>
<p>…creates a <strong>compelling case for fine-tuning</strong>, even if
individual improvements are modest. The cumulative effect matters.</p>
<p><strong>Think of it as “bespoke speech recognition”</strong>: like a
tailor-made suit vs. off-the-rack. The general model (Whisper) is
excellent, but it’s cut for the average user. Fine-tuning tailors it to
your specific “fit.”</p>
<p><strong>Whisper won’t naturally improve on your specific patterns
without fine-tuning.</strong> General models optimize for broad
accuracy, not individual users.</p>
<p><strong>The question isn’t “Is this legitimate?”</strong> but rather
<strong>“What’s the best approach for your specific needs?”</strong>—and
fine-tuning is a proven, practical answer.</p>
<hr />
<p><strong>Note</strong>: This analysis was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Personal ASR fine-tuning is an emerging area—effectiveness varies by
individual. Start with a pilot to validate ROI before committing to full
data collection. Track metrics (WER, time-saved, subjective usability)
to quantify benefits.</p>
<h2 id="punctuation-personalization-fine-tuning">Punctuation
Personalization Fine Tuning</h2>
<h2 id="question-summary-2">Question Summary</h2>
<p>Daniel observes that punctuation can be idiosyncratic and stylistic -
there may be several valid ways to punctuate the same sentence based on
personal preference. He’s seen both separate punctuation models and ASR
models with built-in punctuation capabilities. The question is: Can you
fine-tune ASR models for your specific punctuation style and
preferences, similar to how you can fine-tune for custom vocabulary?</p>
<h2 id="answer-3">Answer</h2>
<p>Excellent and nuanced question! Punctuation in ASR is indeed a
fascinating area that’s often overlooked. The short answer is:
<strong>Yes, punctuation fine-tuning is possible and increasingly
practical</strong>, but it’s more complex than vocabulary fine-tuning.
Let’s explore why and how.</p>
<h3 id="two-approaches-to-punctuation-in-asr">Two Approaches to
Punctuation in ASR</h3>
<p>First, let’s clarify the architectural landscape you’ve observed:</p>
<h4
id="approach-1-separate-punctuation-model-traditional"><strong>Approach
1: Separate Punctuation Model (Traditional)</strong></h4>
<pre><code>Architecture:

Audio Input
    ↓
ASR Model (Whisper, Wav2Vec, etc.)
    ↓
Unpunctuated Text: &quot;the quick brown fox jumps over the lazy dog&quot;
    ↓
Punctuation Model
    ↓
Punctuated Text: &quot;The quick brown fox jumps over the lazy dog.&quot;</code></pre>
<p><strong>Examples:</strong> - <strong>FullStop</strong> (punctuation
restoration model) - <strong>deepmultilingualpunctuation</strong> -
<strong>Punctuator2</strong> - Custom BERT-based models for
punctuation</p>
<p><strong>How It Works:</strong> - ASR outputs raw text without
punctuation - Separate NLP model adds punctuation based on: - Word
sequences - Context - Language modeling - Learned patterns from training
data</p>
<p><strong>Pros:</strong> - Modular (can swap punctuation models
independently) - Can be fine-tuned separately from ASR - Often better
punctuation quality (dedicated task)</p>
<p><strong>Cons:</strong> - Two-stage process (slower) - ASR doesn’t see
prosody cues that indicate punctuation - Requires two models (more
complex)</p>
<h4 id="approach-2-integrated-punctuation-modern"><strong>Approach 2:
Integrated Punctuation (Modern)</strong></h4>
<pre><code>Architecture:

Audio Input
    ↓
Multimodal ASR Model (Whisper, Canary, etc.)
    ↓
Punctuated Text: &quot;The quick brown fox jumps over the lazy dog.&quot;</code></pre>
<p><strong>Examples:</strong> - <strong>Whisper</strong> (all versions)
- <strong>NVIDIA Canary</strong> - <strong>Google USM</strong> -
<strong>Assembly AI models</strong></p>
<p><strong>How It Works:</strong> - Model learns to predict punctuation
during ASR training - Uses both acoustic features AND language context:
- Prosody (pauses, intonation) - Breathing sounds - Language patterns -
Word sequences</p>
<p><strong>Pros:</strong> - End-to-end (simpler, faster) - Can use
acoustic cues (pauses → periods, rising intonation → question marks) -
Single model</p>
<p><strong>Cons:</strong> - Punctuation quality depends on ASR model
quality - Harder to customize punctuation independently - Training data
must include punctuated transcripts</p>
<h3 id="your-observation-about-personal-punctuation-style">Your
Observation About Personal Punctuation Style</h3>
<p>You’re absolutely right that punctuation can be stylistic and
idiosyncratic:</p>
<pre><code>Example Sentence (Spoken): &quot;I went to the store then I came home&quot;

Valid Punctuation Variations:

1. &quot;I went to the store. Then I came home.&quot;
   (Two sentences, formal style)

2. &quot;I went to the store, then I came home.&quot;
   (Comma splice, common in casual writing)

3. &quot;I went to the store; then I came home.&quot;
   (Semicolon, literary style)

4. &quot;I went to the store - then I came home.&quot;
   (Em dash, informal/conversational)

5. &quot;I went to the store then I came home.&quot;
   (No punctuation, run-on)

All are arguably &quot;correct&quot; depending on style guide and context!</code></pre>
<p><strong>Individual Preferences Examples:</strong></p>
<pre><code>Oxford Comma User:
&quot;I like apples, oranges, and bananas.&quot;

Non-Oxford Comma User:
&quot;I like apples, oranges and bananas.&quot;

---

Ellipsis Enthusiast:
&quot;I&#39;m not sure... maybe we should wait...&quot;

Period Minimalist:
&quot;I&#39;m not sure maybe we should wait&quot;

---

Em Dash Lover:
&quot;The project—which took six months—finally launched.&quot;

Parenthetical User:
&quot;The project (which took six months) finally launched.&quot;</code></pre>
<h3 id="can-you-fine-tune-for-personal-punctuation-style">Can You
Fine-Tune for Personal Punctuation Style?</h3>
<p><strong>Yes, but with important caveats:</strong></p>
<h4
id="option-1-fine-tuning-integrated-asr-whisper-style-models"><strong>Option
1: Fine-Tuning Integrated ASR (Whisper-style models)</strong></h4>
<p><strong>What Happens:</strong></p>
<pre><code>Your Training Data:
- 10 hours of your speech
- Transcripts reflecting YOUR punctuation style
- Example: You always use Oxford commas, em dashes, minimal ellipses

Fine-Tuning Process:
- Model learns correlations:
  - Your pause patterns → your punctuation choices
  - Your intonation → your question mark vs. period preferences
  - Your list speech → Oxford comma insertion

Result:
- Model punctuates similar to how you would write
- Learns your stylistic preferences</code></pre>
<p><strong>Real Example:</strong></p>
<pre><code>Before Fine-Tuning (Generic Whisper):
Speech: &quot;I need milk eggs and bread&quot; [with slight pause before &quot;and&quot;]
Output: &quot;I need milk, eggs and bread.&quot;

After Fine-Tuning (Your Oxford Comma Preference):
Speech: &quot;I need milk eggs and bread&quot; [with slight pause before &quot;and&quot;]
Output: &quot;I need milk, eggs, and bread.&quot;

Model learned: Your pauses in lists → Oxford comma</code></pre>
<p><strong>Limitations:</strong></p>
<ol type="1">
<li><strong>Acoustic Ambiguity:</strong>
<ul>
<li>You must speak consistently with your punctuation style</li>
<li>Pause before period, shorter pause for comma, etc.</li>
<li>If your speech doesn’t reflect punctuation, model can’t learn</li>
</ul></li>
<li><strong>Small Dataset Challenge:</strong>
<ul>
<li>Punctuation is sparse in data</li>
<li>10 hours might have only 50-100 instances of specific patterns</li>
<li>Harder to learn than vocabulary (which is dense)</li>
</ul></li>
<li><strong>Conflicting Preferences:</strong>
<ul>
<li>Your speaking style might not match your writing style</li>
<li>Model can only learn what’s in the audio+transcript</li>
</ul></li>
</ol>
<h4 id="option-2-fine-tuning-separate-punctuation-model"><strong>Option
2: Fine-Tuning Separate Punctuation Model</strong></h4>
<p>This is actually <strong>more practical</strong> for personal
punctuation preferences:</p>
<p><strong>Architecture:</strong></p>
<pre><code>ASR Model (Generic, no punctuation)
    ↓
Unpunctuated transcript
    ↓
Fine-Tuned Punctuation Model (YOUR style)
    ↓
Punctuated text in YOUR style</code></pre>
<p><strong>Why This Works Better:</strong></p>
<pre><code>Training Data for Punctuation Model:
- Your writing samples (emails, documents, blog posts)
- 100K-1M words of your written text
- Much easier to collect than speech data!

Fine-Tuning:
- Start with pre-trained punctuation model (e.g., BERT-based)
- Fine-tune on your writing style
- Learns your:
  - Comma preferences
  - Sentence length preferences
  - Em dash vs. parentheses
  - Oxford comma usage
  - Ellipsis frequency</code></pre>
<p><strong>Practical Example:</strong></p>
<div class="sourceCode" id="cb59"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> deepmultilingualpunctuation <span class="im">import</span> PunctuationModel</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> PunctuationModel()</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>your_writing <span class="op">=</span> load_texts([</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;your_emails.txt&quot;</span>,       <span class="co"># 50K words</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;your_blog_posts.txt&quot;</span>,   <span class="co"># 30K words</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;your_documents.txt&quot;</span>     <span class="co"># 20K words</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>fine_tuned_model <span class="op">=</span> finetune(</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>    base_model,</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>    your_writing,</span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">1e-5</span></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Result:</strong></p>
<pre><code>Input: &quot;I went to the store then I came home&quot;

Generic Model Output:
&quot;I went to the store. Then I came home.&quot;

Your Fine-Tuned Model:
&quot;I went to the store—then I came home.&quot;
(Because you love em dashes in your writing!)</code></pre>
<h3 id="specific-punctuation-preferences-you-can-fine-tune">Specific
Punctuation Preferences You Can Fine-Tune</h3>
<p>Here are punctuation styles that can be learned through
fine-tuning:</p>
<h4 id="comma-frequency"><strong>1. Comma Frequency</strong></h4>
<pre><code>Minimalist Comma User:
&quot;The project which took six months finally launched last week.&quot;

Heavy Comma User:
&quot;The project, which took six months, finally launched, last week.&quot;

Fine-tuning learns your preference from your writing samples.</code></pre>
<h4 id="sentence-length"><strong>2. Sentence Length</strong></h4>
<pre><code>Short Sentence Preference:
&quot;I went to the store. I bought milk. Then I came home.&quot;

Long Sentence Preference:
&quot;I went to the store, bought milk, and then came home.&quot;

Model learns your typical sentence boundary patterns.</code></pre>
<h4 id="question-mark-vs.-period-for-rhetorical-questions"><strong>3.
Question Mark vs. Period for Rhetorical Questions</strong></h4>
<pre><code>Conservative:
&quot;Why would anyone do that.&quot;

Liberal:
&quot;Why would anyone do that?&quot;

Depends on your speech intonation patterns (if fine-tuning ASR)
Or your writing patterns (if fine-tuning punctuation model)</code></pre>
<h4 id="list-punctuation"><strong>4. List Punctuation</strong></h4>
<pre><code>Oxford Comma Always:
&quot;I like Python, JavaScript, and Rust.&quot;

Oxford Comma Never:
&quot;I like Python, JavaScript and Rust.&quot;

Semicolon Lists:
&quot;I like Python, for data science; JavaScript, for web dev; and Rust, for systems.&quot;

Your model learns which you prefer.</code></pre>
<h4 id="dash-usage"><strong>5. Dash Usage</strong></h4>
<pre><code>Em Dash Enthusiast:
&quot;The weather—surprisingly—was perfect.&quot;

Parentheses Preferred:
&quot;The weather (surprisingly) was perfect.&quot;

Comma Conventional:
&quot;The weather, surprisingly, was perfect.&quot;</code></pre>
<h4 id="ellipsis-frequency"><strong>6. Ellipsis Frequency</strong></h4>
<pre><code>Frequent Ellipsis User:
&quot;I don&#39;t know... maybe we should wait... what do you think...&quot;

Minimal Ellipsis:
&quot;I don&#39;t know. Maybe we should wait. What do you think?&quot;

Model learns your baseline ellipsis frequency.</code></pre>
<h3 id="challenges-in-punctuation-fine-tuning">Challenges in Punctuation
Fine-Tuning</h3>
<h4 id="challenge-1-data-scarcity"><strong>Challenge 1: Data
Scarcity</strong></h4>
<pre><code>Vocabulary fine-tuning:
- Each word appears many times
- &quot;PyTorch&quot; might appear 100 times in 10 hours

Punctuation pattern fine-tuning:
- Specific patterns are rare
- Oxford comma in 3-item list: maybe 20 times in 10 hours
- Hard to learn from so few examples

Solution:
- Augment with your written text (for separate punctuation model)
- Collect more diverse speech samples
- Use regularization to prevent overfitting</code></pre>
<h4 id="challenge-2-inconsistency-in-natural-speech"><strong>Challenge
2: Inconsistency in Natural Speech</strong></h4>
<pre><code>Problem:
- You might punctuate written text carefully
- But speak in run-on sentences
- Model confusion: Which style to learn?

Example:
Your speech: &quot;I went to the store and bought milk and eggs and bread and then came home&quot;
Your writing: &quot;I went to the store. I bought milk, eggs, and bread. Then I came home.&quot;

Which does the model learn?

Solution:
- Decide: Do you want transcripts to match your speech OR your writing?
- Be consistent in your training data labeling</code></pre>
<h4 id="challenge-3-context-dependent-preferences"><strong>Challenge 3:
Context-Dependent Preferences</strong></h4>
<pre><code>You might punctuate differently based on context:

Formal Email:
&quot;I appreciate your consideration. Please let me know if you need further information.&quot;

Casual Text:
&quot;thanks! lmk if you need anything else&quot;

Model needs context to know which style to apply.

Solution:
- Multiple fine-tuned models for different contexts
- Prompt-based control (upcoming feature in some models)</code></pre>
<h3
id="practical-workflow-for-personal-punctuation-fine-tuning">Practical
Workflow for Personal Punctuation Fine-Tuning</h3>
<p><strong>Recommended Approach (Most Practical):</strong></p>
<pre><code>Step 1: Use Generic ASR without Punctuation
- Run Whisper with no_speech_prob filter
- Or use separate ASR that outputs unpunctuated text

Step 2: Collect Your Writing Samples
- Emails, blog posts, documents
- 50K-100K words minimum
- Representative of your preferred style

Step 3: Fine-Tune Punctuation Model
- Use pre-trained BERT/RoBERTa punctuation model
- Fine-tune on your writing
- Takes 1-2 hours on GPU

Step 4: Pipeline
Audio → ASR → Unpunctuated Text → Your Punctuation Model → Your Style!

Result:
- Your speech transcribed in YOUR writing style
- Consistent with how you actually write</code></pre>
<p><strong>Alternative (Integrated ASR Fine-Tuning):</strong></p>
<pre><code>Step 1: Collect Speech Data
- Record yourself speaking (10+ hours)
- Transcribe with your preferred punctuation style
- Important: Punctuate as you WANT it, not necessarily literally

Step 2: Fine-Tune Whisper
- Include punctuation in transcripts
- Model learns acoustic cues + your style

Step 3: Deploy
- Whisper directly outputs in your style

Limitation:
- Requires more data
- Acoustic cues must be consistent
- Harder than vocabulary fine-tuning</code></pre>
<h3 id="tools-and-resources">Tools and Resources</h3>
<p><strong>For Separate Punctuation Model Fine-Tuning:</strong></p>
<div class="sourceCode" id="cb72"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install deepmultilingualpunctuation</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install fullstop</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/ottokart/punctuator2</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a><span class="ex">from</span> transformers import BertForTokenClassification</span></code></pre></div>
<p><strong>For Integrated ASR Fine-Tuning:</strong></p>
<div class="sourceCode" id="cb73"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai-whisper</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="ex">from</span> transformers import WhisperForConditionalGeneration</span></code></pre></div>
<h3 id="research-frontier-controllable-punctuation">Research Frontier:
Controllable Punctuation</h3>
<p>Emerging research allows <strong>runtime control</strong> of
punctuation style:</p>
<pre><code>Future Capability:

prompt = &quot;Transcribe this audio with formal punctuation&quot;

prompt = &quot;Transcribe this audio with casual punctuation&quot;

model.transcribe(audio, prompt=prompt)

Same audio, different punctuation based on prompt!</code></pre>
<p><strong>Current Examples:</strong></p>
<pre><code>NVIDIA Canary supports style prompts:
&quot;&lt;formal&gt;&quot; → More periods, proper grammar
&quot;&lt;casual&gt;&quot; → Fewer commas, run-on sentences

Not yet personalized, but direction is promising!</code></pre>
<h3 id="conclusion-2">Conclusion</h3>
<p>To answer your question comprehensively:</p>
<p><strong>Yes, you can fine-tune for personal punctuation
preferences:</strong></p>
<ol type="1">
<li><strong>Best approach:</strong> Fine-tune separate punctuation model
on your writing samples
<ul>
<li>Most practical</li>
<li>Uses abundant written data</li>
<li>Directly captures your style</li>
</ul></li>
<li><strong>Alternative:</strong> Fine-tune integrated ASR (Whisper)
with punctuated transcripts
<ul>
<li>Captures acoustic cues + your style</li>
<li>Requires more speech data</li>
<li>Harder to achieve consistency</li>
</ul></li>
<li><strong>Challenges:</strong>
<ul>
<li>Punctuation patterns are sparse in speech data</li>
<li>Your speech style might differ from writing style</li>
<li>Context-dependent preferences are hard to capture</li>
</ul></li>
<li><strong>Practical recommendation:</strong>
<ul>
<li>Use unpunctuated ASR or generic punctuation</li>
<li>Fine-tune separate punctuation model on 50K-100K words of your
writing</li>
<li>Pipeline: ASR → Your Punctuation Model</li>
<li>Result: Transcripts in YOUR writing style</li>
</ul></li>
</ol>
<p><strong>The Bottom Line:</strong> Just as you can fine-tune for
vocabulary, you can fine-tune for punctuation. But because punctuation
is stylistic and context-dependent, it’s often more practical to
fine-tune a separate punctuation restoration model on your written work
rather than fine-tuning the ASR directly. This gives you maximum control
and requires less speech data.</p>
<p>Your intuition that punctuation is idiosyncratic and personal is
absolutely correct, and customizing it is not only possible but
increasingly practical with modern fine-tuning techniques!</p>
<hr />
<p><em>Note: This response was generated by Claude Code as part of
Daniel’s STT Fine-Tuning Notebook project. Information is based on
current ASR research, punctuation restoration techniques, and
fine-tuning best practices.</em></p>
<h2 id="training-from-scratch-vs-fine-tuning">Training From Scratch Vs
Fine Tuning</h2>
<h2 id="the-hypothetical">The Hypothetical</h2>
<p>Instead of fine-tuning an existing ASR model (like Whisper) on your
voice and vocabulary, what if you trained a completely new ASR model
from scratch—as if you were creating a new Whisper?</p>
<p><strong>Would this make sense? What would the process look like? How
would the results differ?</strong></p>
<h2 id="short-answer-1">Short Answer</h2>
<p><strong>This would not make any sense for personalizing ASR to your
voice/vocabulary.</strong> Training from scratch would require:</p>
<ul>
<li>100,000+ hours of diverse audio data (vs. 1-10 hours for
fine-tuning)</li>
<li>Millions of dollars in compute costs (vs. $50-500)</li>
<li>Months of training time (vs. hours)</li>
<li>Deep ML expertise (vs. following tutorials)</li>
<li>Worse results than fine-tuning for your specific use case</li>
</ul>
<p><strong>Fine-tuning is not a shortcut—it’s the correct
approach.</strong> But let’s explore the hypothetical to understand
<em>why</em>.</p>
<h2 id="what-is-training-from-scratch">What is “Training from
Scratch”?</h2>
<p>Training from scratch means:</p>
<ol type="1">
<li>Starting with <strong>randomly initialized weights</strong> (no
pre-existing knowledge)</li>
<li>Teaching the model <strong>everything</strong> about speech and
language:
<ul>
<li>How audio waveforms correspond to phonemes</li>
<li>How phonemes combine into words</li>
<li>How words combine into sentences</li>
<li>Grammar, syntax, and language structure</li>
<li>Accents, speaking styles, and acoustic variations</li>
</ul></li>
<li>Using only your training data (no leveraging of existing
models)</li>
</ol>
<h2 id="the-training-process-for-asr-from-scratch">The Training Process
for ASR from Scratch</h2>
<h3 id="step-1-architecture-design">Step 1: Architecture Design</h3>
<p>You’d need to design the model architecture:</p>
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScratchASR:</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.audio_encoder <span class="op">=</span> AudioEncoder(</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>            layers<span class="op">=</span><span class="dv">32</span>,           <span class="co"># vs. Whisper&#39;s encoder</span></span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>            hidden_dim<span class="op">=</span><span class="dv">1280</span>,     <span class="co"># Embedding dimensions</span></span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>            attention_heads<span class="op">=</span><span class="dv">20</span></span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.text_decoder <span class="op">=</span> TextDecoder(</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>            layers<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>            hidden_dim<span class="op">=</span><span class="dv">1280</span>,</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>            vocab_size<span class="op">=</span><span class="dv">51865</span>     <span class="co"># Number of tokens</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
<p><strong>Decisions required:</strong></p>
<ul>
<li>Model size (how many parameters? 50M? 500M? 1.5B?)</li>
<li>Architecture type (Transformer? Conformer? Hybrid?)</li>
<li>Attention mechanism (standard, flash attention, sliding
window?)</li>
<li>Audio preprocessing (mel spectrograms, raw waveform?)</li>
<li>Tokenization strategy (character-level, BPE, word-level?)</li>
</ul>
<p><strong>Time investment:</strong> Weeks to months of architectural
experimentation</p>
<h3 id="step-2-data-collection">Step 2: Data Collection</h3>
<p>For a model to learn <strong>general</strong> speech recognition,
you’d need:</p>
<p><strong>Minimum viable dataset:</strong></p>
<ul>
<li><strong>100,000+ hours</strong> of transcribed audio</li>
<li>Covering:
<ul>
<li>Multiple speakers (10,000+ different voices)</li>
<li>Multiple accents (American, British, Australian, Indian, etc.)</li>
<li>Multiple domains (conversations, podcasts, audiobooks,
lectures)</li>
<li>Multiple recording conditions (clean, noisy, reverberant)</li>
<li>Multiple speaking styles (fast, slow, formal, casual)</li>
</ul></li>
</ul>
<p><strong>Whisper’s training data:</strong> 680,000 hours</p>
<p><strong>Your personal data:</strong> 1-10 hours</p>
<p><strong>Comparison:</strong> Your data is 0.001% of what’s needed</p>
<p><strong>Data collection cost:</strong></p>
<ul>
<li>Transcription services: ~$0.10-1.00 per minute</li>
<li>100,000 hours = 6,000,000 minutes</li>
<li>Cost: $600,000 - $6,000,000 for transcription alone</li>
</ul>
<h3 id="step-3-data-preprocessing">Step 3: Data Preprocessing</h3>
<p>Prepare your dataset:</p>
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;audio&quot;</span>: preprocessed_audio,      <span class="co"># Mel spectrogram</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;text&quot;</span>: transcription,             <span class="co"># Cleaned text</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;language&quot;</span>: <span class="st">&quot;en&quot;</span>,                  <span class="co"># Language code</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;speaker_id&quot;</span>: <span class="dv">12345</span>,              <span class="co"># For multi-speaker handling</span></span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;sample_rate&quot;</span>: <span class="dv">16000</span>,</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;duration&quot;</span>: <span class="fl">15.3</span>                   <span class="co"># seconds</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Tasks:</strong></p>
<ul>
<li>Audio normalization and preprocessing</li>
<li>Text cleaning and normalization</li>
<li>Dataset balancing (ensure diverse coverage)</li>
<li>Train/validation/test splits</li>
<li>Creating data loaders optimized for your training setup</li>
</ul>
<p><strong>Time investment:</strong> 2-4 weeks for large-scale data
pipeline</p>
<h3 id="step-4-training-setup">Step 4: Training Setup</h3>
<p><strong>Compute requirements:</strong></p>
<p>For a Whisper Large-scale model (1.5B parameters):</p>
<ul>
<li><strong>Minimum:</strong> 8× A100 GPUs (80GB each)</li>
<li><strong>Optimal:</strong> 64-256 GPUs in distributed training</li>
<li><strong>Training time:</strong>
<ul>
<li>On 8× A100: ~6 months</li>
<li>On 64× A100: ~3-4 weeks</li>
<li>On 256× A100: ~1 week</li>
</ul></li>
</ul>
<p><strong>Cost:</strong></p>
<ul>
<li>Cloud A100: ~$2-4/hour per GPU</li>
<li>8 GPUs × 4 months × 24 hours/day × $3/hour = $690,000</li>
<li>Plus storage, networking, data transfer costs</li>
</ul>
<p><strong>For comparison, fine-tuning:</strong></p>
<ul>
<li>Single RTX 4090 or consumer GPU</li>
<li>2-12 hours training time</li>
<li>Cost: $50-200 in electricity/cloud compute</li>
</ul>
<h3 id="step-5-training-process">Step 5: Training Process</h3>
<p>The training loop (simplified):</p>
<div class="sourceCode" id="cb78"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ScratchASR()</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):  <span class="co"># Could be 50-200 epochs</span></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> dataloader:</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>        audio, text <span class="op">=</span> batch</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>        predicted_text <span class="op">=</span> model(audio)</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute loss</span></span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> compute_loss(predicted_text, text)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This happens millions of times</span></span></code></pre></div>
<p><strong>What the model learns:</strong></p>
<ul>
<li><strong>Epoch 1-10:</strong> Basic phoneme recognition (recognizing
“ah”, “ee”, “s” sounds)</li>
<li><strong>Epoch 10-30:</strong> Word recognition (mapping sounds to
common words)</li>
<li><strong>Epoch 30-60:</strong> Sentence structure (understanding word
order, grammar)</li>
<li><strong>Epoch 60-100:</strong> Robustness (handling noise, accents,
variations)</li>
<li><strong>Epoch 100-200:</strong> Refinement (punctuation,
capitalization, edge cases)</li>
</ul>
<p><strong>Critical point:</strong> With only 1-10 hours of your
personal data, the model would:</p>
<ul>
<li>Massively overfit (memorize your specific recordings)</li>
<li>Fail to generalize to any variations</li>
<li>Not learn general speech recognition at all</li>
</ul>
<h3 id="step-6-evaluation-and-iteration">Step 6: Evaluation and
Iteration</h3>
<p>After training, evaluate on held-out test sets:</p>
<pre><code>Test Set 1 (Clean speech): 45% WER  ← Terrible
Test Set 2 (Noisy speech): 78% WER  ← Catastrophically bad
Test Set 3 (Accented speech): 92% WER  ← Essentially non-functional</code></pre>
<p><strong>Why so bad?</strong></p>
<ul>
<li>Insufficient training data</li>
<li>Insufficient diversity</li>
<li>Model hasn’t learned general acoustic-linguistic mappings</li>
</ul>
<p><strong>You’d need to:</strong></p>
<ul>
<li>Collect more data (another 50,000+ hours)</li>
<li>Retrain from scratch</li>
<li>Iterate for 6-12 months</li>
</ul>
<h2 id="what-would-the-results-look-like">What Would the Results Look
Like?</h2>
<p>Let’s compare three scenarios:</p>
<h3
id="scenario-a-training-from-scratch-on-10-hours-of-your-data">Scenario
A: Training from Scratch on 10 hours of your data</h3>
<p><strong>What you’d get:</strong></p>
<ul>
<li>Model that memorized your 10 hours of recordings</li>
<li>Perfect transcription of those exact recordings</li>
<li>Complete failure on anything else:
<ul>
<li>Different words than in training: 90%+ WER</li>
<li>Different acoustic conditions: 95%+ WER</li>
<li>Different speaking pace: 85%+ WER</li>
</ul></li>
</ul>
<p><strong>Usability:</strong> Essentially zero. Model is a 1.5GB lookup
table of your training data.</p>
<h3
id="scenario-b-fine-tuning-whisper-on-10-hours-of-your-data">Scenario B:
Fine-tuning Whisper on 10 hours of your data</h3>
<p><strong>What you’d get:</strong></p>
<ul>
<li>Model that leveraged 680,000 hours of pre-training</li>
<li>Improved accuracy on your voice and vocabulary</li>
<li>Still handles general speech well:
<ul>
<li>Your voice + target vocabulary: 2-5% WER (vs. 8-12% before
fine-tuning)</li>
<li>General speech: 5-8% WER (vs. 3-5% for base Whisper)</li>
<li>Different acoustic conditions: 10-15% WER</li>
</ul></li>
</ul>
<p><strong>Usability:</strong> Excellent for your specific use case.</p>
<h3
id="scenario-c-training-from-scratch-on-680000-hours-whisper-scale">Scenario
C: Training from Scratch on 680,000 hours (Whisper-scale)</h3>
<p><strong>What you’d get:</strong></p>
<ul>
<li>Model comparable to Whisper</li>
<li>General speech recognition capabilities</li>
<li>NOT optimized for your voice/vocabulary</li>
</ul>
<p><strong>Cost:</strong> $2-10 million in compute + years of effort</p>
<p><strong>Result:</strong> You’ve recreated Whisper, which already
exists and is free.</p>
<h2 id="why-fine-tuning-is-the-correct-approach">Why Fine-Tuning is the
Correct Approach</h2>
<p>Fine-tuning works because of <strong>transfer learning:</strong></p>
<pre><code>Pre-trained Whisper knowledge (680,000 hours):
├── Phoneme recognition ✓ (keep this)
├── Common English words ✓ (keep this)
├── Grammar and syntax ✓ (keep this)
├── Noise robustness ✓ (keep this)
└── Your specific voice/vocab ✗ (learn this)</code></pre>
<p>Fine-tuning says: <strong>“Keep 99.9% of what Whisper knows, adjust
0.1% for my specific needs.”</strong></p>
<p>Training from scratch says: <strong>“Forget everything, start
over.”</strong></p>
<h3 id="the-mathematics-of-transfer-learning">The Mathematics of
Transfer Learning</h3>
<p><strong>Pre-training:</strong> Model learns general features from
massive data</p>
<pre><code>θ_pretrained = optimize(L(D_large))
where D_large = 680,000 hours</code></pre>
<p><strong>Fine-tuning:</strong> Small adjustments to pre-trained
weights</p>
<pre><code>θ_finetuned = θ_pretrained + Δθ
where Δθ = optimize(L(D_small))
and D_small = 10 hours</code></pre>
<p><strong>Training from scratch:</strong> Learn everything from limited
data</p>
<pre><code>θ_scratch = optimize(L(D_small))
where D_small = 10 hours  ← Impossible to learn general ASR</code></pre>
<p><strong>Key insight:</strong></p>
<ul>
<li>θ_pretrained contains 680,000 hours of learned knowledge</li>
<li>Fine-tuning adjusts this vast knowledge slightly</li>
<li>Training from scratch tries to learn everything from 10 hours</li>
</ul>
<p>It’s like:</p>
<ul>
<li><strong>Fine-tuning:</strong> “Here’s a comprehensive encyclopedia.
Let me add a few pages about my specific topic.”</li>
<li><strong>Training from scratch:</strong> “Here are 10 pages. Write a
comprehensive encyclopedia.”</li>
</ul>
<h2 id="when-training-from-scratch-makes-sense">When Training from
Scratch Makes Sense</h2>
<p>There are legitimate use cases for training ASR from scratch:</p>
<h3 id="new-architecture-research">1. <strong>New Architecture
Research</strong></h3>
<p>You’ve invented a novel architecture that might outperform
Transformers:</p>
<ul>
<li>You have research funding and compute resources</li>
<li>You train on standard datasets (LibriSpeech, Common Voice,
etc.)</li>
<li>Goal is advancing ASR research, not personalizing to your voice</li>
</ul>
<h3 id="extremely-low-resource-languages">2. <strong>Extremely
Low-Resource Languages</strong></h3>
<p>You’re working on a language with &lt;1,000 speakers and no existing
ASR:</p>
<ul>
<li>No pre-trained model exists for this language family</li>
<li>You collect all available audio in the language (maybe 100-1,000
hours)</li>
<li>Train a small model from scratch as a starting point</li>
</ul>
<h3 id="privacysecurity-constraints">3. <strong>Privacy/Security
Constraints</strong></h3>
<p>You work in defense/intelligence with extreme security
requirements:</p>
<ul>
<li>Cannot use any external models (even open-source)</li>
<li>Have access to vast amounts of classified audio data</li>
<li>Budget and security requirements justify the cost</li>
</ul>
<h3 id="embeddedspecialized-hardware">4. <strong>Embedded/Specialized
Hardware</strong></h3>
<p>You’re designing a custom chip with novel ASR capabilities:</p>
<ul>
<li>Need to co-design model architecture with hardware</li>
<li>Have specialized architecture constraints</li>
<li>Existing models don’t fit your hardware paradigm</li>
</ul>
<h2 id="the-practical-reality">The Practical Reality</h2>
<p>Even in these scenarios, practitioners typically:</p>
<ol type="1">
<li><strong>Start with transfer learning</strong> when possible (use
Wav2Vec2, Whisper, etc. as starting point)</li>
<li><strong>Only train from scratch</strong> when absolutely
necessary</li>
<li><strong>Use massive datasets</strong> (100,000+ hours minimum)</li>
<li><strong>Work in teams</strong> with specialized ML engineers</li>
<li><strong>Take months to years</strong> for the project</li>
</ol>
<p>For personalizing ASR to your voice and vocabulary, <strong>training
from scratch is never the answer.</strong></p>
<h2 id="comparison-table">Comparison Table</h2>
<table>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Training from Scratch</th>
<th>Fine-Tuning Whisper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Data required</strong></td>
<td>100,000+ hours</td>
<td>1-10 hours</td>
</tr>
<tr class="even">
<td><strong>Compute cost</strong></td>
<td>$500K - $10M</td>
<td>$50 - $500</td>
</tr>
<tr class="odd">
<td><strong>Time to train</strong></td>
<td>1-6 months</td>
<td>2-12 hours</td>
</tr>
<tr class="even">
<td><strong>Expertise required</strong></td>
<td>Deep ML research</td>
<td>Follow tutorials</td>
</tr>
<tr class="odd">
<td><strong>Result for your voice</strong></td>
<td>Catastrophic failure</td>
<td>Excellent</td>
</tr>
<tr class="even">
<td><strong>Result for general speech</strong></td>
<td>Bad (unless huge data)</td>
<td>Good</td>
</tr>
<tr class="odd">
<td><strong>Makes sense?</strong></td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<h2 id="hypothetical-step-by-step-training-from-scratch">Hypothetical
Step-by-Step: Training from Scratch</h2>
<p>If you really wanted to do this (hypothetically):</p>
<h3 id="month-1-2-planning-and-architecture">Month 1-2: Planning and
Architecture</h3>
<ul>
<li>Design model architecture</li>
<li>Set up training infrastructure</li>
<li>Prepare distributed training across GPU cluster</li>
</ul>
<h3 id="month-3-8-data-collection">Month 3-8: Data Collection</h3>
<ul>
<li>Record or purchase 100,000+ hours of transcribed audio</li>
<li>Clean and preprocess all data</li>
<li>Create training pipelines</li>
</ul>
<h3 id="month-9-12-initial-training">Month 9-12: Initial Training</h3>
<ul>
<li>Train initial model version</li>
<li>Monitor for convergence</li>
<li>Debug training instabilities</li>
</ul>
<h3 id="month-13-15-evaluation-and-iteration">Month 13-15: Evaluation
and Iteration</h3>
<ul>
<li>Evaluate on test sets</li>
<li>Identify failure modes</li>
<li>Collect additional targeted data</li>
</ul>
<h3 id="month-16-18-retraining-and-refinement">Month 16-18: Retraining
and Refinement</h3>
<ul>
<li>Retrain with augmented data</li>
<li>Tune hyperparameters</li>
<li>Optimize inference speed</li>
</ul>
<h3 id="month-19-24-production-preparation">Month 19-24: Production
Preparation</h3>
<ul>
<li>Quantize for deployment</li>
<li>Build serving infrastructure</li>
<li>Document and release</li>
</ul>
<p><strong>Total:</strong> 2 years, $2-5 million, team of 5-10
people</p>
<p><strong>Result:</strong> A model roughly equivalent to Whisper Base,
which already exists for free</p>
<p><strong>For your voice:</strong> No better than fine-tuning, possibly
worse</p>
<h2 id="conclusion-3">Conclusion</h2>
<p>Training an ASR model from scratch for your personal voice and
vocabulary makes no sense because:</p>
<ol type="1">
<li><strong>Fundamentally wrong approach:</strong> You need general ASR
+ personal adaptation, not personal-only ASR</li>
<li><strong>Impossible data requirements:</strong> 100,000+ hours
vs. your available 1-10 hours</li>
<li><strong>Prohibitive costs:</strong> Millions of dollars
vs. hundreds</li>
<li><strong>Worse results:</strong> Would catastrophically overfit and
fail to generalize</li>
<li><strong>Reinventing the wheel:</strong> Whisper already exists and
has learned general speech</li>
</ol>
<p><strong>Fine-tuning is not a compromise—it’s the correct engineering
approach</strong>, leveraging transfer learning to adapt massive
pre-trained knowledge to your specific needs with minimal data and
compute.</p>
<p>The only time training from scratch makes sense:</p>
<ul>
<li>You’re an ASR research lab with $10M+ funding</li>
<li>You’re advancing the state-of-the-art</li>
<li>You have 100,000+ hours of diverse training data</li>
<li>You’re not trying to personalize—you’re building a general
model</li>
</ul>
<p>For personalization, <strong>fine-tuning is always the
answer.</strong></p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
<h2 id="training-parameters">Training Parameters</h2>
<h2 id="overview">Overview</h2>
<p>This guide covers the essential training parameters (hyperparameters)
used when fine-tuning speech-to-text models, particularly focusing on
Whisper and similar transformer-based architectures. Understanding these
parameters is crucial for achieving optimal model performance.</p>
<hr />
<h2 id="core-training-parameters">Core Training Parameters</h2>
<h3 id="epochs">1. Epochs</h3>
<p><strong>Definition</strong>: One epoch represents a complete pass
through the entire training dataset.</p>
<p><strong>Typical Range</strong>: 3-20 epochs for fine-tuning</p>
<p><strong>How It Works</strong>:</p>
<pre><code>Total Training Steps = (Dataset Size / Batch Size) × Number of Epochs</code></pre>
<p><strong>Considerations</strong>:</p>
<ul>
<li><strong>Too Few Epochs</strong>: Model underfits, doesn’t learn
patterns
<ul>
<li>Symptoms: High training loss, poor performance</li>
<li>Solution: Increase epochs</li>
</ul></li>
<li><strong>Too Many Epochs</strong>: Model overfits, memorizes training
data
<ul>
<li>Symptoms: Training loss decreases but validation loss increases</li>
<li>Solution: Use early stopping or reduce epochs</li>
</ul></li>
</ul>
<p><strong>Best Practices</strong>: - Start with 5-10 epochs for initial
experiments - Use early stopping to prevent overtraining - Monitor
validation metrics to determine optimal number - Smaller datasets need
fewer epochs (3-5) - Larger datasets can benefit from more epochs
(10-20)</p>
<p><strong>Example Configuration</strong>:</p>
<div class="sourceCode" id="cb85"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> Seq2SeqTrainingArguments(</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Complete passes through data</span></span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<h3 id="batch-size">2. Batch Size</h3>
<p><strong>Definition</strong>: Number of training examples processed
simultaneously in one forward/backward pass.</p>
<p><strong>Types</strong>: -
<strong>per_device_train_batch_size</strong>: Batch size per GPU/CPU -
<strong>per_device_eval_batch_size</strong>: Batch size for validation -
<strong>gradient_accumulation_steps</strong>: Simulates larger batch
sizes</p>
<p><strong>Typical Range</strong>: 4-32 per device (depends on GPU
memory)</p>
<p><strong>Effective Batch Size Calculation</strong>:</p>
<pre><code>Effective Batch Size = per_device_batch_size × num_devices × gradient_accumulation_steps</code></pre>
<p><strong>Trade-offs</strong>:</p>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Batch Size</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small (4-8)</td>
<td>Less memory usage<br>More gradient updates<br>Better
generalization</td>
<td>Slower training<br>Noisier gradients<br>Less stable</td>
</tr>
<tr class="even">
<td>Large (16-32+)</td>
<td>Faster training<br>Stable gradients<br>Better GPU utilization</td>
<td>High memory requirements<br>May overfit<br>Needs more data</td>
</tr>
</tbody>
</table>
<p><strong>Best Practices</strong>: - Start with largest batch size that
fits in GPU memory - Use gradient accumulation to simulate larger
batches - Typical setup:
<code>batch_size=16, gradient_accumulation_steps=2</code> (effective
batch size = 32) - Reduce batch size if encountering OOM (Out of Memory)
errors</p>
<p><strong>Example</strong>:</p>
<div class="sourceCode" id="cb87"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> Seq2SeqTrainingArguments(</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,  <span class="co"># Can be larger (no gradients stored)</span></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,  <span class="co"># Effective batch = 8 × 4 = 32</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<h3 id="learning-rate">3. Learning Rate</h3>
<p><strong>Definition</strong>: Controls how much model weights are
updated during training. The most critical hyperparameter.</p>
<p><strong>Typical Range</strong>: 1e-5 to 1e-4 for fine-tuning</p>
<p><strong>Components</strong>:</p>
<h4 id="base-learning-rate">Base Learning Rate</h4>
<div class="sourceCode" id="cb88"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">5e-5</span>  <span class="co"># Common starting point for fine-tuning</span></span></code></pre></div>
<h4 id="learning-rate-schedule">Learning Rate Schedule</h4>
<p>Controls how learning rate changes during training:</p>
<p><strong>Common Schedules</strong>:</p>
<ol type="1">
<li><p><strong>Linear Decay</strong></p>
<div class="sourceCode" id="cb89"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler_type <span class="op">=</span> <span class="st">&quot;linear&quot;</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a><span class="co"># LR decreases linearly from initial value to 0</span></span></code></pre></div></li>
<li><p><strong>Cosine Annealing</strong></p>
<div class="sourceCode" id="cb90"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler_type <span class="op">=</span> <span class="st">&quot;cosine&quot;</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="co"># LR follows cosine curve, smooth decay</span></span></code></pre></div></li>
<li><p><strong>Constant</strong></p>
<div class="sourceCode" id="cb91"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler_type <span class="op">=</span> <span class="st">&quot;constant&quot;</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="co"># LR stays fixed throughout training</span></span></code></pre></div></li>
<li><p><strong>Constant with Warmup</strong></p>
<div class="sourceCode" id="cb92"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>lr_scheduler_type <span class="op">=</span> <span class="st">&quot;constant_with_warmup&quot;</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a><span class="co"># LR increases linearly for warmup, then stays constant</span></span></code></pre></div></li>
</ol>
<h4 id="warmup-steps">Warmup Steps</h4>
<p><strong>Definition</strong>: Number of steps where learning rate
gradually increases from 0 to target value.</p>
<p><strong>Purpose</strong>: Prevents unstable training at the
beginning</p>
<p><strong>Typical Range</strong>: 500-2000 steps (or 5-10% of total
steps)</p>
<div class="sourceCode" id="cb93"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>warmup_steps <span class="op">=</span> <span class="dv">500</span>  <span class="co"># Absolute number</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>warmup_ratio <span class="op">=</span> <span class="fl">0.1</span>  <span class="co"># 10% of total training steps</span></span></code></pre></div>
<p><strong>Visualization</strong>:</p>
<pre><code>Learning Rate Schedule (Linear with Warmup)

LR  ^
    |     /‾‾‾‾‾‾‾‾‾‾\
    |    /              \
    |   /                 \
    |  /                    \
    | /                       \
    |/_________________________\___&gt; Steps
      Warmup    Training       End</code></pre>
<p><strong>Best Practices</strong>: - <strong>For fine-tuning</strong>:
Start with 1e-5 to 5e-5 - <strong>For training from scratch</strong>:
Start with 1e-4 to 5e-4 - Use warmup to stabilize initial training -
Monitor loss curves to adjust if needed - If loss explodes: reduce
learning rate - If loss plateaus early: increase learning rate</p>
<p><strong>Example</strong>:</p>
<div class="sourceCode" id="cb95"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> Seq2SeqTrainingArguments(</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">&quot;linear&quot;</span>,</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<h3 id="weight-decay">4. Weight Decay</h3>
<p><strong>Definition</strong>: L2 regularization that penalizes large
weights to prevent overfitting.</p>
<p><strong>Typical Range</strong>: 0.0 to 0.1</p>
<p><strong>How It Works</strong>: Adds penalty term to loss function</p>
<pre><code>Loss_total = Loss_task + weight_decay × Σ(weights²)</code></pre>
<p><strong>Guidelines</strong>: - <strong>No weight decay
(0.0)</strong>: No regularization - <strong>Light (0.01)</strong>:
Minimal regularization, common default - <strong>Moderate
(0.05)</strong>: Good for smaller datasets - <strong>Heavy
(0.1)</strong>: Strong regularization for overfitting prevention</p>
<p><strong>Best Practices</strong>: - Start with 0.01 (common default) -
Increase if overfitting occurs - Decrease if underfitting - Monitor
validation metrics</p>
<div class="sourceCode" id="cb97"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># L2 regularization strength</span></span></code></pre></div>
<hr />
<h3 id="gradient-clipping">5. Gradient Clipping</h3>
<p><strong>Definition</strong>: Limits the maximum gradient value to
prevent exploding gradients.</p>
<p><strong>Parameter</strong>: <code>max_grad_norm</code></p>
<p><strong>Typical Value</strong>: 1.0</p>
<p><strong>How It Works</strong>:</p>
<div class="sourceCode" id="cb98"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> gradient_norm <span class="op">&gt;</span> max_grad_norm:</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> gradient × (max_grad_norm <span class="op">/</span> gradient_norm)</span></code></pre></div>
<p><strong>Purpose</strong>: - Prevents training instability - Stops
gradient explosions - Particularly important for RNNs and long
sequences</p>
<p><strong>Best Practices</strong>: - Default value of 1.0 works well
for most cases - Increase to 5.0 if you need more gradient freedom -
Decrease to 0.5 for very stable training</p>
<div class="sourceCode" id="cb99"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>max_grad_norm <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># Clip gradients above this norm</span></span></code></pre></div>
<hr />
<h3 id="dropout">6. Dropout</h3>
<p><strong>Definition</strong>: Randomly drops (sets to zero) a
percentage of neurons during training to prevent overfitting.</p>
<p><strong>Typical Range</strong>: 0.0 to 0.3</p>
<p><strong>Types</strong>: - <strong>Attention Dropout</strong>: Applied
to attention weights - <strong>Activation Dropout</strong>: Applied to
hidden states - <strong>Overall Dropout</strong>: General dropout
rate</p>
<p><strong>Guidelines</strong>: - <strong>No dropout (0.0)</strong>: No
regularization - <strong>Light (0.1)</strong>: Standard for well-sized
datasets - <strong>Moderate (0.2)</strong>: Good for smaller datasets -
<strong>Heavy (0.3)</strong>: Aggressive overfitting prevention</p>
<p><strong>Note</strong>: Dropout is only active during training,
disabled during evaluation.</p>
<div class="sourceCode" id="cb100"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>attention_dropout <span class="op">=</span> <span class="fl">0.1</span></span></code></pre></div>
<hr />
<h2 id="evaluation-and-monitoring-parameters">Evaluation and Monitoring
Parameters</h2>
<h3 id="evaluation-strategy">7. Evaluation Strategy</h3>
<p><strong>Definition</strong>: How often to evaluate model on
validation set.</p>
<p><strong>Options</strong>:</p>
<div class="sourceCode" id="cb101"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>evaluation_strategy <span class="op">=</span> <span class="st">&quot;steps&quot;</span></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>eval_steps <span class="op">=</span> <span class="dv">500</span>  <span class="co"># Evaluate every 500 training steps</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>evaluation_strategy <span class="op">=</span> <span class="st">&quot;epoch&quot;</span></span></code></pre></div>
<p><strong>Best Practices</strong>: - For small datasets:
<code>evaluation_strategy="epoch"</code> - For large datasets:
<code>evaluation_strategy="steps"</code> with
<code>eval_steps=500-1000</code> - More frequent evaluation = better
monitoring but slower training</p>
<hr />
<h3 id="save-strategy">8. Save Strategy</h3>
<p><strong>Definition</strong>: How often to save model checkpoints.</p>
<div class="sourceCode" id="cb102"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>save_strategy <span class="op">=</span> <span class="st">&quot;steps&quot;</span>  <span class="co"># or &quot;epoch&quot;</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a>save_steps <span class="op">=</span> <span class="dv">500</span>  <span class="co"># Save every 500 steps</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>save_total_limit <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Keep only best 3 checkpoints</span></span></code></pre></div>
<p><strong>Best Practices</strong>: - Match save strategy to evaluation
strategy - Use <code>save_total_limit</code> to prevent disk space
issues - Enable <code>load_best_model_at_end=True</code> for optimal
final model</p>
<hr />
<h3 id="logging">9. Logging</h3>
<p><strong>Definition</strong>: How often to log training metrics.</p>
<div class="sourceCode" id="cb103"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>logging_steps <span class="op">=</span> <span class="dv">100</span>  <span class="co"># Log every 100 steps</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>report_to <span class="op">=</span> [<span class="st">&quot;tensorboard&quot;</span>]  <span class="co"># or &quot;wandb&quot;, &quot;none&quot;</span></span></code></pre></div>
<hr />
<h2 id="advanced-parameters">Advanced Parameters</h2>
<h3 id="optimizer">10. Optimizer</h3>
<p><strong>Definition</strong>: Algorithm used to update model
weights.</p>
<p><strong>Common Options</strong>:</p>
<div class="sourceCode" id="cb104"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> <span class="st">&quot;adamw_torch&quot;</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> <span class="st">&quot;adamw_8bit&quot;</span></span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="#cb104-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-8"><a href="#cb104-8" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> <span class="st">&quot;adafactor&quot;</span></span></code></pre></div>
<p><strong>Best Practice</strong>: Use AdamW for most cases</p>
<hr />
<h3 id="mixed-precision-training">11. Mixed Precision Training</h3>
<p><strong>Definition</strong>: Uses lower precision (FP16/BF16) to
speed up training and reduce memory.</p>
<div class="sourceCode" id="cb105"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>fp16 <span class="op">=</span> <span class="va">True</span>  <span class="co"># For older GPUs (Nvidia Volta, Turing)</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>bf16 <span class="op">=</span> <span class="va">True</span>  <span class="co"># For newer GPUs (Nvidia Ampere, Ada) - more stable</span></span></code></pre></div>
<p><strong>Benefits</strong>: - 2x faster training - 50% less memory
usage - Minimal accuracy impact</p>
<hr />
<h3 id="generation-parameters-for-seq2seq">12. Generation Parameters
(for Seq2Seq)</h3>
<p><strong>For STT models during evaluation</strong>:</p>
<div class="sourceCode" id="cb106"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>generation_max_length <span class="op">=</span> <span class="dv">225</span></span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>generation_num_beams <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Greedy decoding (fastest)</span></span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>generation_num_beams <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Better quality, slower</span></span></code></pre></div>
<hr />
<h2 id="complete-example-configuration">Complete Example
Configuration</h2>
<div class="sourceCode" id="cb107"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> Seq2SeqTrainingArguments</span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a>training_args <span class="op">=</span> Seq2SeqTrainingArguments(</span>
<span id="cb107-4"><a href="#cb107-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output</span></span>
<span id="cb107-5"><a href="#cb107-5" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">&quot;./whisper-finetuned&quot;</span>,</span>
<span id="cb107-6"><a href="#cb107-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-7"><a href="#cb107-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training duration</span></span>
<span id="cb107-8"><a href="#cb107-8" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb107-9"><a href="#cb107-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-10"><a href="#cb107-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Batch sizes</span></span>
<span id="cb107-11"><a href="#cb107-11" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb107-12"><a href="#cb107-12" aria-hidden="true" tabindex="-1"></a>    per_device_eval_batch_size<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb107-13"><a href="#cb107-13" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">4</span>,  <span class="co"># Effective batch size = 32</span></span>
<span id="cb107-14"><a href="#cb107-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-15"><a href="#cb107-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Learning rate</span></span>
<span id="cb107-16"><a href="#cb107-16" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb107-17"><a href="#cb107-17" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">&quot;linear&quot;</span>,</span>
<span id="cb107-18"><a href="#cb107-18" aria-hidden="true" tabindex="-1"></a>    warmup_steps<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb107-19"><a href="#cb107-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-20"><a href="#cb107-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Regularization</span></span>
<span id="cb107-21"><a href="#cb107-21" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb107-22"><a href="#cb107-22" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb107-23"><a href="#cb107-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-24"><a href="#cb107-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation</span></span>
<span id="cb107-25"><a href="#cb107-25" aria-hidden="true" tabindex="-1"></a>    evaluation_strategy<span class="op">=</span><span class="st">&quot;steps&quot;</span>,</span>
<span id="cb107-26"><a href="#cb107-26" aria-hidden="true" tabindex="-1"></a>    eval_steps<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb107-27"><a href="#cb107-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-28"><a href="#cb107-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saving</span></span>
<span id="cb107-29"><a href="#cb107-29" aria-hidden="true" tabindex="-1"></a>    save_strategy<span class="op">=</span><span class="st">&quot;steps&quot;</span>,</span>
<span id="cb107-30"><a href="#cb107-30" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb107-31"><a href="#cb107-31" aria-hidden="true" tabindex="-1"></a>    save_total_limit<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb107-32"><a href="#cb107-32" aria-hidden="true" tabindex="-1"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb107-33"><a href="#cb107-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-34"><a href="#cb107-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Logging</span></span>
<span id="cb107-35"><a href="#cb107-35" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb107-36"><a href="#cb107-36" aria-hidden="true" tabindex="-1"></a>    report_to<span class="op">=</span>[<span class="st">&quot;tensorboard&quot;</span>],</span>
<span id="cb107-37"><a href="#cb107-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-38"><a href="#cb107-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Performance</span></span>
<span id="cb107-39"><a href="#cb107-39" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">True</span>,  <span class="co"># or bf16=True for newer GPUs</span></span>
<span id="cb107-40"><a href="#cb107-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-41"><a href="#cb107-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generation (for evaluation)</span></span>
<span id="cb107-42"><a href="#cb107-42" aria-hidden="true" tabindex="-1"></a>    predict_with_generate<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb107-43"><a href="#cb107-43" aria-hidden="true" tabindex="-1"></a>    generation_max_length<span class="op">=</span><span class="dv">225</span>,</span>
<span id="cb107-44"><a href="#cb107-44" aria-hidden="true" tabindex="-1"></a>    generation_num_beams<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb107-45"><a href="#cb107-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-46"><a href="#cb107-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optimization</span></span>
<span id="cb107-47"><a href="#cb107-47" aria-hidden="true" tabindex="-1"></a>    optim<span class="op">=</span><span class="st">&quot;adamw_torch&quot;</span>,</span>
<span id="cb107-48"><a href="#cb107-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb107-49"><a href="#cb107-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Misc</span></span>
<span id="cb107-50"><a href="#cb107-50" aria-hidden="true" tabindex="-1"></a>    push_to_hub<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb107-51"><a href="#cb107-51" aria-hidden="true" tabindex="-1"></a>    metric_for_best_model<span class="op">=</span><span class="st">&quot;wer&quot;</span>,  <span class="co"># Word Error Rate</span></span>
<span id="cb107-52"><a href="#cb107-52" aria-hidden="true" tabindex="-1"></a>    greater_is_better<span class="op">=</span><span class="va">False</span>,  <span class="co"># Lower WER is better</span></span>
<span id="cb107-53"><a href="#cb107-53" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<hr />
<h2 id="parameter-tuning-guidelines">Parameter Tuning Guidelines</h2>
<h3 id="starting-point-conservative">Starting Point (Conservative)</h3>
<div class="sourceCode" id="cb108"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>num_train_epochs<span class="op">=</span><span class="dv">5</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>per_device_train_batch_size<span class="op">=</span><span class="dv">8</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>learning_rate<span class="op">=</span><span class="fl">1e-5</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a>warmup_steps<span class="op">=</span><span class="dv">500</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a>weight_decay<span class="op">=</span><span class="fl">0.01</span></span></code></pre></div>
<h3 id="for-small-datasets-20-hours">For Small Datasets (&lt; 20
hours)</h3>
<div class="sourceCode" id="cb109"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>num_train_epochs<span class="op">=</span><span class="dv">3</span><span class="op">-</span><span class="dv">5</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>per_device_train_batch_size<span class="op">=</span><span class="dv">4</span><span class="op">-</span><span class="dv">8</span></span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>learning_rate<span class="op">=</span><span class="fl">1e-5</span></span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>weight_decay<span class="op">=</span><span class="fl">0.05</span>  <span class="co"># Higher regularization</span></span>
<span id="cb109-5"><a href="#cb109-5" aria-hidden="true" tabindex="-1"></a>dropout<span class="op">=</span><span class="fl">0.2</span></span></code></pre></div>
<h3 id="for-large-datasets-100-hours">For Large Datasets (&gt; 100
hours)</h3>
<div class="sourceCode" id="cb110"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>num_train_epochs<span class="op">=</span><span class="dv">10</span><span class="op">-</span><span class="dv">20</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true" tabindex="-1"></a>per_device_train_batch_size<span class="op">=</span><span class="dv">16</span><span class="op">-</span><span class="dv">32</span></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true" tabindex="-1"></a>learning_rate<span class="op">=</span><span class="fl">5e-5</span></span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true" tabindex="-1"></a>weight_decay<span class="op">=</span><span class="fl">0.01</span></span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true" tabindex="-1"></a>warmup_steps<span class="op">=</span><span class="dv">1000</span></span></code></pre></div>
<h3 id="if-overfitting">If Overfitting</h3>
<div class="sourceCode" id="cb111"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>num_train_epochs <span class="op">-=</span> <span class="dv">2</span></span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">+=</span> <span class="fl">0.02</span></span>
<span id="cb111-6"><a href="#cb111-6" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">+=</span> <span class="fl">0.1</span></span>
<span id="cb111-7"><a href="#cb111-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-8"><a href="#cb111-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-9"><a href="#cb111-9" aria-hidden="true" tabindex="-1"></a>early_stopping_patience<span class="op">=</span><span class="dv">3</span></span></code></pre></div>
<h3 id="if-underfitting">If Underfitting</h3>
<div class="sourceCode" id="cb112"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true" tabindex="-1"></a>num_train_epochs <span class="op">+=</span> <span class="dv">5</span></span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">*=</span> <span class="dv">2</span></span>
<span id="cb112-6"><a href="#cb112-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-7"><a href="#cb112-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb112-8"><a href="#cb112-8" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">/=</span> <span class="dv">2</span></span></code></pre></div>
<hr />
<h2 id="monitoring-guidelines">Monitoring Guidelines</h2>
<p>Track these metrics during training:</p>
<ol type="1">
<li><strong>Training Loss</strong>: Should steadily decrease</li>
<li><strong>Validation Loss</strong>: Should decrease and track training
loss</li>
<li><strong>WER (Word Error Rate)</strong>: Should steadily
decrease</li>
<li><strong>Learning Rate</strong>: Check schedule is working as
expected</li>
<li><strong>Gradient Norm</strong>: Should be stable, not exploding</li>
</ol>
<p><strong>Red Flags</strong>: - Validation loss increases while
training loss decreases → Overfitting - Both losses plateau early →
Underfitting or learning rate too low - Loss becomes NaN → Gradient
explosion (reduce LR or clip gradients) - No improvement after several
epochs → Hyperparameter adjustment needed</p>
<hr />
<h2 id="summary-table">Summary Table</h2>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 26%" />
<col style="width: 16%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Typical Range</th>
<th>Purpose</th>
<th>Adjustment Strategy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Epochs</td>
<td>3-20</td>
<td>Training duration</td>
<td>Monitor validation loss</td>
</tr>
<tr class="even">
<td>Batch Size</td>
<td>4-32</td>
<td>Memory/speed trade-off</td>
<td>Maximize within GPU limits</td>
</tr>
<tr class="odd">
<td>Learning Rate</td>
<td>1e-5 to 1e-4</td>
<td>Update speed</td>
<td>Reduce if unstable</td>
</tr>
<tr class="even">
<td>Weight Decay</td>
<td>0.0-0.1</td>
<td>Regularization</td>
<td>Increase if overfitting</td>
</tr>
<tr class="odd">
<td>Warmup Steps</td>
<td>500-2000</td>
<td>Training stability</td>
<td>5-10% of total steps</td>
</tr>
<tr class="even">
<td>Gradient Clipping</td>
<td>1.0</td>
<td>Prevent explosions</td>
<td>Keep at 1.0 usually</td>
</tr>
<tr class="odd">
<td>Dropout</td>
<td>0.0-0.3</td>
<td>Regularization</td>
<td>Increase if overfitting</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="conclusion-4">Conclusion</h2>
<p>Successful fine-tuning requires careful balancing of these
parameters. Start with conservative defaults, monitor validation metrics
closely, and adjust based on training behavior. Remember that every
dataset is different, so experimentation and iteration are key to
achieving optimal results.</p>
<h1 id="part-v-inference-deployment">Part V: Inference &amp;
Deployment</h1>
<p><em>Running and deploying ASR models</em></p>
<hr />
<h2 id="second-chunking-whisper-streaming">30 Second Chunking Whisper
Streaming</h2>
<h2 id="question-1">Question</h2>
<p>Why is there a 30-second limit in Whisper transcription (especially
on mobile)? Does it make a meaningful difference whether you use “voice
typing” (real-time chunking) versus “async transcription” (recording
then transcribing in one go)? Are the processing architectures
different, and do you get different results?</p>
<h2 id="answer-4">Answer</h2>
<p>Excellent question that touches on a fundamental Whisper architecture
constraint and how it manifests in practical applications. Let’s unpack
this.</p>
<hr />
<h2 id="why-the-30-second-limit-exists">Why the 30-Second Limit
Exists</h2>
<h3 id="whispers-architecture-constraint"><strong>Whisper’s Architecture
Constraint</strong></h3>
<p><strong>Whisper has a hard 30-second input window</strong> by design,
set during training:</p>
<p><strong>Technical Details:</strong></p>
<pre><code>Audio Input → Mel-spectrogram (80 bins)
Time dimension: Fixed at 3000 frames (30 seconds at 100 fps)
Positional encodings: Fixed to 1500 positions (encoder + decoder)

Whisper cannot process audio &gt;30 seconds without chunking.</code></pre>
<p><strong>Why 30 Seconds?</strong></p>
<ol type="1">
<li><strong>Training Data</strong>: Most training samples were subtitle
segments (&lt;30s)</li>
<li><strong>Memory</strong>: Transformer attention is O(n²)—longer
sequences = exponential memory</li>
<li><strong>Positional Encodings</strong>: Fixed-length learned
positional embeddings (not relative)</li>
<li><strong>Balance</strong>: 30s is long enough for context, short
enough for efficiency</li>
</ol>
<p><strong>This is NOT a mobile limitation—it’s a fundamental model
constraint.</strong></p>
<p>Desktop Whisper has the same 30-second limit. Mobile inherits it
because they’re using the same model architecture.</p>
<hr />
<h2 id="how-applications-handle-30-second-audio">How Applications Handle
&gt;30-Second Audio</h2>
<p>When you transcribe longer audio, there are two common
approaches:</p>
<h3
id="approach-1-sequential-chunking-what-youre-experiencing"><strong>Approach
1: Sequential Chunking (What You’re Experiencing)</strong></h3>
<p><strong>How It Works:</strong></p>
<pre><code>Audio (5 minutes) → Split into 30s chunks → Process chunk 1 → chunk 2 → ... → chunk 10</code></pre>
<p><strong>Implementation (Typical Mobile App):</strong></p>
<div class="sourceCode" id="cb115"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> transcribe_long_audio(audio_file):</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>    chunks <span class="op">=</span> split_audio_30s(audio_file)</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>    transcriptions <span class="op">=</span> []</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> chunk <span class="kw">in</span> chunks:</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> whisper.transcribe(chunk)  <span class="co"># Each takes 2-5 seconds</span></span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a>        transcriptions.append(result)</span>
<span id="cb115-8"><a href="#cb115-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb115-9"><a href="#cb115-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&quot; &quot;</span>.join(transcriptions)</span></code></pre></div>
<p><strong>What You’re Noticing:</strong> - Processing happens
<strong>sequentially</strong> (one chunk at a time) - There’s a
delay/stutter at 30s boundaries - Each chunk is independent (no context
from previous chunks)</p>
<p><strong>Problems:</strong> 1. <strong>Boundary Issues</strong>:
Words/sentences split at 30s mark → transcription errors 2.
<strong>Sequential Latency</strong>: Each chunk takes 2-5s → 5min audio
= 10 chunks × 3s = 30s processing 3. <strong>Context Loss</strong>:
Chunk 2 doesn’t know what was said in chunk 1</p>
<h3
id="approach-2-overlapping-chunking-better-but-rarer"><strong>Approach
2: Overlapping Chunking (Better, But Rarer)</strong></h3>
<p><strong>How It Works:</strong></p>
<pre><code>Chunk 1: [0-30s]
Chunk 2: [25-55s]  ← 5-second overlap
Chunk 3: [50-80s]  ← 5-second overlap
...</code></pre>
<p><strong>Benefits:</strong> - Overlap ensures words at boundaries are
fully captured - Can merge overlapping transcriptions intelligently -
Reduces boundary errors</p>
<p><strong>Drawbacks:</strong> - More chunks to process (slightly
slower) - Need smarter merging logic</p>
<p><strong>Few mobile apps implement this</strong> (more complex
code).</p>
<hr />
<h2 id="voice-typing-vs.-async-transcription-key-differences">Voice
Typing vs. Async Transcription: Key Differences</h2>
<h3 id="voice-typing-real-time-streaming"><strong>Voice Typing
(Real-Time / Streaming)</strong></h3>
<p><strong>How It Works:</strong></p>
<pre><code>You speak → App captures 30s → Processes → Displays text → Captures next 30s → ...</code></pre>
<p><strong>Implementation Details:</strong> - <strong>Live audio
buffer</strong>: Continuously recording - <strong>Trigger at
30s</strong>: When buffer fills, send to Whisper - <strong>Display
immediately</strong>: Show text as it’s transcribed - <strong>Next
chunk</strong>: Start new buffer while displaying previous result</p>
<p><strong>User Experience:</strong> - Text appears in ~30-second bursts
- Noticeable pauses at 30s boundaries (processing delay) - Can’t go back
and correct later chunks based on earlier context</p>
<p><strong>Pros:</strong> - ✅ Immediate feedback (see text as you
speak) - ✅ Good for short dictation (emails, messages)</p>
<p><strong>Cons:</strong> - ❌ Stuttering at boundaries - ❌ Higher
cognitive load (watching text appear) - ❌ Boundary errors more
noticeable (mid-sentence splits)</p>
<hr />
<h3 id="async-transcription-record-then-transcribe"><strong>Async
Transcription (Record Then Transcribe)</strong></h3>
<p><strong>How It Works:</strong></p>
<pre><code>You speak (5 min) → Record entire audio → Send for transcription → Process all chunks → Return full text</code></pre>
<p><strong>Implementation Details:</strong> - <strong>Record full
audio</strong>: Capture entire note/recording - <strong>Save as single
file</strong>: WAV, MP3, etc. - <strong>Chunk at processing
time</strong>: Split into 30s segments when transcribing -
<strong>Process in batch</strong>: Can use parallel processing (if
hardware supports)</p>
<p><strong>User Experience:</strong> - No live feedback while speaking -
Processing happens all at once after recording - Get complete
transcription result</p>
<p><strong>Pros:</strong> - ✅ Better for long-form (lectures, meetings,
notes) - ✅ Can optimize chunking (overlapping, silence detection) - ✅
Parallel processing possible (faster on multi-core) - ✅ Can add
post-processing (punctuation, paragraphs)</p>
<p><strong>Cons:</strong> - ❌ No live feedback (don’t know if it’s
working) - ❌ All-or-nothing (if it fails, lose everything)</p>
<hr />
<h2 id="does-it-make-a-meaningful-difference">Does It Make a Meaningful
Difference?</h2>
<h3 id="short-answer-yes-but-nuanced."><strong>Short Answer: Yes, but
nuanced.</strong></h3>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 30%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Voice Typing</th>
<th>Async Transcription</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>Same (model is identical)</td>
<td>Same (model is identical)</td>
</tr>
<tr class="even">
<td><strong>Boundary Errors</strong></td>
<td>More noticeable</td>
<td>Can be reduced with overlap</td>
</tr>
<tr class="odd">
<td><strong>Processing Speed</strong></td>
<td>Perceived slower (sequential + waiting)</td>
<td>Can be faster (batch + parallel)</td>
</tr>
<tr class="even">
<td><strong>User Experience</strong></td>
<td>Choppy, stuttering</td>
<td>Smooth, all-at-once</td>
</tr>
<tr class="odd">
<td><strong>Best For</strong></td>
<td>Short dictation (&lt;2 min)</td>
<td>Long notes (&gt;2 min)</td>
</tr>
</tbody>
</table>
<h3 id="accuracy-mostly-the-same"><strong>Accuracy: Mostly the
Same</strong></h3>
<p>If both approaches use <strong>sequential chunking without
overlap</strong>, accuracy will be identical: - Same model - Same chunks
- Same transcription per chunk</p>
<p><strong>However</strong>, async transcription CAN be more accurate
if: 1. <strong>Overlapping chunks</strong>: Reduces boundary errors 2.
<strong>Smart segmentation</strong>: Chunks split at pauses, not
arbitrary 30s 3. <strong>Post-processing</strong>: Can apply
punctuation/paragraph models on full text</p>
<h3 id="performance-async-can-be-faster"><strong>Performance: Async Can
Be Faster</strong></h3>
<p><strong>Voice Typing (Serial Processing):</strong></p>
<pre><code>Speak 30s → Wait 3s (processing) → Speak 30s → Wait 3s → ...
Total time: 5 min speaking + 30s processing = 5:30 total</code></pre>
<p><strong>Async (Batch Processing):</strong></p>
<pre><code>Speak 5 min → Process all 10 chunks in parallel (if multi-core) → 3-5s total
Total time: 5 min speaking + 5s processing = 5:05 total</code></pre>
<p><strong>But your phone (OnePlus Nord 3) likely does NOT
parallelize</strong> (APU may not support it, or app doesn’t implement
it), so async is processed sequentially anyway:</p>
<pre><code>Speak 5 min → Process chunks 1-10 sequentially → 30s processing
Total time: 5 min speaking + 30s processing = 5:30 total</code></pre>
<p><strong>So performance is similar for your hardware</strong> unless
the app is highly optimized.</p>
<h3 id="boundary-handling-async-can-be-better"><strong>Boundary
Handling: Async Can Be Better</strong></h3>
<p><strong>Voice Typing:</strong></p>
<pre><code>[Chunk 1]: &quot;...and then we decided to go to the st-&quot;
[Chunk 2]: &quot;ore to buy some groceries&quot;</code></pre>
<p>Result: “st ore” (word split, likely transcription error)</p>
<p><strong>Async with Overlapping:</strong></p>
<pre><code>[Chunk 1]: &quot;...and then we decided to go to the st-&quot;
[Overlap]: &quot;to the store to buy&quot;  ← captures full word
[Chunk 2]: &quot;ore to buy some groceries&quot;

Merge: &quot;...and then we decided to go to the store to buy some groceries&quot;</code></pre>
<p>Result: Correct transcription</p>
<p><strong>Most mobile apps don’t do overlapping</strong>, so this
advantage is theoretical unless you use a sophisticated app.</p>
<hr />
<h2 id="practical-implications-for-your-use-case">Practical Implications
for Your Use Case</h2>
<h3 id="your-observation-choppy-process-around-30s-mark"><strong>Your
Observation: “Choppy Process” Around 30s Mark</strong></h3>
<p><strong>What’s Happening:</strong> 1. At ~29 seconds: App prepares to
send chunk to Whisper 2. At 30 seconds: Processing starts (2-5 second
delay) 3. During processing: Either - Audio recording pauses (you can’t
speak) → <strong>very choppy</strong> - Audio recording continues but
processing blocks UI → <strong>laggy</strong></p>
<p><strong>This is a real-time processing bottleneck</strong>, not
inherent to Whisper.</p>
<p><strong>Solution:</strong> - <strong>Better apps</strong>: Buffer
next chunk while processing previous (seamless) - <strong>Async
transcription</strong>: Avoid this issue entirely (no live
processing)</p>
<hr />
<h3 id="which-approach-should-you-use"><strong>Which Approach Should You
Use?</strong></h3>
<h4 id="for-note-taking-your-primary-use-case"><strong>For Note-Taking
(Your Primary Use Case):</strong></h4>
<p><strong>Recommendation: Async Transcription</strong></p>
<p><strong>Why:</strong> 1. <strong>Better accuracy</strong>: Can use
overlapping chunks 2. <strong>No interruptions</strong>: Record full
thought without pauses 3. <strong>Post-processing</strong>: Can apply
punctuation/paragraph tools after 4. <strong>Less frustrating</strong>:
No choppy 30s boundaries</p>
<p><strong>Implementation:</strong> - Use a voice recorder app (record
full note) - Transcribe afterward using: - Desktop (Faster-Whisper with
overlapping) - Mobile app that supports async (SpeechNote, others)</p>
<h4 id="for-short-dictation-messages-emails"><strong>For Short Dictation
(Messages, Emails):</strong></h4>
<p><strong>Voice typing is fine</strong> (&lt;2 minutes, a few
chunks).</p>
<h4 id="best-of-both-worlds"><strong>Best of Both Worlds:</strong></h4>
<p><strong>Use a hybrid approach:</strong> 1. <strong>Short inputs
(&lt;1 min)</strong>: Voice typing for immediacy 2. <strong>Long inputs
(&gt;2 min)</strong>: Async transcription for quality</p>
<hr />
<h2 id="optimizing-async-transcription-on-your-setup">Optimizing Async
Transcription on Your Setup</h2>
<h3 id="on-desktop-amd-7700-xt"><strong>On Desktop (AMD 7700
XT):</strong></h3>
<p>Use <strong>Faster-Whisper with overlapping</strong>:</p>
<div class="sourceCode" id="cb124"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> faster_whisper <span class="im">import</span> WhisperModel</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> WhisperModel(<span class="st">&quot;medium&quot;</span>, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>, compute_type<span class="op">=</span><span class="st">&quot;float16&quot;</span>)</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>segments, info <span class="op">=</span> model.transcribe(</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;long_note.wav&quot;</span>,</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    vad_filter<span class="op">=</span><span class="va">True</span>,  <span class="co"># Voice Activity Detection (skip silence)</span></span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true" tabindex="-1"></a>    vad_parameters<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true" tabindex="-1"></a>        min_silence_duration_ms<span class="op">=</span><span class="dv">500</span>,  <span class="co"># Chunk at pauses</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true" tabindex="-1"></a>full_transcription <span class="op">=</span> <span class="st">&quot; &quot;</span>.join([seg.text <span class="cf">for</span> seg <span class="kw">in</span> segments])</span></code></pre></div>
<p><strong>Benefits:</strong> - VAD (Voice Activity Detection) chunks at
natural pauses (not arbitrary 30s) - Faster processing (CTranslate2
engine) - Better boundary handling</p>
<hr />
<h3 id="on-phone-oneplus-nord-3"><strong>On Phone (OnePlus Nord
3):</strong></h3>
<p><strong>Option 1: Record + Upload to Desktop</strong></p>
<pre><code>Record on phone → Transfer to desktop → Transcribe with Faster-Whisper</code></pre>
<p>Best accuracy, but requires transfer step.</p>
<p><strong>Option 2: Use App with Smart Chunking</strong> Look for
Android apps that support: - Overlapping chunks - VAD-based segmentation
- Post-processing</p>
<p><strong>Candidates:</strong> - <strong>SpeechNote</strong> (Linux,
but check Android version features) - <strong>Whisper.cpp-based
apps</strong> (some support smart chunking) - <strong>Transcription
tools with VAD</strong></p>
<hr />
<h2 id="the-underlying-question-can-we-remove-the-30s-limit">The
Underlying Question: Can We Remove the 30s Limit?</h2>
<p><strong>Short answer: Not with current Whisper
architecture.</strong></p>
<p><strong>Future Models:</strong> - <strong>Relative positional
encodings</strong>: Could support arbitrary length - <strong>Sliding
window transformers</strong>: Process long audio in overlapping windows
- <strong>Chunking-aware training</strong>: Train models specifically to
handle chunks better</p>
<p><strong>Current Research:</strong> -
<strong>Whisper-Longformer</strong>: Experimental variants with longer
context - <strong>Streaming Whisper</strong>: Optimized for real-time
with better boundary handling</p>
<p><strong>But for now, 30-second chunking is unavoidable with
Whisper.</strong></p>
<hr />
<h2 id="summary">Summary</h2>
<table>
<colgroup>
<col style="width: 55%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th>Question</th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Why 30s limit?</strong></td>
<td>Whisper’s architecture (fixed positional encodings, memory
constraints)</td>
</tr>
<tr class="even">
<td><strong>Voice typing vs. async: different
architectures?</strong></td>
<td>No—both use same chunking, but async can optimize better</td>
</tr>
<tr class="odd">
<td><strong>Meaningfully different results?</strong></td>
<td>Accuracy: same. UX: async is better for long-form</td>
</tr>
<tr class="even">
<td><strong>Recommend for note-taking?</strong></td>
<td><strong>Async transcription</strong> with overlapping/VAD</td>
</tr>
</tbody>
</table>
<p><strong>The “choppy” experience you’re noticing is a real-time
processing UX issue</strong>, not fundamental to Whisper. Async
transcription (record → transcribe) avoids this and allows for better
optimization (overlapping chunks, VAD, post-processing).</p>
<p><strong>For your use case (note-taking, longer recordings), async
transcription is superior.</strong></p>
<hr />
<p><strong>Note</strong>: This explanation was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Whisper’s 30-second limit is architectural and unlikely to change in
current versions. For production note-taking workflows, consider using
Faster-Whisper on desktop with VAD-based chunking for best results, or
mobile apps that implement intelligent segmentation. Always test both
approaches with your specific audio to verify practical differences.</p>
<h2 id="deployment-options-for-custom-asr">Deployment Options For Custom
Asr</h2>
<h2 id="question-summary-3">Question Summary</h2>
<p>Daniel is exploring deployment options for fine-tuned or custom ASR
models, particularly for individual/solo users. He’s found Replicate for
serverless but is concerned about costs for 24/7 operation. He wants to
understand the full spectrum of deployment options and cost implications
for both serverless and always-on (local or cloud) deployments.</p>
<h2 id="answer-5">Answer</h2>
<p>You’re right that this is somewhat niche territory for individual
users, but it’s increasingly relevant as more people fine-tune their own
ASR models. Let me break down the deployment landscape
comprehensively.</p>
<h3 id="serverless-inference-options">Serverless Inference Options</h3>
<p><strong>1. Replicate</strong> - <strong>What you found:</strong> Yes,
Replicate is the most prominent serverless option -
<strong>Pricing:</strong> Pay-per-second of inference time - Typically
$0.0005-0.0025 per second depending on hardware (CPU vs GPU) - For
Whisper-sized models on GPU: ~$0.001/second - <strong>Cost
Example:</strong> - 1 hour of audio processing ≈ 6 minutes inference
time (10x realtime) - Cost: ~$0.36 per hour of audio transcribed - For
intermittent use (say, 5 hours of audio/month): ~$1.80/month -
<strong>Pros:</strong> Zero setup, scales automatically, no idle costs -
<strong>Cons:</strong> Cold start latency (2-15 seconds), per-request
costs add up quickly for heavy use</p>
<p><strong>2. Hugging Face Inference Endpoints</strong> -
<strong>Overview:</strong> Serverless inference for models hosted on
HuggingFace - <strong>Pricing Tiers:</strong> - Free tier: Limited
requests, public models only - Paid: $0.06/hour (CPU) to $1.50/hour
(GPU) when running - Auto-scales to zero when idle (no requests for 15
minutes) - <strong>Cost Example:</strong> - If processing requests
sporadically (active 2 hours/day): ~$90/month for GPU instance - Better
than 24/7 ($1,080/month) but still pricey for continuous use -
<strong>Pros:</strong> Good HuggingFace integration, custom model
support - <strong>Cons:</strong> Not truly serverless (charges per hour
active, not per request)</p>
<p><strong>3. Modal</strong> - <strong>Overview:</strong> Python-native
serverless compute platform - <strong>Pricing:</strong> Pay per
GPU-second - A10G GPU: ~$0.0010/second - T4 GPU: ~$0.0005/second -
<strong>Cost Example:</strong> - Processing 10 hours of audio/day
(realtime inference): ~$36/month on T4 - <strong>Pros:</strong>
Excellent developer experience, true pay-per-use, fast cold starts -
<strong>Cons:</strong> Requires some Python infrastructure code
setup</p>
<p><strong>4. Banana.dev (now Tonic.ai)</strong> -
<strong>Overview:</strong> Serverless GPU inference platform -
<strong>Pricing:</strong> Similar to Replicate (~$0.0008/second for GPU)
- <strong>Status:</strong> Rebranded/transitioning, may be less stable
option currently - <strong>Pros:</strong> Previously popular for ASR
deployments - <strong>Cons:</strong> Platform uncertainty after
rebrand</p>
<p><strong>5. Baseten</strong> - <strong>Overview:</strong> ML inference
platform with serverless and dedicated options -
<strong>Pricing:</strong> Custom pricing, typically
$0.0005-0.0015/second - <strong>Pros:</strong> Good performance, handles
custom models well - <strong>Cons:</strong> Less transparent pricing,
requires contact for details</p>
<p><strong>6. AWS Lambda + GPU (Emerging)</strong> -
<strong>Overview:</strong> AWS is rolling out Lambda support for GPUs -
<strong>Status:</strong> Limited availability, not yet widely practical
for ASR - <strong>Future Potential:</strong> Could become very
cost-effective for sporadic use</p>
<h3 id="self-hosted-options">24/7 Self-Hosted Options</h3>
<p>If you want always-available inference (locally or cloud), here are
the realistic options:</p>
<h4 id="local-deployment-home-server">Local Deployment (Home
Server)</h4>
<p><strong>Option A: Dedicated Machine</strong> - <strong>Hardware
Requirements for Whisper:</strong> - CPU-only: Modern 8-core CPU
(i7/Ryzen 7), 16GB RAM - GPU: RTX 3060 (12GB VRAM) or better for
comfortable performance - Storage: 50-100GB SSD for models and OS</p>
<ul>
<li><strong>Costs:</strong>
<ul>
<li><strong>Initial:</strong> $800-1,500 for dedicated machine (or use
existing hardware)</li>
<li><strong>Electricity:</strong>
<ul>
<li>Idle GPU server: ~100-150W = ~$10-15/month (at $0.12/kWh)</li>
<li>Under load: ~250W = ~$25/month</li>
<li>Annual: ~$120-300/year in electricity</li>
</ul></li>
</ul></li>
<li><strong>Networking:</strong>
<ul>
<li>Port forwarding: Free (security risk - need VPN)</li>
<li>Cloudflare Tunnel: Free (recommended, secure)</li>
<li>Tailscale/ZeroTier: Free for personal use (private network)</li>
</ul></li>
</ul>
<p><strong>Option B: Your Existing Hardware</strong> - You have AMD RX
7700 XT with ROCm - excellent for ASR! - <strong>Costs:</strong> -
Electricity only (~$10-20/month if running 24/7) - Wear and tear on GPU
(negligible for inference) - <strong>Pros:</strong> No additional
hardware cost, full control - <strong>Cons:</strong> Home network
dependency, potential security exposure</p>
<p><strong>Recommended Setup for Local 24/7:</strong></p>
<div class="sourceCode" id="cb126"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h4 id="cloud-vps-deployment">Cloud VPS Deployment</h4>
<p><strong>Option 1: CPU-Only VPS (Budget)</strong> -
<strong>Providers:</strong> Hetzner, OVH, DigitalOcean, Linode -
<strong>Recommended Specs:</strong> 8-core CPU, 16GB RAM -
<strong>Costs:</strong> - Hetzner CCX33: €32.69/month (~$35/month) - 8
vCores, 32GB RAM - DigitalOcean: $48/month - 8 vCPU, 16GB RAM -
<strong>Performance:</strong> - Realtime or slightly faster for
Whisper-large - Acceptable for most use cases - <strong>Pros:</strong>
Predictable costs, reliable, no home network dependency -
<strong>Cons:</strong> Slower than GPU inference</p>
<p><strong>Option 2: GPU Cloud Instances</strong> -
<strong>RunPod:</strong> - RTX A4000 (16GB): ~$0.34/hour = ~$245/month
for 24/7 - RTX 4090 (24GB): ~$0.69/hour = ~$497/month for 24/7 -
<strong>Vast.ai:</strong> - RTX 3060 (12GB): ~$0.15/hour = ~$108/month
for 24/7 - Highly variable pricing (spot market) - <strong>Lambda
Labs:</strong> - A10 GPU: $0.60/hour = ~$432/month - <strong>Google
Cloud / AWS / Azure:</strong> - Much more expensive (~$0.70-2.00/hour
for GPU instances) - GCP T4: ~$0.35/hour = ~$252/month</p>
<p><strong>Option 3: Hybrid Approach (Spot Instances)</strong> -
<strong>Vast.ai Spot Instances:</strong> - Bid on idle GPU capacity -
Can get RTX 3080 for ~$0.10/hour = ~$72/month - Risk: Instance can be
reclaimed (need auto-restart logic) - <strong>AWS Spot / GCP
Preemptible:</strong> - 60-80% cheaper than on-demand - Requires
interruption handling</p>
<h3 id="cost-comparison-summary">Cost Comparison Summary</h3>
<table style="width:100%;">
<colgroup>
<col style="width: 22%" />
<col style="width: 14%" />
<col style="width: 30%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="header">
<th>Deployment Option</th>
<th>Setup Cost</th>
<th>Monthly Cost (Light Use)</th>
<th>Monthly Cost (Heavy/24-7)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Replicate</strong></td>
<td>$0</td>
<td>$5-20</td>
<td>$300-1,000+</td>
</tr>
<tr class="even">
<td><strong>Modal</strong></td>
<td>$0</td>
<td>$10-50</td>
<td>$200-500</td>
</tr>
<tr class="odd">
<td><strong>HF Inference Endpoints</strong></td>
<td>$0</td>
<td>$30-100</td>
<td>$1,080 (GPU always-on)</td>
</tr>
<tr class="even">
<td><strong>Local (Existing HW)</strong></td>
<td>$0</td>
<td>$10-20</td>
<td>$15-30</td>
</tr>
<tr class="odd">
<td><strong>Local (New Server)</strong></td>
<td>$800-1,500</td>
<td>$10-20</td>
<td>$15-30</td>
</tr>
<tr class="even">
<td><strong>CPU VPS (Hetzner)</strong></td>
<td>$0</td>
<td>$35</td>
<td>$35</td>
</tr>
<tr class="odd">
<td><strong>GPU Cloud (Vast.ai)</strong></td>
<td>$0</td>
<td>$108+</td>
<td>$108-500</td>
</tr>
<tr class="even">
<td><strong>GPU Cloud (RunPod)</strong></td>
<td>$0</td>
<td>$245+</td>
<td>$245-500</td>
</tr>
</tbody>
</table>
<h3 id="recommendations-based-on-use-cases">Recommendations Based on Use
Cases</h3>
<p><strong>Scenario 1: Occasional Personal Use (&lt; 10 hours
audio/month)</strong> - <strong>Best Option:</strong> Replicate or Modal
- <strong>Reasoning:</strong> Zero setup, only pay for what you use -
<strong>Cost:</strong> $5-20/month</p>
<p><strong>Scenario 2: Regular Personal Use (Daily, ~2-4 hours
audio/day)</strong> - <strong>Best Option:</strong> Local deployment on
your existing hardware - <strong>Reasoning:</strong> Electricity costs
less than serverless, full control - <strong>Cost:</strong>
~$15-25/month (electricity only) - <strong>Setup:</strong> Docker +
Faster-Whisper + Cloudflare Tunnel</p>
<p><strong>Scenario 3: Service/App Development (Public API)</strong> -
<strong>Best Option:</strong> CPU VPS (Hetzner) with queue system -
<strong>Reasoning:</strong> Predictable costs, good performance,
professional reliability - <strong>Cost:</strong> ~$35-50/month -
<strong>Alternative:</strong> Modal for burst capacity + CPU VPS for
base load</p>
<p><strong>Scenario 4: High-Volume Production (100+ hours
audio/day)</strong> - <strong>Best Option:</strong> Dedicated GPU cloud
(RunPod/Vast.ai) or multiple CPU VPS - <strong>Reasoning:</strong>
Cost-effective at scale - <strong>Cost:</strong> $250-500/month</p>
<h3 id="your-specific-situation-solo-user-custom-model">Your Specific
Situation (Solo User, Custom Model)</h3>
<p>Given your setup (AMD GPU with ROCm), here’s what I’d recommend:</p>
<p><strong>Option A: Local 24/7 (Recommended)</strong></p>
<div class="sourceCode" id="cb127"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Zero additional hardware cost <span class="er">(</span><span class="ex">you</span> have RX 7700 XT<span class="kw">)</span></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Whisper runs well on ROCm <span class="er">(</span><span class="va">HSA_OVERRIDE_GFX_VERSION</span><span class="op">=</span>11.0.1<span class="kw">)</span></span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Can expose via Cloudflare Tunnel <span class="er">(</span><span class="fu">free</span>, secure, no port forwarding<span class="kw">)</span></span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> Total cost: ~<span class="va">$1</span>5-20/month in electricity</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> Docker container with whisper.cpp or faster-whisper</span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> FastAPI wrapper for REST API</span>
<span id="cb127-10"><a href="#cb127-10" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> Cloudflare Tunnel for secure external access</span>
<span id="cb127-11"><a href="#cb127-11" aria-hidden="true" tabindex="-1"></a><span class="ex">4.</span> Optional: Nginx reverse proxy for API management</span></code></pre></div>
<p><strong>Option B: Hybrid (Local + Serverless Fallback)</strong></p>
<div class="sourceCode" id="cb128"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Option C: CPU VPS (If You Don’t Want Local Running
24/7)</strong></p>
<div class="sourceCode" id="cb129"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="practical-cost-calculation-examples">Practical Cost Calculation
Examples</h3>
<p><strong>Scenario: Processing 5 hours of audio per day</strong></p>
<table>
<thead>
<tr class="header">
<th>Option</th>
<th>Daily Cost</th>
<th>Monthly Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Replicate (10x RT)</td>
<td>$1.80</td>
<td>$54</td>
<td>Quick bursts</td>
</tr>
<tr class="even">
<td>Modal (realtime)</td>
<td>$1.20</td>
<td>$36</td>
<td>Python-friendly</td>
</tr>
<tr class="odd">
<td>Local (Your GPU)</td>
<td>$0.50</td>
<td>$15</td>
<td>Electricity only</td>
</tr>
<tr class="even">
<td>Hetzner CPU VPS</td>
<td>$1.10</td>
<td>$33</td>
<td>Always available</td>
</tr>
<tr class="odd">
<td>Vast.ai GPU (spot)</td>
<td>$2.40</td>
<td>$72</td>
<td>Fast processing</td>
</tr>
</tbody>
</table>
<p><strong>Verdict for Solo User:</strong> Local deployment on your
existing hardware is by far the most cost-effective for 24/7
availability.</p>
<h3 id="exposuresecurity-considerations">Exposure/Security
Considerations</h3>
<p>If running locally and exposing to internet:</p>
<ol type="1">
<li><p><strong>Never expose ports directly</strong> - major security
risk</p></li>
<li><p><strong>Use Cloudflare Tunnel</strong> (recommended):</p>
<div class="sourceCode" id="cb130"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Free, secure, no port forwarding needed</span></span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a><span class="ex">cloudflared</span> tunnel create my-asr</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates encrypted tunnel from your server to Cloudflare edge</span></span></code></pre></div></li>
<li><p><strong>Alternative: Tailscale</strong> - Private mesh network
(free for personal use)</p></li>
<li><p><strong>API Authentication:</strong> Always implement API
keys/tokens</p></li>
<li><p><strong>Rate Limiting:</strong> Prevent abuse with request
limits</p></li>
<li><p><strong>HTTPS Only:</strong> Cloudflare provides this
automatically</p></li>
</ol>
<h3 id="advanced-options-for-solo-users">Advanced Options for Solo
Users</h3>
<p><strong>Option: Fly.io</strong> - Deploy containers globally - Pay
per request (scales to zero) - ~$0.0008/sec GPU or $0.00025/sec CPU -
Good middle ground between VPS and serverless</p>
<p><strong>Option: Railway.app</strong> - $5/month base + usage - Good
for hobby projects - No GPU support (CPU only)</p>
<p><strong>Option: Self-hosted on Oracle Cloud Free Tier</strong> - 4
ARM cores, 24GB RAM - completely free forever - Can run CPU inference -
Performance: Slower than x86, but usable for Whisper-base/small - Great
for experimentation</p>
<h3 id="final-recommendation-for-you">Final Recommendation for You</h3>
<p>Based on your setup and likely use pattern:</p>
<ol type="1">
<li><strong>Start with local deployment</strong> on your RX 7700 XT
<ul>
<li>Use Docker + faster-whisper with ROCm</li>
<li>Expose via Cloudflare Tunnel</li>
<li>Cost: ~$15-20/month electricity</li>
<li>Benefit: Full control, lowest cost, instant inference</li>
</ul></li>
<li><strong>Add Modal as backup</strong> for when local is down
<ul>
<li>Minimal cost if rarely used</li>
<li>Python-friendly deployment</li>
<li>Automatic fallback logic in your client</li>
</ul></li>
<li><strong>If you outgrow local:</strong> Migrate to Hetzner CPU VPS
<ul>
<li>Still cheaper than GPU cloud options</li>
<li>Professional reliability</li>
<li>~$35/month predictable cost</li>
</ul></li>
</ol>
<h3 id="code-example-local-deployment-with-cloudflare-tunnel">Code
Example: Local Deployment with Cloudflare Tunnel</h3>
<div class="sourceCode" id="cb131"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fu">wget</span> <span class="at">-q</span> https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> dpkg <span class="at">-i</span> cloudflared-linux-amd64.deb</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a><span class="ex">cloudflared</span> tunnel login</span>
<span id="cb131-7"><a href="#cb131-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-8"><a href="#cb131-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-9"><a href="#cb131-9" aria-hidden="true" tabindex="-1"></a><span class="ex">cloudflared</span> tunnel create my-asr-api</span>
<span id="cb131-10"><a href="#cb131-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-11"><a href="#cb131-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-12"><a href="#cb131-12" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span> <span class="op">&gt;</span> ~/.cloudflared/config.yml <span class="op">&lt;&lt; EOF</span></span>
<span id="cb131-13"><a href="#cb131-13" aria-hidden="true" tabindex="-1"></a><span class="st">tunnel: my-asr-api</span></span>
<span id="cb131-14"><a href="#cb131-14" aria-hidden="true" tabindex="-1"></a><span class="st">credentials-file: /home/daniel/.cloudflared/&lt;tunnel-id&gt;.json</span></span>
<span id="cb131-15"><a href="#cb131-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-16"><a href="#cb131-16" aria-hidden="true" tabindex="-1"></a><span class="st">ingress:</span></span>
<span id="cb131-17"><a href="#cb131-17" aria-hidden="true" tabindex="-1"></a><span class="st">  - hostname: asr.yourdomain.com</span></span>
<span id="cb131-18"><a href="#cb131-18" aria-hidden="true" tabindex="-1"></a><span class="st">    service: http://localhost:8000</span></span>
<span id="cb131-19"><a href="#cb131-19" aria-hidden="true" tabindex="-1"></a><span class="st">  - service: http_status:404</span></span>
<span id="cb131-20"><a href="#cb131-20" aria-hidden="true" tabindex="-1"></a><span class="op">EOF</span></span>
<span id="cb131-21"><a href="#cb131-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-22"><a href="#cb131-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-23"><a href="#cb131-23" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> cloudflared service install</span>
<span id="cb131-24"><a href="#cb131-24" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> systemctl start cloudflared</span>
<span id="cb131-25"><a href="#cb131-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb131-26"><a href="#cb131-26" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<h3 id="conclusion-5">Conclusion</h3>
<p>For a solo user with a custom ASR model: - <strong>Serverless options
exist beyond Replicate</strong> (Modal, HF Inference Endpoints, Baseten,
Fly.io) - <strong>Local 24/7 deployment is surprisingly
affordable</strong> (~$15-30/month) using existing hardware -
<strong>Cloud VPS CPU instances</strong> are the sweet spot for
“always-on” without managing local hardware (~$35/month) - <strong>GPU
cloud is expensive for 24/7</strong> ($250-500/month) but reasonable for
burst usage - <strong>Your specific situation:</strong> Local deployment
on RX 7700 XT + Cloudflare Tunnel is the optimal approach</p>
<p>The niche you’re in (solo user with custom ASR) actually has more
options than you might think, and costs can be quite reasonable with the
right architecture!</p>
<hr />
<p><em>Note: This response was generated by Claude Code as part of
Daniel’s STT Fine-Tuning Notebook project. Pricing information is
approximate and based on 2025 rates. Always verify current pricing with
service providers.</em></p>
<h2 id="live-vs-batch-transcription">Live Vs Batch Transcription</h2>
<h2 id="overview-1">Overview</h2>
<p>ASR systems typically operate in two distinct modes:</p>
<ol type="1">
<li><strong>Live/streaming transcription:</strong> Real-time
transcription as you speak, with text appearing incrementally</li>
<li><strong>Batch/file transcription:</strong> Upload a complete audio
file and receive the full transcription after processing</li>
</ol>
<p>While these often use the same underlying model (e.g., Whisper),
there are significant architectural and accuracy differences between
these approaches.</p>
<h2 id="architectural-differences">Architectural Differences</h2>
<h3 id="livestreaming-transcription">Live/Streaming Transcription</h3>
<p><strong>How it works:</strong></p>
<ol type="1">
<li><strong>Audio buffering:</strong> Audio is captured in small chunks
(typically 0.5-3 seconds)</li>
<li><strong>Continuous processing:</strong> Each chunk is processed as
it arrives, with minimal delay</li>
<li><strong>Context windowing:</strong> The model maintains a sliding
context window, using previous chunks to inform current
transcription</li>
<li><strong>Incremental output:</strong> Text appears progressively as
each chunk is transcribed</li>
<li><strong>Voice Activity Detection (VAD):</strong> System detects when
you’re speaking vs silent to determine chunk boundaries</li>
</ol>
<p><strong>Technical implementation:</strong></p>
<pre><code>Audio stream → VAD → Chunking (0.5-3s) → Buffering → Model inference → Text output
                ↓
         Context window (previous 30s typically maintained)</code></pre>
<p><strong>Constraints:</strong></p>
<ul>
<li><strong>Fixed latency requirements:</strong> Must process within
~100-500ms to feel “real-time”</li>
<li><strong>Limited context:</strong> Can only look back at recent audio
(typically 30 seconds maximum)</li>
<li><strong>No future context:</strong> Cannot see what comes next in
the sentence</li>
<li><strong>Chunk boundary issues:</strong> Must make decisions about
incomplete phrases</li>
<li><strong>Computational pressure:</strong> Must process continuously
without falling behind</li>
</ul>
<h3 id="batchfile-transcription">Batch/File Transcription</h3>
<p><strong>How it works:</strong></p>
<ol type="1">
<li><strong>Complete file upload:</strong> Entire audio file is
available before processing begins</li>
<li><strong>Preprocessing:</strong> Can apply audio normalization, noise
reduction, and enhancement to the entire file</li>
<li><strong>Optimal segmentation:</strong> Can analyze the entire audio
to find natural boundaries (pauses, speaker changes)</li>
<li><strong>Full context processing:</strong> Model can use surrounding
context from the entire recording</li>
<li><strong>Multi-pass processing:</strong> Can make multiple passes
over ambiguous sections</li>
<li><strong>Post-processing:</strong> Can apply additional cleanup,
punctuation restoration, and confidence-based corrections</li>
</ol>
<p><strong>Technical implementation:</strong></p>
<pre><code>Complete audio file → Preprocessing → Optimal chunking → Parallel processing
                                              ↓
                                    Full context available
                                              ↓
                                    Post-processing &amp; refinement
                                              ↓
                                    Final transcription</code></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>No latency constraints:</strong> Can take as long as needed
for optimal results</li>
<li><strong>Full bidirectional context:</strong> Can look both backward
and forward</li>
<li><strong>Better segmentation:</strong> Can find optimal chunk
boundaries after analyzing the whole file</li>
<li><strong>Multiple passes:</strong> Can revisit uncertain sections
with more context</li>
<li><strong>Better preprocessing:</strong> Can apply sophisticated audio
enhancement knowing the full characteristics</li>
</ul>
<h2 id="why-batch-transcription-often-performs-better">Why Batch
Transcription Often Performs Better</h2>
<p>The perception that batch transcription is more accurate is
<strong>not imagination</strong>—there are real technical reasons:</p>
<h3 id="bidirectional-context">1. <strong>Bidirectional
Context</strong></h3>
<ul>
<li><strong>Live:</strong> Can only look backward (previous 30 seconds
typically)</li>
<li><strong>Batch:</strong> Can look both backward AND forward</li>
<li><strong>Impact:</strong> Understanding upcoming context helps
disambiguate current words (e.g., knowing someone will say “bank
account” vs “river bank”)</li>
</ul>
<h3 id="optimal-chunk-boundaries">2. <strong>Optimal Chunk
Boundaries</strong></h3>
<ul>
<li><strong>Live:</strong> Must chunk based on real-time constraints,
sometimes cutting mid-phrase</li>
<li><strong>Batch:</strong> Can analyze entire audio to find natural
pauses and segment at optimal points</li>
<li><strong>Impact:</strong> Models perform better when chunks align
with linguistic boundaries (sentence/phrase endings)</li>
</ul>
<h3 id="audio-preprocessing">3. <strong>Audio
Preprocessing</strong></h3>
<ul>
<li><strong>Live:</strong> Limited preprocessing (simple noise gating,
maybe basic noise reduction)</li>
<li><strong>Batch:</strong> Can analyze full audio characteristics and
apply:
<ul>
<li>Sophisticated noise profiling and removal</li>
<li>Dynamic range compression optimized for the specific recording</li>
<li>Spectral enhancement tuned to the speaker’s voice
characteristics</li>
</ul></li>
<li><strong>Impact:</strong> Cleaner audio input = better transcription
accuracy</li>
</ul>
<h3 id="no-pressure-for-real-time-performance">4. <strong>No Pressure
for Real-Time Performance</strong></h3>
<ul>
<li><strong>Live:</strong> Must use faster, sometimes less accurate
inference settings</li>
<li><strong>Batch:</strong> Can use slower, more accurate inference
parameters:
<ul>
<li>Higher beam search width</li>
<li>More sophisticated language model scoring</li>
<li>Temperature sampling for better alternatives</li>
</ul></li>
<li><strong>Impact:</strong> 5-15% accuracy improvement possible with
more computational resources</li>
</ul>
<h3 id="error-correction-opportunities">5. <strong>Error Correction
Opportunities</strong></h3>
<ul>
<li><strong>Live:</strong> Text is output immediately, limited ability
to revise</li>
<li><strong>Batch:</strong> Can apply post-processing:
<ul>
<li>Confidence-based revision</li>
<li>Language model rescoring</li>
<li>Consistency checking across the full transcript</li>
</ul></li>
<li><strong>Impact:</strong> Can catch and correct errors that seem
wrong in broader context</li>
</ul>
<h3 id="speaker-adaptation">6. <strong>Speaker Adaptation</strong></h3>
<ul>
<li><strong>Live:</strong> Limited adaptation in first 30-60
seconds</li>
<li><strong>Batch:</strong> Can analyze the entire recording first to:
<ul>
<li>Identify speaker characteristics</li>
<li>Build speaker-specific acoustic model adjustments</li>
<li>Learn vocabulary and speaking patterns used throughout</li>
</ul></li>
<li><strong>Impact:</strong> Better performance on uncommon
pronunciations and speaking styles</li>
</ul>
<h2 id="api-architecture-differences">API Architecture Differences</h2>
<p>Most ASR service providers (OpenAI, AssemblyAI, Deepgram, etc.) use
<strong>different endpoints</strong> for live vs batch:</p>
<h3 id="streaming-endpoints">Streaming Endpoints</h3>
<ul>
<li>Use WebSocket connections for bidirectional communication</li>
<li>Implement different inference optimizations (speed over
accuracy)</li>
<li>May use lighter model variants</li>
<li>Limited preprocessing capabilities</li>
<li>Stateful connections with context management</li>
</ul>
<h3 id="batch-endpoints">Batch Endpoints</h3>
<ul>
<li>Use standard HTTP POST with file upload</li>
<li>Implement full inference optimizations (accuracy over speed)</li>
<li>May use larger/better model variants</li>
<li>Full preprocessing pipeline</li>
<li>Stateless processing with full context available</li>
</ul>
<h2 id="the-15-minute-recording-scenario">The 15-Minute Recording
Scenario</h2>
<p>Let’s compare your two approaches for a 15-minute recording:</p>
<h3 id="approach-1-live-transcription-with-30-second-chunks">Approach 1:
Live transcription with 30-second chunks</h3>
<p><strong>What happens:</strong> - Audio captured in ~30 half-second
chunks - Each chunk processed with context from previous ~30 seconds -
Model makes ~30 independent inference decisions - Text appears
progressively - Total processing: 15 minutes of real-time processing</p>
<p><strong>Accuracy factors:</strong> - ✗ Forward context not available
- ✗ Chunk boundaries not optimized - ✗ Limited preprocessing - ✗ Fast
inference parameters required - ✗ No multi-pass opportunities</p>
<h3 id="approach-2-record-in-audacity-upload-mp3-transcribe">Approach 2:
Record in Audacity → upload MP3 → transcribe</h3>
<p><strong>What happens:</strong> - Complete 15-minute audio file
available - System analyzes full audio for characteristics - Optimal
chunk boundaries identified (perhaps 60-90 chunks at natural pauses) -
Each chunk processed with full recording context - Post-processing
applied to final transcript - Total processing: 1-3 minutes</p>
<p><strong>Accuracy factors:</strong> - ✓ Full bidirectional context - ✓
Optimized chunk boundaries - ✓ Full preprocessing applied - ✓ Optimal
inference parameters - ✓ Post-processing applied</p>
<p><strong>Expected accuracy difference:</strong> 5-20% word error rate
improvement, depending on audio quality and content complexity</p>
<h2 id="when-live-transcription-makes-sense">When Live Transcription
Makes Sense</h2>
<p>Despite the accuracy tradeoffs, live transcription is valuable
for:</p>
<ol type="1">
<li><strong>Interactive applications:</strong> Dictation, voice
commands, live captions</li>
<li><strong>Immediate feedback needs:</strong> Making corrections while
recording</li>
<li><strong>Long recordings:</strong> Don’t want to wait 2 hours for a
2-hour meeting</li>
<li><strong>Memory constraints:</strong> Can’t store entire large audio
file</li>
<li><strong>Privacy concerns:</strong> Don’t want to upload complete
files</li>
</ol>
<h2 id="recommendations-for-best-results">Recommendations for Best
Results</h2>
<h3 id="for-live-transcription">For Live Transcription:</h3>
<ol type="1">
<li><strong>Use models optimized for streaming:</strong> Some Whisper
variants are specifically tuned for streaming</li>
<li><strong>Ensure good audio quality:</strong> Use quality microphone,
quiet environment</li>
<li><strong>Speak clearly with pauses:</strong> Help the VAD and
chunking</li>
<li><strong>Use longer context windows:</strong> If supported (e.g.,
45-60 seconds vs 30)</li>
<li><strong>Consider hybrid approaches:</strong> Live transcription with
post-recording refinement pass</li>
</ol>
<h3 id="for-batch-transcription">For Batch Transcription:</h3>
<ol type="1">
<li><strong>Use highest quality audio:</strong> Record at 16kHz+ sample
rate, minimal compression</li>
<li><strong>Include silence at start/end:</strong> Helps with processing
boundary issues</li>
<li><strong>Use lossless formats when possible:</strong> WAV/FLAC better
than MP3</li>
<li><strong>Segment very long files:</strong> Break multi-hour
recordings into 30-60 minute segments</li>
<li><strong>Use provider’s best quality tier:</strong> Most services
offer “fast” vs “accurate” tiers</li>
</ol>
<h2 id="technical-deep-dive-chunking-in-live-transcription">Technical
Deep Dive: Chunking in Live Transcription</h2>
<p>Under the hood during live transcription:</p>
<div class="sourceCode" id="cb134"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>audio_buffer <span class="op">=</span> []</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a>context_window <span class="op">=</span> []</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> recording:</span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Capture audio chunk (e.g., 30ms)</span></span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>    chunk <span class="op">=</span> capture_audio(<span class="dv">30</span><span class="er">ms</span>)</span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>    audio_buffer.append(chunk)</span>
<span id="cb134-9"><a href="#cb134-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-10"><a href="#cb134-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># When buffer reaches processing size (e.g., 1 second)</span></span>
<span id="cb134-11"><a href="#cb134-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(audio_buffer) <span class="op">&gt;=</span> processing_size:</span>
<span id="cb134-12"><a href="#cb134-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Combine with context window</span></span>
<span id="cb134-13"><a href="#cb134-13" aria-hidden="true" tabindex="-1"></a>        input_audio <span class="op">=</span> context_window <span class="op">+</span> audio_buffer</span>
<span id="cb134-14"><a href="#cb134-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-15"><a href="#cb134-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run inference</span></span>
<span id="cb134-16"><a href="#cb134-16" aria-hidden="true" tabindex="-1"></a>        transcription <span class="op">=</span> model.transcribe(input_audio)</span>
<span id="cb134-17"><a href="#cb134-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-18"><a href="#cb134-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output new text</span></span>
<span id="cb134-19"><a href="#cb134-19" aria-hidden="true" tabindex="-1"></a>        output(transcription.new_text)</span>
<span id="cb134-20"><a href="#cb134-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-21"><a href="#cb134-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update context window (sliding window)</span></span>
<span id="cb134-22"><a href="#cb134-22" aria-hidden="true" tabindex="-1"></a>        context_window <span class="op">=</span> audio_buffer[<span class="op">-</span>context_size:]</span>
<span id="cb134-23"><a href="#cb134-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb134-24"><a href="#cb134-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Clear buffer</span></span>
<span id="cb134-25"><a href="#cb134-25" aria-hidden="true" tabindex="-1"></a>        audio_buffer <span class="op">=</span> []</span></code></pre></div>
<p><strong>Key points:</strong></p>
<ul>
<li>The model isn’t truly processing “live”—it’s processing discrete
chunks rapidly</li>
<li>Context window maintains recent audio for better accuracy</li>
<li>Each inference sees only current chunk + recent context</li>
<li>Decisions are made incrementally and can’t easily be revised</li>
</ul>
<h2 id="conclusion-6">Conclusion</h2>
<p>Yes, batch transcription generally provides better accuracy than live
transcription due to:</p>
<ul>
<li>Full bidirectional context</li>
<li>Optimal preprocessing and segmentation</li>
<li>Ability to use more sophisticated inference parameters</li>
<li>Post-processing opportunities</li>
<li>No real-time latency constraints</li>
</ul>
<p>For your 15-minute recording scenario, recording in Audacity and
uploading will almost certainly produce more accurate results than live
transcription, typically with 5-20% better word error rates, especially
for:</p>
<ul>
<li>Technical terminology</li>
<li>Proper nouns</li>
<li>Ambiguous words that need sentence context</li>
<li>Challenging audio conditions</li>
</ul>
<p>The tradeoff is waiting for processing rather than getting immediate
feedback, but if accuracy is the priority, batch processing is the
better choice.</p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
<h2 id="second-gpu-for-stt-workloads">Second Gpu For Stt Workloads</h2>
<h2 id="question-summary-4">Question Summary</h2>
<p>Daniel is considering hardware upgrade strategies for his desktop
setup. He currently uses an AMD GPU (chosen before getting into AI/STT
work) and primarily uses speech-to-text throughout his workday. He’s
evaluating two options: 1. Replace the AMD GPU with an NVIDIA GPU 2.
Keep the AMD GPU for display output and add a second GPU dedicated to
STT processing</p>
<p>The question extends to whether a GPU, NPU, or TPU would be optimal
for STT-only workloads, given that local LLM usage is minimal.</p>
<h2
id="recommendation-strategic-hardware-considerations">Recommendation:
Strategic Hardware Considerations</h2>
<h3 id="primary-recommendation-single-nvidia-gpu-next-build">Primary
Recommendation: Single NVIDIA GPU (Next Build)</h3>
<p>For your next build or major upgrade, I’d recommend <strong>replacing
your AMD GPU with a single NVIDIA GPU</strong> rather than adding a
second GPU. Here’s why:</p>
<p><strong>Advantages:</strong> - <strong>Simplified system
management</strong>: One GPU means less power consumption, heat, driver
complexity, and PCIe lane allocation issues - <strong>CUDA ecosystem
dominance</strong>: The vast majority of AI/ML tools (including STT)
have first-class CUDA support with better optimization -
<strong>Flexibility</strong>: A single NVIDIA GPU can handle both
display and AI workloads efficiently - <strong>Better per-dollar
performance</strong>: You get more AI performance for your money with a
single higher-tier NVIDIA card than splitting budget across two GPUs -
<strong>Lower power draw</strong>: Modern NVIDIA GPUs (especially 4000
series) are remarkably power-efficient for AI workloads</p>
<p><strong>Recommended GPU Tiers for STT + Light LLM:</strong></p>
<ol type="1">
<li><strong>Budget Option (~$500-600)</strong>: NVIDIA RTX 4060 Ti 16GB
<ul>
<li>16GB VRAM is crucial for larger Whisper models and
future-proofing</li>
<li>Excellent for STT inference (Whisper large-v3 runs smoothly)</li>
<li>Can handle local LLMs up to 13B parameters reasonably well</li>
<li>Low power consumption (~160W TDP)</li>
</ul></li>
<li><strong>Mid-Range Sweet Spot (~$800-1000)</strong>: NVIDIA RTX 4070
Ti / 4070 Ti Super
<ul>
<li>12GB VRAM (4070 Ti) or 16GB VRAM (4070 Ti Super)</li>
<li>Significantly faster inference for Whisper</li>
<li>Better headroom for local LLM experimentation</li>
<li>Still reasonable power draw (~285W TDP)</li>
</ul></li>
<li><strong>High-End Option (~$1200-1500)</strong>: NVIDIA RTX 4080 /
4080 Super
<ul>
<li>16GB VRAM</li>
<li>Overkill for STT alone, but excellent for any AI workload you might
explore</li>
<li>Near-workstation performance for AI tasks</li>
</ul></li>
</ol>
<h3 id="why-not-a-second-gpu">Why Not a Second GPU?</h3>
<p><strong>Technical Drawbacks:</strong> - <strong>PCIe lane
limitations</strong>: Most consumer motherboards don’t have enough PCIe
lanes to run two GPUs at full bandwidth, meaning you’d likely run both
at x8 instead of x16 - <strong>Power supply requirements</strong>: You’d
need a significantly larger PSU (likely 1000W+) - <strong>Heat and
cooling</strong>: Two GPUs generate substantial heat; your case might
not have adequate cooling - <strong>Driver complexity</strong>: Running
AMD for display + NVIDIA for compute adds driver management overhead -
<strong>ROCm limitations</strong>: Your current AMD GPU already
struggles with ROCm support for AI (as you’ve likely experienced), so
keeping it doesn’t provide much benefit</p>
<p><strong>Cost Consideration:</strong> A mid-range NVIDIA GPU (~$800)
would likely provide better AI performance than your current AMD GPU + a
budget NVIDIA card costing the same total amount.</p>
<h3 id="gpu-vs-npu-vs-tpu-for-stt">GPU vs NPU vs TPU for STT</h3>
<p><strong>GPU (Recommended for STT):</strong> - ✅ Best option for STT
workloads - ✅ Whisper and similar models are heavily optimized for GPU
- ✅ Flexibility for other AI tasks (image generation, LLMs) - ✅ Mature
software ecosystem (PyTorch, ONNX, faster-whisper, CTranslate2)</p>
<p><strong>NPU (Neural Processing Unit):</strong> - ❌ Not recommended
for desktop STT - NPUs are designed for low-power inference on
mobile/edge devices - Poor software support for Whisper models on NPUs -
Would require significant model conversion/quantization work -
Performance would likely be worse than GPU for your use case - Examples:
Intel’s AI Boost, Qualcomm’s Hexagon NPU (laptop/mobile chips)</p>
<p><strong>TPU (Tensor Processing Unit):</strong> - ❌ Not practical for
consumer desktop use - TPUs are Google’s proprietary accelerators (Cloud
TPU or Google Edge TPU) - Edge TPUs are underpowered for real-time STT
of Whisper-scale models - Cloud TPUs are rental-only and prohibitively
expensive for continuous STT use - Limited software compatibility with
Whisper ecosystem</p>
<h3 id="special-consideration-if-you-must-keep-current-amd-gpu">Special
Consideration: If You Must Keep Current AMD GPU</h3>
<p>If you’re not ready for a full build and want to add a second GPU
with your current setup, here’s what to consider:</p>
<p><strong>Prerequisites:</strong> - Verify your motherboard has a
second PCIe x16 slot (or at least x8) - Ensure adequate PCIe lane
allocation from CPU - Check power supply capacity (likely need 850W+ for
dual-GPU) - Verify case airflow can handle additional heat</p>
<p><strong>Budget Second GPU Options (~$300-400):</strong> -
<strong>NVIDIA RTX 3060 12GB</strong> (used market): Good VRAM for STT,
reasonable performance - <strong>NVIDIA RTX 4060 8GB</strong> (new):
Newer architecture but limited VRAM</p>
<p><strong>Setup Configuration:</strong> - AMD GPU: Primary display
output - NVIDIA GPU: Dedicated to CUDA compute (STT, AI workloads) - Use
<code>CUDA_VISIBLE_DEVICES</code> environment variable to explicitly
route workloads to NVIDIA GPU - Set display manager to use AMD GPU to
avoid NVIDIA driver overhead on display tasks</p>
<h3 id="practical-implementation-for-stt-workloads">Practical
Implementation for STT Workloads</h3>
<p>Regardless of which option you choose, here’s how to optimize for
STT:</p>
<p><strong>Software Stack:</strong> 1. <strong>faster-whisper</strong>
(recommended): CTranslate2-based, highly optimized, low VRAM usage -
large-v3 model runs well on 8GB VRAM - 2-3x faster than OpenAI’s Whisper
implementation - Significantly lower memory footprint</p>
<ol start="2" type="1">
<li><strong>whisper.cpp</strong>: If you want CPU fallback option
<ul>
<li>Uses CUDA when available</li>
<li>Excellent quantized model support</li>
</ul></li>
<li><strong>Hugging Face Transformers</strong>: If you need fine-tuning
capabilities
<ul>
<li>More VRAM intensive</li>
<li>Slower inference than faster-whisper</li>
</ul></li>
</ol>
<p><strong>VRAM Requirements by Whisper Model:</strong> | Model Size |
Minimum VRAM (faster-whisper) | Recommended VRAM |
|————|——————————-|——————| | tiny | 1GB | 2GB | | base | 1GB | 2GB | |
small | 2GB | 4GB | | medium | 4GB | 6GB | | large-v2/v3| 6GB | 10GB
|</p>
<p><strong>Real-Time STT Performance Targets:</strong> - For real-time
transcription (1x speed or faster), you want 4GB+ VRAM - For comfortable
headroom with large-v3 and parallel processing, 12GB+ VRAM is ideal</p>
<h3 id="timeline-recommendation">Timeline Recommendation</h3>
<p><strong>Immediate (if needed):</strong> - Continue using your AMD GPU
with ROCm for STT - Consider <code>whisper.cpp</code> with CPU
offloading if ROCm is problematic</p>
<p><strong>Short-term (3-6 months):</strong> - If STT performance is
blocking your workflow, consider a used RTX 3060 12GB as a second GPU
stopgap - Only if dual-GPU setup is viable on your current system</p>
<p><strong>Next build/major upgrade (12-24 months):</strong> - Replace
with single NVIDIA RTX 4070 Ti Super 16GB or equivalent next-gen card -
This will serve you better than any dual-GPU configuration</p>
<h3 id="additional-considerations">Additional Considerations</h3>
<p><strong>Power Efficiency:</strong> Modern NVIDIA GPUs have excellent
idle power management. If you’re running STT intermittently throughout
the day (not 24/7), the GPU will mostly idle at 10-30W, spiking only
during active transcription.</p>
<p><strong>Future-Proofing:</strong> STT models are trending toward
larger, more capable architectures (Whisper large-v3, Distil-Whisper,
Canary). Having 16GB VRAM provides headroom for these developments.</p>
<p><strong>Local LLM Consideration:</strong> If you expand your local
LLM usage, 16GB VRAM enables: - 13B parameter models at good speed (Q4
quantization) - 7B parameter models at full precision - Simultaneous STT
+ small LLM workloads</p>
<h2 id="summary-1">Summary</h2>
<p><strong>Best Path Forward:</strong> 1. <strong>Next build</strong>:
Single NVIDIA RTX 4070 Ti Super 16GB (or equivalent) 2. <strong>Current
system</strong>: Continue with AMD + ROCm or consider budget second
NVIDIA GPU only if current performance is blocking work 3.
<strong>Hardware type</strong>: GPU only—NPUs and TPUs are not suitable
for desktop STT workloads</p>
<p>The single powerful NVIDIA GPU approach provides the best balance of
performance, flexibility, power efficiency, and system simplicity for
your STT-focused workload.</p>
<hr />
<p><em>Generated by Claude Code (Anthropic) - Please validate
recommendations against your specific motherboard, PSU, and case
specifications before purchasing.</em></p>
<h1 id="part-vi-amd-gpu-optimization">Part VI: AMD GPU Optimization</h1>
<p><em>AMD-specific hardware considerations</em></p>
<hr />
<h2 id="amd-gpu-engines-comparison">Amd Gpu Engines Comparison</h2>
<h2 id="question-2">Question</h2>
<p>With an AMD GPU (rather than NVIDIA), what are the best inference
engines for ASR that have good AMD/ROCm support? And when converting
models from safe-tensors format after fine-tuning, which formats should
you target to work well with AMD GPUs?</p>
<h2 id="answer-6">Answer</h2>
<p>Running speech-to-text (STT) workloads locally on AMD GPUs presents
unique challenges compared to NVIDIA’s more mature CUDA ecosystem. This
guide provides a comprehensive comparison of the most reliable engines
for AMD GPU-accelerated STT inference.</p>
<h2 id="current-state-of-amd-gpu-support-for-stt">Current State of AMD
GPU Support for STT</h2>
<h3 id="the-challenge">The Challenge</h3>
<p>AMD GPU support for AI workloads relies primarily on ROCm (Radeon
Open Compute), which has historically lagged behind NVIDIA’s CUDA in
terms of software support and ecosystem maturity. Many popular inference
engines were built with CUDA as the primary target, with AMD support
added later or not at all.</p>
<h2 id="engine-comparison">Engine Comparison</h2>
<h3 id="faster-whisper-recommended">1. <strong>Faster-Whisper</strong>
(Recommended)</h3>
<p><strong>Status:</strong> Most reliable option for AMD GPU
acceleration</p>
<p><strong>Key Details:</strong> - Uses CTranslate2 backend, which has
ROCm support - Supports AMD GPUs through ROCm (tested with gfx1100,
gfx1101, gfx1030, and other RDNA architectures) - Offers 3-4x faster
inference than OpenAI’s Whisper while maintaining accuracy - Lower VRAM
requirements than original Whisper</p>
<p><strong>Installation:</strong></p>
<div class="sourceCode" id="cb135"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install faster-whisper</span></code></pre></div>
<p><strong>ROCm Requirements:</strong> - ROCm 5.4+ recommended - Proper
<code>HSA_OVERRIDE_GFX_VERSION</code> may be needed for some cards - For
RDNA 3 (RX 7000 series): <code>HSA_OVERRIDE_GFX_VERSION=11.0.0</code> or
<code>11.0.1</code></p>
<p><strong>Verification of GPU Usage:</strong></p>
<div class="sourceCode" id="cb136"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 rocm-smi</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 <span class="st">&#39;rocm-smi --showuse --showmeminfo vram&#39;</span></span></code></pre></div>
<p><strong>Pros:</strong> - Best balance of speed, accuracy, and AMD GPU
support - Active development and community - Good documentation for ROCm
setup</p>
<p><strong>Cons:</strong> - Initial setup can be tricky - ROCm version
compatibility issues</p>
<h3 id="whisper.cpp">2. <strong>whisper.cpp</strong></h3>
<p><strong>Status:</strong> Mixed AMD GPU support - unreliable for
production</p>
<p><strong>Key Details:</strong> - Primarily CPU-optimized (uses SIMD,
AVX, etc.) - HIP (ROCm) support exists but is experimental - Must be
compiled from source with specific flags for AMD GPU support - GPU
acceleration often doesn’t engage properly</p>
<p><strong>Why Your GPU Monitoring Showed No Activity:</strong> The app
you’re using likely: 1. Uses a pre-compiled binary without ROCm support
2. Falls back to CPU when GPU initialization fails 3. Doesn’t have
proper ROCm runtime environment configured</p>
<p><strong>When to Use:</strong> - CPU-only inference (where it excels)
- Embedded/edge devices - When you need minimal dependencies</p>
<p><strong>Pros:</strong> - Excellent CPU performance - Low memory
footprint - Fast for CPU-only workloads</p>
<p><strong>Cons:</strong> - AMD GPU support is experimental and
unreliable - Requires manual compilation with HIP support - Often falls
back to CPU silently</p>
<h3 id="openai-whisper-original">3. <strong>OpenAI Whisper
(Original)</strong></h3>
<p><strong>Status:</strong> No direct AMD GPU support through
PyTorch</p>
<p><strong>Key Details:</strong> - Built on PyTorch with CUDA backend -
PyTorch has experimental ROCm support (separate installation) - Slower
than optimized alternatives - Higher VRAM requirements</p>
<p><strong>ROCm PyTorch Installation:</strong></p>
<div class="sourceCode" id="cb137"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/rocm6.0</span></code></pre></div>
<p><strong>Pros:</strong> - Reference implementation - Most accurate
(though Faster-Whisper matches it) - Good for testing/validation</p>
<p><strong>Cons:</strong> - Slowest inference - ROCm PyTorch support is
hit-or-miss - High VRAM usage - Not optimized for inference</p>
<h3 id="whisper-jax">4. <strong>Whisper-JAX</strong></h3>
<p><strong>Status:</strong> Limited AMD support through experimental
ROCm JAX builds</p>
<p><strong>Key Details:</strong> - Built on JAX framework - JAX has
experimental ROCm support but very limited - Primarily designed for
TPU/CUDA</p>
<p><strong>When to Use:</strong> - You have specific JAX ROCm expertise
- Research/experimental workloads</p>
<p><strong>Recommendation:</strong> Avoid for AMD GPU production use</p>
<h3 id="whisper.onnx-onnx-runtime">5. <strong>Whisper.onnx / ONNX
Runtime</strong></h3>
<p><strong>Status:</strong> Growing AMD GPU support through DirectML and
ROCm</p>
<p><strong>Key Details:</strong> - ONNX Runtime has ROCm execution
provider - Can convert Whisper models to ONNX format - DirectML provider
works on Windows with AMD GPUs</p>
<p><strong>Installation:</strong></p>
<div class="sourceCode" id="cb138"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install onnxruntime-rocm</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> optimum.exporters.onnx <span class="at">--model</span> openai/whisper-base whisper-onnx/</span></code></pre></div>
<p><strong>Pros:</strong> - Cross-platform - Good optimization potential
- Growing ecosystem</p>
<p><strong>Cons:</strong> - Requires model conversion - ROCm provider
less mature than CUDA - More complex setup</p>
<h2 id="ranking-for-amd-gpu-users">Ranking for AMD GPU Users</h2>
<h3 id="tier-1-production-ready">Tier 1: Production-Ready</h3>
<ol type="1">
<li><strong>Faster-Whisper</strong> - Best overall choice for AMD GPUs
<ul>
<li>Reliability: ⭐⭐⭐⭐⭐</li>
<li>Performance: ⭐⭐⭐⭐⭐</li>
<li>Ease of Setup: ⭐⭐⭐⭐</li>
</ul></li>
</ol>
<h3 id="tier-2-workable-with-caveats">Tier 2: Workable with Caveats</h3>
<ol start="2" type="1">
<li><strong>OpenAI Whisper + ROCm PyTorch</strong> - Reference
implementation
<ul>
<li>Reliability: ⭐⭐⭐</li>
<li>Performance: ⭐⭐⭐</li>
<li>Ease of Setup: ⭐⭐⭐</li>
</ul></li>
<li><strong>ONNX Runtime (ROCm provider)</strong> - For specific use
cases
<ul>
<li>Reliability: ⭐⭐⭐</li>
<li>Performance: ⭐⭐⭐⭐</li>
<li>Ease of Setup: ⭐⭐</li>
</ul></li>
</ol>
<h3 id="tier-3-not-recommended-for-amd-gpu">Tier 3: Not Recommended for
AMD GPU</h3>
<ol start="4" type="1">
<li><strong>whisper.cpp</strong> - CPU-focused, unreliable GPU support
<ul>
<li>Reliability (GPU): ⭐⭐</li>
<li>Performance (CPU): ⭐⭐⭐⭐⭐</li>
<li>Ease of Setup (GPU): ⭐</li>
</ul></li>
<li><strong>Whisper-JAX</strong> - Limited ROCm support
<ul>
<li>Reliability: ⭐</li>
<li>Performance: N/A</li>
<li>Ease of Setup: ⭐</li>
</ul></li>
</ol>
<h2 id="practical-recommendations">Practical Recommendations</h2>
<h3 id="for-your-use-case">For Your Use Case</h3>
<p>Given that you’re using an app with whisper.cpp and not seeing GPU
activity, here’s what’s likely happening:</p>
<ol type="1">
<li><strong>The app is using CPU-only whisper.cpp</strong> - Most
pre-packaged apps don’t include ROCm-compiled versions</li>
<li><strong>GPU support is claimed but not functional</strong> - The app
may have been tested only with NVIDIA GPUs</li>
<li><strong>Silent fallback to CPU</strong> - whisper.cpp will use CPU
if GPU initialization fails</li>
</ol>
<h3 id="action-plan">Action Plan</h3>
<p><strong>Option A: Switch to Faster-Whisper (Recommended)</strong></p>
<div class="sourceCode" id="cb139"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install faster-whisper</span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="op">&lt;&lt; EOF</span></span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true" tabindex="-1"></a><span class="st">from faster_whisper import WhisperModel</span></span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-11"><a href="#cb139-11" aria-hidden="true" tabindex="-1"></a><span class="st">model = WhisperModel(&quot;base&quot;, device=&quot;cuda&quot;, compute_type=&quot;float16&quot;)</span></span>
<span id="cb139-12"><a href="#cb139-12" aria-hidden="true" tabindex="-1"></a><span class="st">segments, info = model.transcribe(&quot;audio.wav&quot;)</span></span>
<span id="cb139-13"><a href="#cb139-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-14"><a href="#cb139-14" aria-hidden="true" tabindex="-1"></a><span class="st">for segment in segments:</span></span>
<span id="cb139-15"><a href="#cb139-15" aria-hidden="true" tabindex="-1"></a><span class="st">    print(f&quot;[{segment.start:.2f}s -&gt; {segment.end:.2f}s] {segment.text}&quot;)</span></span>
<span id="cb139-16"><a href="#cb139-16" aria-hidden="true" tabindex="-1"></a><span class="op">EOF</span></span>
<span id="cb139-17"><a href="#cb139-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-18"><a href="#cb139-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-19"><a href="#cb139-19" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 rocm-smi</span></code></pre></div>
<p><strong>Option B: Verify whisper.cpp ROCm Support</strong> If you
want to stick with your current app: 1. Check if the app supports custom
whisper.cpp builds 2. Compile whisper.cpp with HIP support:</p>
<div class="sourceCode" id="cb140"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/ggerganov/whisper.cpp</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> whisper.cpp</span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> build <span class="kw">&amp;&amp;</span> <span class="bu">cd</span> build</span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cmake</span> .. <span class="at">-DWHISPER_HIPBLAS</span><span class="op">=</span>ON</span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span></span></code></pre></div>
<ol start="3" type="1">
<li>Replace the app’s whisper.cpp binary with your ROCm-enabled
build</li>
</ol>
<p><strong>Option C: Use PyTorch ROCm + Original Whisper</strong> For
research/development:</p>
<div class="sourceCode" id="cb141"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install torch torchvision torchaudio <span class="at">--index-url</span> https://download.pytorch.org/whl/rocm6.0</span>
<span id="cb141-3"><a href="#cb141-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-4"><a href="#cb141-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-5"><a href="#cb141-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> openai-whisper</span>
<span id="cb141-6"><a href="#cb141-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-7"><a href="#cb141-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb141-8"><a href="#cb141-8" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-c</span> <span class="st">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span></code></pre></div>
<h2 id="verifying-gpu-usage-on-amd">Verifying GPU Usage on AMD</h2>
<h3 id="rocm-system-management-interface">ROCm System Management
Interface</h3>
<div class="sourceCode" id="cb142"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb142-1"><a href="#cb142-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-2"><a href="#cb142-2" aria-hidden="true" tabindex="-1"></a><span class="ex">rocm-smi</span></span>
<span id="cb142-3"><a href="#cb142-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-4"><a href="#cb142-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-5"><a href="#cb142-5" aria-hidden="true" tabindex="-1"></a><span class="ex">rocm-smi</span> <span class="at">--showuse</span> <span class="at">--showmeminfo</span> vram <span class="at">--showtemp</span></span>
<span id="cb142-6"><a href="#cb142-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-7"><a href="#cb142-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb142-8"><a href="#cb142-8" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 rocm-smi</span></code></pre></div>
<h3 id="process-specific-gpu-usage">Process-Specific GPU Usage</h3>
<div class="sourceCode" id="cb143"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install radeontop</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="ex">radeontop</span></span>
<span id="cb143-4"><a href="#cb143-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-5"><a href="#cb143-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb143-6"><a href="#cb143-6" aria-hidden="true" tabindex="-1"></a><span class="ex">rocm-smi</span> <span class="at">--showpids</span></span></code></pre></div>
<h3 id="pytorch-rocm-verification">PyTorch ROCm Verification</h3>
<div class="sourceCode" id="cb144"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;ROCm available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;ROCm version: </span><span class="sc">{</span>torch<span class="sc">.</span>version<span class="sc">.</span>hip<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb144-4"><a href="#cb144-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Device name: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>get_device_name(<span class="dv">0</span>)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h2 id="common-pitfalls">Common Pitfalls</h2>
<h3 id="silent-cpu-fallback">1. Silent CPU Fallback</h3>
<p>Many inference engines will silently fall back to CPU if GPU
initialization fails. Always verify GPU usage with monitoring tools.</p>
<h3 id="hsa_override_gfx_version">2. HSA_OVERRIDE_GFX_VERSION</h3>
<p>RDNA 2/3 GPUs often need:</p>
<div class="sourceCode" id="cb145"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HSA_OVERRIDE_GFX_VERSION</span><span class="op">=</span>11.0.0  <span class="co"># For gfx1100 (RX 7900 XTX)</span></span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HSA_OVERRIDE_GFX_VERSION</span><span class="op">=</span>11.0.1  <span class="co"># For gfx1101 (RX 7800 XT)</span></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HSA_OVERRIDE_GFX_VERSION</span><span class="op">=</span>10.3.0  <span class="co"># For gfx1030 (RX 6000 series)</span></span></code></pre></div>
<h3 id="rocm-version-compatibility">3. ROCm Version Compatibility</h3>
<p>Different inference engines support different ROCm versions. Check
compatibility before installation.</p>
<h3 id="pre-compiled-binaries">4. Pre-compiled Binaries</h3>
<p>Most pre-compiled applications and Python wheels are CUDA-only. AMD
GPU support often requires: - Custom compilation - Specific ROCm wheels
- Environment configuration</p>
<h2 id="model-recommendations-for-amd-gpus">Model Recommendations for
AMD GPUs</h2>
<p>When using Faster-Whisper or other ROCm-enabled engines:</p>
<h3 id="vram-considerations">VRAM Considerations</h3>
<ul>
<li><strong>4-6GB VRAM:</strong> <code>tiny</code>, <code>base</code>,
<code>small</code> models</li>
<li><strong>8-12GB VRAM:</strong> <code>medium</code> model</li>
<li><strong>16GB+ VRAM:</strong> <code>large-v2</code>,
<code>large-v3</code> models</li>
</ul>
<h3 id="quantization">Quantization</h3>
<p>Faster-Whisper supports various quantization levels: -
<code>float16</code> - Best quality, 2x memory reduction -
<code>int8</code> - Good quality, 4x memory reduction -
<code>int8_float16</code> - Hybrid approach (recommended)</p>
<div class="sourceCode" id="cb146"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> WhisperModel(<span class="st">&quot;large-v3&quot;</span>, device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>, compute_type<span class="op">=</span><span class="st">&quot;int8_float16&quot;</span>)</span></code></pre></div>
<h2 id="future-outlook">Future Outlook</h2>
<h3 id="improving-amd-support">Improving AMD Support</h3>
<ul>
<li>ROCm 6.x+ shows significant improvements</li>
<li>More inference frameworks adding ROCm support</li>
<li>Community-driven optimizations growing</li>
</ul>
<h3 id="alternative-approaches">Alternative Approaches</h3>
<ul>
<li><strong>Containers:</strong> Pre-configured ROCm containers simplify
setup</li>
<li><strong>Cloud inference:</strong> Consider cloud GPUs for production
if local AMD support remains problematic</li>
<li><strong>Hybrid approach:</strong> Use AMD GPU for development,
NVIDIA for production</li>
</ul>
<h2 id="conclusion-7">Conclusion</h2>
<p>For AMD GPU users running local STT workloads,
<strong>Faster-Whisper</strong> is currently the most reliable choice.
It offers: - Proven ROCm support - Excellent performance - Active
development - Straightforward setup</p>
<p>The whisper.cpp issues you’re experiencing are common - the engine
excels at CPU inference but has unreliable AMD GPU support. Unless your
application specifically requires whisper.cpp, switching to
Faster-Whisper will provide better AMD GPU utilization and
performance.</p>
<hr />
<p><em>This document was generated by Claude Code as part of Daniel
Rosehill’s STT Fine-Tuning Notebook. While comprehensive, please verify
specific technical details and version compatibility for your use case.
AMD GPU support in AI workloads evolves rapidly.</em></p>
<h2 id="amd-rocm-inference-optimization">Amd Rocm Inference
Optimization</h2>
<h2 id="question-3">Question</h2>
<p>With an AMD GPU and ROCm, what inference engines have the best
support for ASR, and what model formats should you target when
converting from safe-tensors after fine-tuning?</p>
<h2 id="answer-7">Answer</h2>
<h3 id="the-amdrocm-reality-check">The AMD/ROCm Reality Check</h3>
<p>You’re right—NVIDIA’s CUDA ecosystem dominates AI, and AMD/ROCm
support is more limited. However, for ASR specifically (and Whisper in
particular), there are well-supported pathways that work excellently on
AMD.</p>
<h3 id="best-supported-inference-engines-for-amd-rocm">Best-Supported
Inference Engines for AMD + ROCm</h3>
<h4 id="ctranslate2-recommended---best-performance">1.
<strong>CTranslate2 (Recommended - Best Performance)</strong></h4>
<p><strong>Why CTranslate2:</strong> - Purpose-built for transformer
inference optimization - Excellent ROCm support (officially supported) -
Used by Faster-Whisper (the fastest Whisper implementation) - 4x faster
than PyTorch, significantly lower memory usage - Supports quantization
(INT8, FP16)</p>
<p><strong>ROCm Compatibility:</strong> - Works with ROCm 5.0+ - Your
GPU (gfx1101 - Navi 32) is well-supported - Requires
<code>HSA_OVERRIDE_GFX_VERSION=11.0.1</code> (which you’re already
using)</p>
<p><strong>Target Format:</strong></p>
<pre><code>Safetensors/PyTorch → CTranslate2 format (.ctranslate2/)</code></pre>
<p><strong>Conversion Process:</strong></p>
<div class="sourceCode" id="cb148"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ctranslate2</span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ct2-transformers-converter</span> <span class="at">--model</span> /path/to/finetuned-whisper <span class="dt">\</span></span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output_dir</span> /path/to/ctranslate2-model <span class="dt">\</span></span>
<span id="cb148-7"><a href="#cb148-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quantization</span> float16  <span class="co"># or int8 for faster inference</span></span></code></pre></div>
<p><strong>Why This Works for AMD:</strong> - CTranslate2 uses optimized
ROCm kernels - Well-maintained AMD support - Active community using it
on AMD GPUs</p>
<h4 id="onnx-runtime-with-rocm-execution-provider">2. <strong>ONNX
Runtime with ROCm Execution Provider</strong></h4>
<p><strong>Why ONNX Runtime:</strong> - Open standard (ONNX format) -
Microsoft-backed with official ROCm support - Good performance (though
not as fast as CTranslate2 for Whisper) - Wide compatibility across
frameworks</p>
<p><strong>ROCm Compatibility:</strong> - ONNXRuntime 1.14+ has
ROCmExecutionProvider - Works on gfx1101 with ROCm 5.4+</p>
<p><strong>Target Format:</strong></p>
<pre><code>Safetensors/PyTorch → ONNX (.onnx)</code></pre>
<p><strong>Conversion Process:</strong></p>
<div class="sourceCode" id="cb150"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> optimum.onnxruntime <span class="im">import</span> ORTModelForSpeechSeq2Seq</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> WhisperProcessor</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-4"><a href="#cb150-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-5"><a href="#cb150-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ORTModelForSpeechSeq2Seq.from_pretrained(</span>
<span id="cb150-6"><a href="#cb150-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;path/to/finetuned-whisper&quot;</span>,</span>
<span id="cb150-7"><a href="#cb150-7" aria-hidden="true" tabindex="-1"></a>    export<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb150-8"><a href="#cb150-8" aria-hidden="true" tabindex="-1"></a>    provider<span class="op">=</span><span class="st">&quot;ROCMExecutionProvider&quot;</span></span>
<span id="cb150-9"><a href="#cb150-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb150-10"><a href="#cb150-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-11"><a href="#cb150-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb150-12"><a href="#cb150-12" aria-hidden="true" tabindex="-1"></a>model.save_pretrained(<span class="st">&quot;path/to/onnx-model&quot;</span>)</span></code></pre></div>
<p><strong>Optimization:</strong></p>
<div class="sourceCode" id="cb151"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> onnxruntime.quantization.preprocess <span class="dt">\</span></span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--input</span> model.onnx <span class="dt">\</span></span>
<span id="cb151-4"><a href="#cb151-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output</span> model-quantized.onnx</span></code></pre></div>
<h4 id="pytorch-with-rocm-backend-fallback-option">3. <strong>PyTorch
with ROCm Backend (Fallback Option)</strong></h4>
<p><strong>Why PyTorch:</strong> - Native format (no conversion needed)
- Most flexible for experimentation - Good ROCm support (AMD maintains
torch-rocm) - Easier debugging</p>
<p><strong>ROCm Compatibility:</strong> - PyTorch 2.0+ has solid ROCm
support - Works directly with safetensors/PyTorch checkpoints</p>
<p><strong>Target Format:</strong></p>
<pre><code>Safetensors/PyTorch (native) - no conversion needed</code></pre>
<p><strong>Usage:</strong></p>
<div class="sourceCode" id="cb153"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> WhisperForConditionalGeneration</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-4"><a href="#cb153-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-5"><a href="#cb153-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> WhisperForConditionalGeneration.from_pretrained(</span>
<span id="cb153-6"><a href="#cb153-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;path/to/finetuned-whisper&quot;</span></span>
<span id="cb153-7"><a href="#cb153-7" aria-hidden="true" tabindex="-1"></a>).to(<span class="st">&quot;cuda&quot;</span>)  <span class="co"># &quot;cuda&quot; works with ROCm</span></span>
<span id="cb153-8"><a href="#cb153-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-9"><a href="#cb153-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-10"><a href="#cb153-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torch.<span class="bu">compile</span>(model)</span></code></pre></div>
<p><strong>Performance:</strong> - Slower than CTranslate2 or ONNX -
Higher memory usage - But most straightforward for debugging</p>
<h3 id="comparison-table-1">Comparison Table</h3>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 30%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Engine</th>
<th>Performance</th>
<th>ROCm Support</th>
<th>Conversion Complexity</th>
<th>Best Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>CTranslate2</strong></td>
<td>⭐⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>Medium</td>
<td>Production inference</td>
</tr>
<tr class="even">
<td><strong>ONNX Runtime</strong></td>
<td>⭐⭐⭐⭐</td>
<td>⭐⭐⭐⭐</td>
<td>Medium</td>
<td>Cross-platform deployment</td>
</tr>
<tr class="odd">
<td><strong>PyTorch</strong></td>
<td>⭐⭐⭐</td>
<td>⭐⭐⭐⭐⭐</td>
<td>None</td>
<td>Development/debugging</td>
</tr>
</tbody>
</table>
<h3 id="recommended-workflow-for-your-amd-setup">Recommended Workflow
for Your AMD Setup</h3>
<h4 id="primary-path-ctranslate2-faster-whisper"><strong>Primary Path:
CTranslate2 (Faster-Whisper)</strong></h4>
<p>This is the optimal choice for your AMD GPU:</p>
<div class="sourceCode" id="cb154"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="ex">ct2-transformers-converter</span> <span class="dt">\</span></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model</span> ./finetuned-whisper-medium <span class="dt">\</span></span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output_dir</span> ./finetuned-whisper-medium-ct2 <span class="dt">\</span></span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quantization</span> float16</span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-10"><a href="#cb154-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-11"><a href="#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install faster-whisper</span></code></pre></div>
<div class="sourceCode" id="cb155"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> faster_whisper <span class="im">import</span> WhisperModel</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> WhisperModel(</span>
<span id="cb155-5"><a href="#cb155-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;path/to/finetuned-whisper-medium-ct2&quot;</span>,</span>
<span id="cb155-6"><a href="#cb155-6" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">&quot;cuda&quot;</span>,  <span class="co"># Works with ROCm</span></span>
<span id="cb155-7"><a href="#cb155-7" aria-hidden="true" tabindex="-1"></a>    compute_type<span class="op">=</span><span class="st">&quot;float16&quot;</span></span>
<span id="cb155-8"><a href="#cb155-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb155-9"><a href="#cb155-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-10"><a href="#cb155-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb155-11"><a href="#cb155-11" aria-hidden="true" tabindex="-1"></a>segments, info <span class="op">=</span> model.transcribe(<span class="st">&quot;audio.wav&quot;</span>)</span>
<span id="cb155-12"><a href="#cb155-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> segment <span class="kw">in</span> segments:</span>
<span id="cb155-13"><a href="#cb155-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[</span><span class="sc">{</span>segment<span class="sc">.</span>start<span class="sc">:.2f}</span><span class="ss">s -&gt; </span><span class="sc">{</span>segment<span class="sc">.</span>end<span class="sc">:.2f}</span><span class="ss">s] </span><span class="sc">{</span>segment<span class="sc">.</span>text<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<h4 id="why-this-works-well-on-amd"><strong>Why This Works Well on
AMD:</strong></h4>
<ol type="1">
<li><strong>Optimized Kernels</strong>: CTranslate2 uses ROCm-optimized
kernels</li>
<li><strong>Lower Memory</strong>: Your 7700 XT/7800 XT has less VRAM
than NVIDIA equivalents—CTranslate2’s efficiency helps</li>
<li><strong>Proven Track Record</strong>: Many AMD users successfully
run faster-whisper</li>
<li><strong>Active Maintenance</strong>: CTranslate2 team actively
supports ROCm</li>
</ol>
<h3 id="format-conversion-summary">Format Conversion Summary</h3>
<pre><code>Post Fine-Tuning Workflow:

1. Training Output:
   ├── safetensors (raw weights)
   ├── pytorch_model.bin
   └── config.json

2. Convert to Target Format:
   ├── CTranslate2 (RECOMMENDED for AMD)
   │   └── Use ct2-transformers-converter
   │
   ├── ONNX (Good alternative)
   │   └── Use optimum.onnxruntime
   │
   └── Keep PyTorch (Development only)
       └── No conversion needed</code></pre>
<h3 id="amd-specific-optimizations">AMD-Specific Optimizations</h3>
<p><strong>Environment Variables (You’re Likely Already
Using):</strong></p>
<div class="sourceCode" id="cb157"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">HSA_OVERRIDE_GFX_VERSION</span><span class="op">=</span>11.0.1  <span class="co"># For gfx1101</span></span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">ROCM_PATH</span><span class="op">=</span>/opt/rocm</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">ROC_ENABLE_PRE_VEGA</span><span class="op">=</span>0</span></code></pre></div>
<p><strong>Quantization Strategy:</strong> - <strong>FP16</strong>: Best
balance (2x faster, minimal quality loss) - <strong>INT8</strong>: 4x
faster, slight quality degradation - <strong>FP32</strong>: Slowest,
unnecessary for inference</p>
<p><strong>Batch Size Tuning:</strong> Your 7700 XT/7800 XT has 12GB
VRAM: - Whisper tiny: batch size 16-32 - Whisper small: batch size 8-16
- Whisper medium: batch size 4-8 - Whisper large: batch size 1-2 (or use
quantization)</p>
<h3 id="what-not-to-use-on-amd">What NOT to Use on AMD</h3>
<p>❌ <strong>TensorRT</strong>: NVIDIA-only, won’t work ❌
<strong>CUDA-specific libraries</strong>: FlashAttention, etc. ❌
<strong>Some quantization tools</strong>: GPTQ, AWQ (CUDA-focused)</p>
<h3 id="bottom-line-recommendation">Bottom Line Recommendation</h3>
<p><strong>For your AMD GPU (gfx1101) + ROCm setup:</strong></p>
<ol type="1">
<li><strong>Best Performance</strong>: Fine-tune in PyTorch → Convert to
CTranslate2 → Use faster-whisper</li>
<li><strong>Best Compatibility</strong>: ONNX Runtime with ROCm
execution provider</li>
<li><strong>Easiest Debugging</strong>: Stay in PyTorch</li>
</ol>
<p><strong>The conversion command you’ll use most:</strong></p>
<div class="sourceCode" id="cb158"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ct2-transformers-converter</span> <span class="dt">\</span></span>
<span id="cb158-2"><a href="#cb158-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">--model</span> /path/to/your-finetuned-whisper <span class="dt">\</span></span>
<span id="cb158-3"><a href="#cb158-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">--output_dir</span> /path/to/optimized-model-ct2 <span class="dt">\</span></span>
<span id="cb158-4"><a href="#cb158-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--quantization</span> float16</span></code></pre></div>
<p>This gives you near-NVIDIA performance on AMD hardware for ASR
inference.</p>
<hr />
<p><strong>Note</strong>: This guidance was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook. ROCm
support varies by version—always verify compatibility with your specific
ROCm version (<code>rocm-smi --showdriverversion</code>). For production
deployments, test inference performance with your specific audio
data.</p>
<h2 id="gpu-vram-requirements-whisper">Gpu Vram Requirements
Whisper</h2>
<h2 id="the-observation">The Observation</h2>
<p>Running Whisper Medium on an AMD Radeon RX 7700 XT (8GB VRAM) via
whisper.cpp shows:</p>
<ul>
<li>GPU usage jumping to <strong>100% during inference</strong></li>
<li>Inference happens every few seconds during streaming
transcription</li>
<li>Surprising that Medium model maxes out the GPU</li>
</ul>
<p><strong>Questions:</strong></p>
<ol type="1">
<li>Does 100% GPU usage mean the model is too large?</li>
<li>How much VRAM/GPU power do you really need for quality local
STT?</li>
<li>Is hitting 90-100% GPU utilization during inference
problematic?</li>
</ol>
<h2 id="short-answer-2">Short Answer</h2>
<p><strong>100% GPU usage during inference is completely normal and
expected—it’s actually ideal!</strong> This means:</p>
<ul>
<li>You’re using your GPU efficiently</li>
<li>The model is running at full speed</li>
<li>This is NOT a problem or bottleneck</li>
<li>You do NOT need a bigger GPU</li>
</ul>
<p><strong>The concern about “maxing out” the GPU is based on a
misconception:</strong> Unlike gaming where 100% GPU means you’re
bottlenecked, in AI inference, 100% utilization during processing is the
<em>goal</em>. Between inference bursts, GPU usage drops back down—this
is normal streaming behavior.</p>
<h2 id="understanding-gpu-utilization-in-ai-inference">Understanding GPU
Utilization in AI Inference</h2>
<h3 id="gaminggraphics-workload-continuous">Gaming/Graphics Workload
(Continuous)</h3>
<pre><code>Timeline: [████████████████████████████] 100% sustained
Meaning:  GPU struggling to keep up with frame rate demands
Problem:  You need a better GPU or lower settings</code></pre>
<p>In gaming, continuous 100% means bottleneck.</p>
<h3 id="ai-inference-workload-bursty">AI Inference Workload
(Bursty)</h3>
<pre><code>Timeline: [████____████____████____████] Bursts to 100%
Meaning:  GPU efficiently processing, then idle, then processing
Normal:   This is optimal behavior</code></pre>
<p>In AI inference, bursts to 100% mean efficient utilization.</p>
<h3 id="your-whisper.cpp-streaming-case">Your Whisper.cpp Streaming
Case</h3>
<pre><code>Every 3 seconds:
  [Recording audio]     GPU: 0-5%   ← Waiting for audio
  [Inference begins]    GPU: 100%   ← Processing audio
  [Inference complete]  GPU: 0-5%   ← Done, waiting</code></pre>
<p><strong>This pattern is perfect.</strong> You want GPU to spike to
100% during the brief inference, then return to idle.</p>
<h2 id="vram-requirements-vs-gpu-compute">VRAM Requirements vs GPU
Compute</h2>
<p>Two separate concerns:</p>
<h3 id="vram-capacity-memory-size">1. VRAM Capacity (Memory Size)</h3>
<p><strong>What it determines:</strong> Which model size you can
load</p>
<p><strong>Whisper model VRAM requirements:</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>VRAM (FP16)</th>
<th>VRAM (INT8)</th>
<th>VRAM (Q5)</th>
<th>VRAM (Q4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tiny</td>
<td>39M</td>
<td>~150 MB</td>
<td>~80 MB</td>
<td>~50 MB</td>
<td>~40 MB</td>
</tr>
<tr class="even">
<td>Base</td>
<td>74M</td>
<td>~290 MB</td>
<td>~150 MB</td>
<td>~100 MB</td>
<td>~80 MB</td>
</tr>
<tr class="odd">
<td>Small</td>
<td>244M</td>
<td>~950 MB</td>
<td>~480 MB</td>
<td>~350 MB</td>
<td>~280 MB</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>769M</td>
<td>~3.0 GB</td>
<td>~1.5 GB</td>
<td>~1.1 GB</td>
<td>~900 MB</td>
</tr>
<tr class="odd">
<td>Large-v2</td>
<td>1550M</td>
<td>~6.0 GB</td>
<td>~3.0 GB</td>
<td>~2.2 GB</td>
<td>~1.8 GB</td>
</tr>
<tr class="even">
<td>Large-v3</td>
<td>1550M</td>
<td>~6.0 GB</td>
<td>~3.0 GB</td>
<td>~2.2 GB</td>
<td>~1.8 GB</td>
</tr>
<tr class="odd">
<td>Large-v3-turbo</td>
<td>809M</td>
<td>~3.1 GB</td>
<td>~1.6 GB</td>
<td>~1.2 GB</td>
<td>~1.0 GB</td>
</tr>
</tbody>
</table>
<p><strong>Your AMD RX 7700 XT (8GB VRAM) can handle:</strong></p>
<ul>
<li>✓ Medium (FP16, INT8, all quantizations) with room to spare</li>
<li>✓ Large-v3-turbo (FP16, INT8, quantized)</li>
<li>✓ Large-v2/v3 (INT8 and quantized versions)</li>
<li>✗ Large-v2/v3 (FP16) - would use ~6GB, leaving only 2GB for
system</li>
</ul>
<p><strong>Whisper.cpp default:</strong> Usually uses Q5 or Q4
quantization, so your 8GB is plenty even for Large models</p>
<h3 id="gpu-compute-power-processing-speed">2. GPU Compute Power
(Processing Speed)</h3>
<p><strong>What it determines:</strong> How <em>fast</em> inference
runs</p>
<p><strong>AMD RX 7700 XT specs:</strong></p>
<ul>
<li>Compute Units: 54</li>
<li>Peak FP16 performance: ~35 TFLOPS</li>
<li>Memory bandwidth: 432 GB/s</li>
<li>Architecture: RDNA 3 (Navi 32)</li>
</ul>
<p><strong>This is a mid-to-upper-tier GPU</strong>—very capable for
local AI.</p>
<h2 id="decoding-the-100-gpu-usage">Decoding the “100% GPU Usage”</h2>
<h3 id="whats-actually-happening">What’s Actually Happening</h3>
<p>When whisper.cpp processes audio:</p>
<div class="sourceCode" id="cb162"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>audio_chunk <span class="op">=</span> capture_audio(<span class="dv">3</span><span class="er">_seconds</span>)  <span class="co"># GPU: 0%</span></span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>gpu_buffer <span class="op">=</span> transfer_to_gpu(audio_chunk)  <span class="co"># GPU: 5-10%</span></span>
<span id="cb162-6"><a href="#cb162-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-7"><a href="#cb162-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-8"><a href="#cb162-8" aria-hidden="true" tabindex="-1"></a>transcription <span class="op">=</span> model.forward(gpu_buffer)   <span class="co"># GPU: 100%</span></span>
<span id="cb162-9"><a href="#cb162-9" aria-hidden="true" tabindex="-1"></a>                                            <span class="co"># Duration: 0.5-2 seconds</span></span>
<span id="cb162-10"><a href="#cb162-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-11"><a href="#cb162-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-12"><a href="#cb162-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(transcription)  <span class="co"># GPU: 0%</span></span>
<span id="cb162-13"><a href="#cb162-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-14"><a href="#cb162-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-15"><a href="#cb162-15" aria-hidden="true" tabindex="-1"></a>time.sleep(<span class="dv">1</span>)  <span class="co"># GPU: 0%</span></span></code></pre></div>
<p><strong>Your observation:</strong> GPU hits 100% during
<code>model.forward()</code></p>
<p><strong>This is correct and optimal!</strong> You <em>want</em> the
GPU to work at full capacity during inference.</p>
<h3 id="why-this-isnt-a-problem">Why This Isn’t a Problem</h3>
<p><strong>1. Inference is short:</strong> Even at 100%, each inference
burst lasts only 0.5-2 seconds</p>
<p><strong>2. Duty cycle is low:</strong> If inference takes 1 second
every 3 seconds, that’s only 33% average utilization</p>
<p><strong>3. Temperature managed:</strong> AMD GPUs throttle if they
overheat—100% for 1 second won’t cause thermal issues</p>
<p><strong>4. No frame drops:</strong> Unlike gaming, there’s no frame
rate to drop. Either inference finishes or it doesn’t—and yours is
finishing successfully.</p>
<h3 id="the-displaysystem-concern">The Display/System Concern</h3>
<p><strong>Your question:</strong> “Doesn’t GPU also need to run
displays?”</p>
<p><strong>Answer:</strong> GPU time-shares, and display composition
uses negligible compute:</p>
<pre><code>GPU time allocation (simplified):
[Inference: 0.8s] [Display: 0.01s] [Idle: 1.99s] [Inference: 0.8s] ...</code></pre>
<p><strong>Display needs:</strong> ~5-20ms per frame at 60 FPS = ~0.3-1%
of GPU time</p>
<p>Even at 100% inference utilization, there’s enough GPU time between
frames for display updates. You’d notice display issues (stuttering,
lag) if this were a problem—and you haven’t mentioned any.</p>
<h2 id="real-world-performance-expectations">Real-World Performance
Expectations</h2>
<h3 id="inference-speed-real-time-factor">Inference Speed (Real-Time
Factor)</h3>
<p><strong>Real-Time Factor (RTF):</strong> How long to transcribe vs
audio duration</p>
<pre><code>RTF = inference_time / audio_duration

RTF = 1.0 → Real-time (1 second to process 1 second of audio)
RTF = 0.5 → 2× real-time (0.5 seconds to process 1 second of audio)
RTF = 2.0 → 0.5× real-time (2 seconds to process 1 second of audio)</code></pre>
<p><strong>Your AMD RX 7700 XT expected performance (whisper.cpp with
ROCm):</strong></p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>RTF (approx)</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tiny</td>
<td>0.05-0.1</td>
<td>10-20× real-time</td>
</tr>
<tr class="even">
<td>Base</td>
<td>0.1-0.15</td>
<td>6-10× real-time</td>
</tr>
<tr class="odd">
<td>Small</td>
<td>0.2-0.3</td>
<td>3-5× real-time</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>0.4-0.6</td>
<td>1.6-2.5× real-time</td>
</tr>
<tr class="odd">
<td>Large-v3</td>
<td>0.7-1.0</td>
<td>1-1.4× real-time</td>
</tr>
<tr class="even">
<td>Large-v3-turbo</td>
<td>0.5-0.7</td>
<td>1.4-2× real-time</td>
</tr>
</tbody>
</table>
<p><strong>Your Medium at ~100% GPU usage likely achieving RTF ≈
0.5</strong>, meaning it’s processing 2× faster than real-time—which is
<em>excellent</em> for streaming transcription.</p>
<h3 id="what-quality-results-requires">What “Quality Results”
Requires</h3>
<p><strong>Myth:</strong> High GPU utilization = poor quality
results</p>
<p><strong>Reality:</strong> Quality depends on:</p>
<ol type="1">
<li><strong>Model accuracy</strong> (Medium is highly accurate)</li>
<li><strong>Successful completion</strong> (your transcriptions are
working)</li>
<li><strong>Reasonable latency</strong> (you’re getting results every
few seconds)</li>
</ol>
<p><strong>GPU utilization percentage is irrelevant to output
quality.</strong> As long as inference completes successfully (which it
is), you’re getting full-quality results.</p>
<h2 id="when-would-you-actually-need-more-gpu">When Would You Actually
Need More GPU?</h2>
<p>You’d need a bigger GPU if:</p>
<h3 id="real-time-factor-too-slow">1. Real-Time Factor Too Slow</h3>
<pre><code>Your audio: 3 seconds
Inference time: 4+ seconds
Result: Transcription falls behind</code></pre>
<p><strong>Your case:</strong> Not happening—Medium is processing faster
than real-time</p>
<h3 id="running-multiple-models-simultaneously">2. Running Multiple
Models Simultaneously</h3>
<pre><code>Whisper + Stable Diffusion + LLM inference
Result: Out of VRAM or extreme slowdown</code></pre>
<p><strong>Your case:</strong> Only running Whisper</p>
<h3 id="batch-processing-many-files">3. Batch Processing Many Files</h3>
<pre><code>Processing 100 audio files
Want: 5× faster throughput
Result: Larger GPU would help batch processing</code></pre>
<p><strong>Your case:</strong> Streaming transcription—batch speed less
relevant</p>
<h3 id="using-unquantized-large-models">4. Using Unquantized Large
Models</h3>
<pre><code>Loading Large-v3 in FP16: 6GB VRAM
Remaining: 2GB for system
Result: Might struggle with very large models in full precision</code></pre>
<p><strong>Your case:</strong> whisper.cpp uses quantization—you’re
fine</p>
<h2 id="optimizing-your-current-setup">Optimizing Your Current
Setup</h2>
<p>You don’t need a new GPU, but you can optimize:</p>
<h3 id="ensure-rocm-is-properly-configured">1. Ensure ROCm is Properly
Configured</h3>
<div class="sourceCode" id="cb169"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-2"><a href="#cb169-2" aria-hidden="true" tabindex="-1"></a><span class="ex">rocm-smi</span></span>
<span id="cb169-3"><a href="#cb169-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb169-4"><a href="#cb169-4" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>If whisper.cpp falls back to CPU:</strong> Performance would
be much worse, but wouldn’t show 100% GPU usage</p>
<h3 id="try-large-v3-turbo">2. Try Large-v3-Turbo</h3>
<div class="sourceCode" id="cb170"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-2"><a href="#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="ex">whisper.cpp</span> <span class="at">--model</span> large-v3-turbo input.wav</span>
<span id="cb170-3"><a href="#cb170-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-4"><a href="#cb170-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-5"><a href="#cb170-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-6"><a href="#cb170-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-7"><a href="#cb170-7" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>Expected:</strong> GPU still hits 100% during inference
(which is fine), but possibly slightly longer bursts</p>
<h3 id="check-thermal-throttling">3. Check Thermal Throttling</h3>
<div class="sourceCode" id="cb171"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 rocm-smi</span>
<span id="cb171-3"><a href="#cb171-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-4"><a href="#cb171-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-5"><a href="#cb171-5" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>If throttling:</strong> GPU automatically reduces clock speed
when hot—this <em>would</em> hurt performance, but 100% utilization
doesn’t necessarily mean throttling</p>
<h3 id="monitor-vram-usage-not-just-utilization">4. Monitor VRAM Usage,
Not Just Utilization</h3>
<div class="sourceCode" id="cb172"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a><span class="ex">rocm-smi</span> <span class="kw">|</span> <span class="fu">grep</span> <span class="st">&quot;Memory&quot;</span></span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>If VRAM is nearly full (&gt;7GB):</strong> Then you’re at the
limit</p>
<p><strong>If VRAM usage is low (~1-2GB):</strong> You have lots of
headroom</p>
<h2 id="model-selection-guide-for-your-gpu">Model Selection Guide for
Your GPU</h2>
<p><strong>Your AMD RX 7700 XT (8GB) can comfortably run:</strong></p>
<h3 id="recommended-for-quality-speed-balance">Recommended for Quality +
Speed Balance:</h3>
<p><strong>1. Large-v3-Turbo (best choice)</strong></p>
<ul>
<li>Accuracy: 90-95% of Large-v3</li>
<li>Speed: ~1.4-2× real-time on your GPU</li>
<li>VRAM: ~1GB (Q4 quantization)</li>
<li><strong>Best overall option</strong></li>
</ul>
<p><strong>2. Medium (what you’re using)</strong></p>
<ul>
<li>Accuracy: Excellent for most use cases</li>
<li>Speed: ~2-2.5× real-time on your GPU</li>
<li>VRAM: ~900MB (Q4 quantization)</li>
<li><strong>Very solid choice, no need to change unless you want better
accuracy</strong></li>
</ul>
<h3 id="if-you-want-maximum-accuracy">If You Want Maximum Accuracy:</h3>
<p><strong>3. Large-v3 (quantized)</strong></p>
<ul>
<li>Accuracy: Best available</li>
<li>Speed: ~1-1.4× real-time on your GPU</li>
<li>VRAM: ~2GB (Q4 quantization)</li>
<li><strong>Slight latency increase, but still real-time
capable</strong></li>
</ul>
<h3 id="if-you-want-maximum-speed">If You Want Maximum Speed:</h3>
<p><strong>4. Small</strong></p>
<ul>
<li>Accuracy: Good for clean audio</li>
<li>Speed: ~3-5× real-time on your GPU</li>
<li>VRAM: ~300MB</li>
<li><strong>Fast, but noticeably less accurate than Medium</strong></li>
</ul>
<h2 id="comparing-your-gpu-to-others">Comparing Your GPU to Others</h2>
<p><strong>Your AMD RX 7700 XT ranks:</strong></p>
<table>
<thead>
<tr class="header">
<th>GPU Class</th>
<th>Example</th>
<th>Whisper Medium RTF</th>
<th>Can Handle Large?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Entry-level</strong></td>
<td>GTX 1650, RX 6500 XT</td>
<td>0.8-1.2</td>
<td>Barely</td>
</tr>
<tr class="even">
<td><strong>Mid-range</strong></td>
<td>RTX 3060, RX 6700 XT</td>
<td>0.5-0.7</td>
<td>Yes (quantized)</td>
</tr>
<tr class="odd">
<td><strong>Your tier</strong></td>
<td>RX 7700 XT, RTX 3070</td>
<td>0.4-0.6</td>
<td>Yes, easily</td>
</tr>
<tr class="even">
<td><strong>High-end</strong></td>
<td>RTX 4070 Ti, RX 7900 XT</td>
<td>0.3-0.4</td>
<td>Yes, very fast</td>
</tr>
<tr class="odd">
<td><strong>Flagship</strong></td>
<td>RTX 4090, RX 7900 XTX</td>
<td>0.15-0.25</td>
<td>Yes, blazing</td>
</tr>
</tbody>
</table>
<p><strong>You’re in a very good tier for local STT.</strong> A 4090
would be ~2× faster, but you’re already faster than real-time, so it
wouldn’t meaningfully improve user experience.</p>
<h2 id="the-psychology-of-100">The Psychology of 100%</h2>
<p><strong>Why 100% <em>feels</em> wrong:</strong></p>
<ul>
<li>Gaming culture: 100% GPU = “maxed out”, need upgrade</li>
<li>CPU usage: 100% CPU often means system is struggling</li>
<li>Temperature concerns: High utilization = heat</li>
</ul>
<p><strong>Why 100% is actually <em>right</em> for AI
inference:</strong></p>
<ul>
<li>You’re paying for compute—use it!</li>
<li>Burst workload: 100% for 1 second every 3 seconds ≠ sustained
load</li>
<li>Efficient resource usage: Idle GPU is wasted GPU during
inference</li>
<li>No quality impact: Model runs full computation regardless</li>
</ul>
<p><strong>Better metrics to watch:</strong></p>
<ul>
<li>✓ Inference speed (faster than real-time?)</li>
<li>✓ VRAM usage (under 7GB?)</li>
<li>✓ Temperature (under 85°C?)</li>
<li>✓ Transcription latency (acceptable?)</li>
<li>✗ GPU utilization percentage (irrelevant for quality)</li>
</ul>
<h2 id="recommendations">Recommendations</h2>
<h3 id="what-you-should-do">What You Should Do</h3>
<p><strong>1. Keep using Medium—it’s working great!</strong></p>
<ul>
<li>Your GPU is handling it well</li>
<li>100% utilization during inference is optimal</li>
<li>Results are good quality</li>
</ul>
<p><strong>2. Optionally try Large-v3-Turbo</strong></p>
<div class="sourceCode" id="cb173"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a><span class="ex">whisper.cpp</span> <span class="at">--model</span> large-v3-turbo</span></code></pre></div>
<ul>
<li>Test if accuracy improvement is worth slight latency increase</li>
<li>Your GPU can handle it</li>
</ul>
<p><strong>3. Monitor VRAM and temperature, not utilization</strong></p>
<div class="sourceCode" id="cb174"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="ex">watch</span> <span class="at">-n</span> 1 <span class="st">&#39;rocm-smi | grep -E &quot;Temperature|Memory&quot;&#39;</span></span></code></pre></div>
<ul>
<li>VRAM &lt;7GB? ✓ You’re fine</li>
<li>Temperature &lt;85°C? ✓ You’re fine</li>
<li>Utilization 100%? ✓ This is correct!</li>
</ul>
<h3 id="what-you-should-not-do">What You Should NOT Do</h3>
<p><strong>✗ Don’t upgrade GPU based on 100% utilization</strong></p>
<ul>
<li>You’re not bottlenecked</li>
<li>Inference is faster than real-time</li>
<li>Quality is excellent</li>
</ul>
<p><strong>✗ Don’t drop to Small/Tiny to “reduce GPU load”</strong></p>
<ul>
<li>You’d lose accuracy for no benefit</li>
<li>GPU sitting at 50% instead of 100% doesn’t help anything</li>
<li>Use the compute power you have!</li>
</ul>
<p><strong>✗ Don’t worry about “maxing out” GPU</strong></p>
<ul>
<li>This isn’t gaming</li>
<li>Burst loads are normal and healthy</li>
<li>Your GPU is designed for this</li>
</ul>
<h2 id="ballpark-vram-requirements">Ballpark VRAM Requirements</h2>
<p><strong>For quality local STT with Whisper:</strong></p>
<table>
<thead>
<tr class="header">
<th>Use Case</th>
<th>Recommended</th>
<th>VRAM Needed</th>
<th>GPU Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Minimum viable</strong></td>
<td>Small</td>
<td>2GB</td>
<td>GTX 1050 Ti</td>
</tr>
<tr class="even">
<td><strong>Good experience</strong></td>
<td>Medium</td>
<td>4GB</td>
<td>GTX 1660</td>
</tr>
<tr class="odd">
<td><strong>Excellent</strong></td>
<td>Medium/Large-Turbo</td>
<td>6-8GB</td>
<td>RX 7700 XT, RTX 3070</td>
</tr>
<tr class="even">
<td><strong>Best</strong></td>
<td>Large-v3</td>
<td>8-12GB</td>
<td>RTX 3080, RX 7900 XT</td>
</tr>
<tr class="odd">
<td><strong>Overkill</strong></td>
<td>Large + fine-tunes</td>
<td>16GB+</td>
<td>RTX 4090</td>
</tr>
</tbody>
</table>
<p><strong>You’re in the “Excellent” tier.</strong> You don’t need more
VRAM for quality local STT—you already have it.</p>
<h2 id="conclusion-8">Conclusion</h2>
<p><strong>Your AMD RX 7700 XT (8GB VRAM) is more than sufficient for
quality local STT.</strong></p>
<p><strong>100% GPU utilization during inference is:</strong></p>
<ul>
<li>✓ Normal and expected</li>
<li>✓ Sign of efficient resource usage</li>
<li>✓ Not a problem or bottleneck</li>
<li>✓ Not affecting quality</li>
</ul>
<p><strong>You can comfortably run:</strong></p>
<ul>
<li>Medium (what you’re using)—excellent choice</li>
<li>Large-v3-Turbo—better accuracy, still good speed</li>
<li>Large-v3 (quantized)—best accuracy, acceptable speed</li>
</ul>
<p><strong>You should NOT:</strong></p>
<ul>
<li>Worry about 100% GPU spikes</li>
<li>Think you need to upgrade</li>
<li>Drop to smaller models to “reduce load”</li>
</ul>
<p><strong>The display/system concern is unfounded:</strong> Display
composition takes &lt;1% of GPU time and doesn’t compete meaningfully
with inference bursts.</p>
<p><strong>Your whisper.cpp setup is working optimally.</strong> Enjoy
your fast, accurate local transcription!</p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
</body>
</html>
