<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Speech-to-Text Fine-Tuning Guide - Book 3 - Specialized Topics</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="book-style.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Speech-to-Text Fine-Tuning Guide - Book 3 -
Specialized Topics</h1>
</header>
<h1 id="speech-to-text-fine-tuning-guide">Speech-to-Text Fine-Tuning
Guide</h1>
<h2 id="book-3-specialized-topics">Book 3: Specialized Topics</h2>
<p><em>Mobile ASR, File Formats &amp; Vocabulary</em></p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<p><strong>Part VII: Mobile ASR</strong><br />
Mobile and edge device deployment (1 chapters)</p>
<p><strong>Part VIII: File Formats</strong><br />
Audio and model file formats (2 chapters)</p>
<p><strong>Part IX: Vocabulary &amp; Language</strong><br />
Vocabulary recognition and language considerations (2 chapters)</p>
<hr />
<h1 id="part-vii-mobile-asr">Part VII: Mobile ASR</h1>
<p><em>Mobile and edge device deployment</em></p>
<hr />
<h2 id="phone-vs-desktop-whisper-performance">Phone Vs Desktop Whisper
Performance</h2>
<h2 id="question">Question</h2>
<p>How can a OnePlus Nord 3 5G smartphone run Whisper at comparable
performance to an AMD Radeon 7700 XT desktop GPU, despite the massive
difference in physical hardware size?</p>
<h2 id="answer">Answer</h2>
<p>Your observation is astute and highlights one of the most interesting
developments in mobile AI. Let’s break down why your phone performs
better than you’d expect.</p>
<hr />
<h2 id="hardware-comparison">Hardware Comparison</h2>
<h3 id="your-desktop-gpu-amd-radeon-7700-xt"><strong>Your Desktop GPU:
AMD Radeon 7700 XT</strong></h3>
<ul>
<li><strong>Architecture</strong>: RDNA 3 (Navi 32)</li>
<li><strong>Die Size</strong>: ~346 mm²</li>
<li><strong>Power Consumption</strong>: 245W TDP</li>
<li><strong>VRAM</strong>: 12GB GDDR6</li>
<li><strong>Compute</strong>: ~35 TFLOPS FP32, ~70 TFLOPS FP16</li>
<li><strong>Manufacturing</strong>: 5nm + 6nm chiplet</li>
<li><strong>Physical dimensions</strong>: ~267mm x 111mm x 50mm</li>
<li><strong>Weight</strong>: ~1kg</li>
</ul>
<h3 id="your-phone-oneplus-nord-3-5g"><strong>Your Phone: OnePlus Nord 3
5G</strong></h3>
<ul>
<li><strong>SoC</strong>: MediaTek Dimensity 9000</li>
<li><strong>Architecture</strong>: ARM Mali-G710 MC10 GPU</li>
<li><strong>Die Size</strong>: ~120 mm² (entire SoC, including CPU, GPU,
NPU, modem)</li>
<li><strong>Power Consumption</strong>: ~10W peak (entire device)</li>
<li><strong>Memory</strong>: Shared 8-16GB LPDDR5X</li>
<li><strong>Compute</strong>: ~5 TFLOPS FP32 (GPU) + dedicated AI
accelerator</li>
<li><strong>Manufacturing</strong>: 4nm TSMC</li>
<li><strong>Physical dimensions (entire phone)</strong>: ~163mm x 75mm x
8mm</li>
<li><strong>Weight</strong>: ~195g</li>
</ul>
<p><strong>Your intuition is right: the desktop GPU is physically ~10x
larger and uses ~25x more power.</strong></p>
<hr />
<h2 id="why-the-performance-gap-is-smaller-than-expected">Why the
Performance Gap Is Smaller Than Expected</h2>
<h3 id="dedicated-ai-accelerators-on-mobile-npusapus"><strong>1.
Dedicated AI Accelerators on Mobile (NPUs/APUs)</strong></h3>
<p><strong>Critical insight: Your phone isn’t running Whisper primarily
on its GPU.</strong></p>
<p>Modern flagship SoCs like the Dimensity 9000 have <strong>dedicated
AI Processing Units (APUs)</strong> optimized for neural network
inference:</p>
<p><strong>Dimensity 9000 APU Specs:</strong> - <strong>5th-gen
APU</strong>: 4x faster than previous gen - <strong>6 TOPS (trillion
operations per second) INT8 performance</strong> - <strong>Optimized for
transformer models</strong> (like Whisper) - <strong>Power
efficiency</strong>: 5x more efficient than GPU for AI workloads -
<strong>Dedicated memory access paths</strong> (minimizes bandwidth
bottlenecks)</p>
<p><strong>Why This Matters:</strong></p>
<pre><code>Desktop GPU: General-purpose compute (graphics, AI, compute)
  → Not optimized specifically for transformer inference
  → Whisper uses a fraction of available compute

Phone APU: Purpose-built for AI inference
  → Every transistor designed for neural network operations
  → Whisper runs on optimized silicon</code></pre>
<p><strong>Analogy:</strong> It’s like comparing a large dump truck
(desktop GPU) to a Formula 1 race car (phone APU) for driving on a
highway. The dump truck is bigger and more powerful, but the F1 car is
optimized for speed in its specific use case.</p>
<hr />
<h3 id="quantization-and-mobile-optimized-models"><strong>2.
Quantization and Mobile-Optimized Models</strong></h3>
<p><strong>Your phone likely isn’t running the same Whisper model as
your desktop.</strong></p>
<p><strong>Desktop (typical):</strong> - <strong>Precision</strong>:
FP32 or FP16 (32-bit or 16-bit floating-point) - <strong>Model</strong>:
Full Whisper base/small/medium - <strong>Framework</strong>: PyTorch
with ROCm</p>
<p><strong>Phone (typical):</strong> - <strong>Precision</strong>: INT8
(8-bit integer quantization) - <strong>Model</strong>: Quantized Whisper
variant optimized for mobile - <strong>Framework</strong>: TensorFlow
Lite, ONNX Runtime Mobile, or vendor-specific (MediaTek NeuroPilot)</p>
<p><strong>Quantization Impact:</strong></p>
<pre><code>FP32 model: 1.0 GB, 100% accuracy baseline
INT8 model:  0.25 GB (4x smaller), ~98% accuracy

Speed improvement: 2-4x faster inference
Memory bandwidth: 4x reduction
Power efficiency: 5-10x better</code></pre>
<p><strong>Your phone achieves similar perceptual quality with 1/4 the
data movement and compute.</strong></p>
<hr />
<h3 id="memory-bandwidth-and-data-movement"><strong>3. Memory Bandwidth
and Data Movement</strong></h3>
<p><strong>Counterintuitive fact: For Whisper inference, memory
bandwidth matters more than raw compute.</strong></p>
<p><strong>Why Transformers Are Memory-Bound:</strong> Whisper (and all
transformer models) spend most time: - Loading weights from memory -
Moving activations between layers - Accessing attention matrices</p>
<p><strong>Not</strong> performing math operations (those are fast on
modern hardware).</p>
<p><strong>Desktop Setup (Naive):</strong></p>
<pre><code>CPU → PCIe bus → GPU VRAM → Compute cores
      ^slow^</code></pre>
<p><strong>Desktop Setup (Optimized):</strong></p>
<pre><code>All data in GPU VRAM → Compute cores
  ^fast, but still limited by VRAM bandwidth^</code></pre>
<p><strong>Phone Setup:</strong></p>
<pre><code>APU integrated in SoC → Unified memory → Direct access
  ^no PCIe bottleneck, low latency^</code></pre>
<p><strong>Key Difference:</strong> - <strong>Desktop GPU</strong>: High
bandwidth (384 GB/s), but data must traverse PCIe bus from system RAM
unless pre-loaded - <strong>Phone APU</strong>: Lower bandwidth (60-100
GB/s), but <strong>integrated in SoC</strong> with direct memory access
and lower latency</p>
<p><strong>For Whisper’s inference pattern (small batches, streaming
audio), low latency often beats high bandwidth.</strong></p>
<hr />
<h3 id="optimization-and-software-stack"><strong>4. Optimization and
Software Stack</strong></h3>
<p><strong>Mobile AI Software Is Highly Optimized (Out of
Necessity)</strong></p>
<h4 id="phone-software-stack-highly-optimized"><strong>Phone Software
Stack (Highly Optimized):</strong></h4>
<ul>
<li><strong>MediaTek NeuroPilot</strong>: Vendor-specific APU
acceleration</li>
<li><strong>TensorFlow Lite / ONNX Runtime Mobile</strong>: Optimized
for mobile inference</li>
<li><strong>Kernel fusion</strong>: Multiple operations combined into
single kernels</li>
<li><strong>Mixed precision</strong>: Uses INT8 where possible, FP16
where necessary</li>
<li><strong>Pruning</strong>: Removes unnecessary model weights</li>
<li><strong>Hardware-specific tuning</strong>: Optimized for Dimensity
9000 specifically</li>
</ul>
<h4 id="desktop-stack-less-optimized-for-whisper"><strong>Desktop Stack
(Less Optimized for Whisper):</strong></h4>
<ul>
<li><strong>PyTorch + ROCm</strong>: General-purpose, not
Whisper-specific</li>
<li><strong>FP16/FP32</strong>: Larger data types (more accurate but
slower)</li>
<li><strong>Fewer mobile optimizations</strong>: Desktop ecosystem
prioritizes flexibility over efficiency</li>
</ul>
<p><strong>Mobile developers had to squeeze every drop of
performance</strong> due to power/thermal constraints. Desktop
developers have more headroom, so less aggressive optimization.</p>
<hr />
<h3 id="thermal-and-power-constraints-paradoxically-helpful"><strong>5.
Thermal and Power Constraints (Paradoxically Helpful)</strong></h3>
<p><strong>Your desktop GPU throttles less, but also wastes
more.</strong></p>
<p><strong>Desktop (AMD 7700 XT):</strong> - Runs at high clock speeds
(2.5 GHz+) - High power consumption (200W+) - Large cooling solution
allows sustained performance - <strong>But</strong>: Whisper doesn’t
fully utilize the GPU (low occupancy) - GPU is running at high clocks
waiting for memory - Wasting power on idle cores</p>
<p><strong>Phone (Dimensity 9000 APU):</strong> - Runs at lower clocks
(~1 GHz APU) - Low power consumption (5-10W) - Thermal throttling kicks
in quickly - <strong>But</strong>: APU is fully utilized (100%
occupancy) - Every core doing useful work - Efficient at its target
workload</p>
<p><strong>Efficiency Comparison:</strong></p>
<pre><code>Desktop: 245W to run Whisper → 0.5x realtime (example)
Phone:   5W to run Whisper → 0.4x realtime

Performance: Similar
Efficiency: Phone wins by 20-30x</code></pre>
<hr />
<h3 id="model-size-sweet-spot"><strong>6. Model Size Sweet
Spot</strong></h3>
<p><strong>Whisper Base/Small models fit mobile hardware
perfectly.</strong></p>
<h4 id="whisper-model-sizes"><strong>Whisper Model Sizes:</strong></h4>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Disk Size</th>
<th>VRAM/RAM Needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tiny</td>
<td>39M</td>
<td>73 MB</td>
<td>~400 MB</td>
</tr>
<tr class="even">
<td>Base</td>
<td>74M</td>
<td>139 MB</td>
<td>~600 MB</td>
</tr>
<tr class="odd">
<td>Small</td>
<td>244M</td>
<td>461 MB</td>
<td>~1.5 GB</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>769M</td>
<td>1.45 GB</td>
<td>~4 GB</td>
</tr>
<tr class="odd">
<td>Large</td>
<td>1.5B</td>
<td>2.87 GB</td>
<td>~8 GB</td>
</tr>
</tbody>
</table>
<p><strong>Your Phone (8-16GB RAM):</strong> - Can comfortably run
<strong>Base</strong> or <strong>Small</strong> (INT8 quantized) -
Quantized Small: ~350 MB - Leaves plenty of RAM for OS and other
apps</p>
<p><strong>Your Desktop GPU (12GB VRAM):</strong> - Can run up to
<strong>Large</strong> (FP16) - But you’re likely testing
<strong>Base</strong> or <strong>Small</strong> for fair comparison -
Desktop is underutilized (using &lt;5% of VRAM)</p>
<p><strong>When testing equivalent model sizes, desktop advantage
shrinks dramatically.</strong></p>
<hr />
<h3 id="real-world-performance-comparison"><strong>7. Real-World
Performance Comparison</strong></h3>
<p>Let’s estimate actual inference speeds:</p>
<h4
id="scenario-whisper-small-244m-params-30-second-audio-clip"><strong>Scenario:
Whisper Small (244M params), 30-second audio clip</strong></h4>
<p><strong>Desktop (AMD 7700 XT, FP16, PyTorch + ROCm):</strong> -
Inference time: ~2-4 seconds - Preprocessing: 0.5 seconds -
<strong>Total: ~2.5-4.5 seconds</strong> - <strong>Realtime factor:
0.08-0.15x</strong> (6-12x faster than realtime)</p>
<p><strong>Phone (Dimensity 9000, INT8, TensorFlow Lite):</strong> -
Inference time: ~3-5 seconds - Preprocessing: 0.5 seconds -
<strong>Total: ~3.5-5.5 seconds</strong> - <strong>Realtime factor:
0.12-0.18x</strong> (5-8x faster than realtime)</p>
<p><strong>Difference: Desktop is ~1.3-1.5x faster</strong></p>
<p><strong>Your observation: “not drastically better” is
accurate!</strong></p>
<hr />
<h2 id="why-desktop-isnt-10x-faster-summary">Why Desktop Isn’t 10x
Faster (Summary)</h2>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 40%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor</th>
<th>Desktop Advantage</th>
<th>Why Gap Is Smaller</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Raw compute</strong></td>
<td>7x more TFLOPS</td>
<td>Whisper is memory-bound, not compute-bound</td>
</tr>
<tr class="even">
<td><strong>Memory bandwidth</strong></td>
<td>4x higher</td>
<td>Mobile has lower latency, integrated design</td>
</tr>
<tr class="odd">
<td><strong>Die size</strong></td>
<td>3x larger</td>
<td>Phone has dedicated AI silicon (APU)</td>
</tr>
<tr class="even">
<td><strong>Power consumption</strong></td>
<td>25x higher</td>
<td>Wasted on idle cores, not efficiently utilized</td>
</tr>
<tr class="odd">
<td><strong>Optimization</strong></td>
<td>Less optimized</td>
<td>Mobile stack highly tuned for efficiency</td>
</tr>
<tr class="even">
<td><strong>Quantization</strong></td>
<td>Uses FP16/FP32</td>
<td>Phone uses INT8 (4x smaller, faster)</td>
</tr>
<tr class="odd">
<td><strong>Hardware specialization</strong></td>
<td>General GPU</td>
<td>APU purpose-built for transformers</td>
</tr>
</tbody>
</table>
<p><strong>Bottom line: For Whisper inference specifically, your phone’s
dedicated AI silicon and optimized software stack nearly closes the gap
with your desktop’s brute-force GPU power.</strong></p>
<hr />
<h2 id="when-desktop-wins-big">When Desktop Wins Big</h2>
<p>Desktop advantage grows significantly when:</p>
<ol type="1">
<li><strong>Batch processing</strong>: Desktop can process 8-16 audio
files simultaneously
<ul>
<li>Phone: Limited by RAM (batch size 1-2)</li>
<li>Desktop: Can batch 16+ (10x faster throughput)</li>
</ul></li>
<li><strong>Larger models</strong>: Whisper Large or custom fine-tuned
models
<ul>
<li>Phone: Cannot run Large (insufficient RAM)</li>
<li>Desktop: Runs Large-v3 comfortably</li>
</ul></li>
<li><strong>Training/fine-tuning</strong>: Desktop crushes phone
<ul>
<li>Phone: Not designed for training (APUs are inference-only)</li>
<li>Desktop: Can fine-tune models 100x faster</li>
</ul></li>
<li><strong>Long-form audio</strong>: Hours of audio
<ul>
<li>Phone: Thermal throttling becomes an issue</li>
<li>Desktop: Sustained performance over hours</li>
</ul></li>
</ol>
<p><strong>For single-clip, base/small model inference (your use case),
the gap is small.</strong></p>
<hr />
<h2 id="broader-implications">Broader Implications</h2>
<h3 id="the-mobile-ai-revolution"><strong>The Mobile AI
Revolution</strong></h3>
<p>Your observation reflects a broader trend:</p>
<p><strong>2015-2020: Desktop/Cloud Dominated AI</strong> - Models too
large for mobile - Mobile = cloud API calls</p>
<p><strong>2020-2025: Mobile AI Catches Up</strong> - Dedicated AI
accelerators (Apple Neural Engine, Google TPU, MediaTek APU, Qualcomm AI
Engine) - Quantization techniques (INT8, INT4) - On-device inference for
privacy, latency, offline use</p>
<p><strong>Result: Flagship phones now rival mid-range desktop GPUs for
inference.</strong></p>
<h3 id="efficiency-raw-power-for-inference"><strong>Efficiency &gt; Raw
Power for Inference</strong></h3>
<p>For inference (not training): - <strong>Purpose-built
silicon</strong> (APU) beats general-purpose (GPU) - <strong>Software
optimization</strong> matters as much as hardware - <strong>Memory
hierarchy</strong> (latency, bandwidth) matters more than compute -
<strong>Quantization</strong> enables massive speedups with minimal
quality loss</p>
<p><strong>Your phone is a testament to the power of specialized,
efficient design.</strong></p>
<hr />
<h2 id="practical-takeaways">Practical Takeaways</h2>
<h3 id="when-to-use-desktop"><strong>When to Use Desktop:</strong></h3>
<ul>
<li>Fine-tuning models</li>
<li>Batch processing (dozens of files)</li>
<li>Large models (Whisper Medium/Large)</li>
<li>Long recording sessions (hours)</li>
<li>Experimenting with custom models</li>
</ul>
<h3 id="when-to-use-phone"><strong>When to Use Phone:</strong></h3>
<ul>
<li>Real-time transcription</li>
<li>On-the-go recordings</li>
<li>Single clips (&lt;5 minutes)</li>
<li>Privacy (offline inference)</li>
<li>Power efficiency</li>
</ul>
<p><strong>For your daily use case (speech-to-text input), phone is
likely sufficient—and more convenient.</strong></p>
<hr />
<h2 id="future-outlook">Future Outlook</h2>
<p><strong>Mobile AI is getting better, faster:</strong></p>
<ul>
<li><strong>Next-gen SoCs (2024-2025)</strong>: 10-15 TOPS APUs</li>
<li><strong>Improved quantization</strong>: INT4, mixed INT8/FP16</li>
<li><strong>On-device fine-tuning</strong>: Possible within 2-3
years</li>
<li><strong>Larger models on-device</strong>: Whisper Medium on flagship
phones soon</li>
</ul>
<p><strong>Desktop advantage will remain for:</strong> - Training and
fine-tuning - Extremely large models (10B+ parameters) - Batch
processing at scale</p>
<p><strong>But for inference, mobile will continue closing the
gap.</strong></p>
<hr />
<h2 id="conclusion">Conclusion</h2>
<p><strong>Your OnePlus Nord 3 5G performs surprisingly well
because:</strong></p>
<ol type="1">
<li><strong>Dedicated AI silicon (APU)</strong> purpose-built for
transformers</li>
<li><strong>Aggressive quantization</strong> (INT8 vs. FP16/FP32)</li>
<li><strong>Highly optimized software stack</strong> (TensorFlow Lite,
vendor kernels)</li>
<li><strong>Integrated memory architecture</strong> (low latency, no
PCIe bottleneck)</li>
<li><strong>Whisper is memory-bound</strong> (not compute-bound),
favoring efficient designs</li>
</ol>
<p><strong>Your desktop GPU has more raw power, but Whisper inference
doesn’t fully utilize it.</strong></p>
<p>The result: <strong>Phone ~0.6-0.8x the speed of desktop for
equivalent models</strong>—much closer than the 10x physical size
difference would suggest.</p>
<p><strong>This is modern AI hardware engineering: efficiency through
specialization.</strong></p>
<hr />
<p><strong>Note</strong>: This analysis was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Performance varies by model size, implementation, and specific hardware.
For the most accurate comparison, benchmark both devices with identical
models (same Whisper variant, same precision) using tools like
<code>faster-whisper</code> (desktop) and <code>whisper.cpp</code>
(mobile). Mobile AI capabilities are rapidly evolving—expect continued
improvements in coming years.</p>
<h1 id="part-viii-file-formats">Part VIII: File Formats</h1>
<p><em>Audio and model file formats</em></p>
<hr />
<h2 id="formats">Formats</h2>
<p>When you fine-tune Whisper using a standard notebook example, you’ll
end up with a folder structure like this:</p>
<figure>
<img src="screenshots/1.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>The directory contains <code>runs</code> for resuming from
checkpoints, but the core model file is:</p>
<p><strong><code>model.safetensors</code></strong></p>
<p>This file is directly usable for STT/inference. However, given the
wide variety of Whisper deployment scenarios and the fact that you may
want to use your fine-tuned model on devices with different processing
capabilities, you’ll likely need to convert it to other formats.</p>
<h2 id="common-model-formats">Common Model Formats</h2>
<h3 id="safetensors-original-format">SafeTensors (Original Format)</h3>
<ul>
<li><strong>File extension</strong>: <code>.safetensors</code></li>
<li><strong>Use case</strong>: Direct inference, training, PyTorch-based
applications</li>
<li><strong>Advantages</strong>: Safe serialization format, prevents
arbitrary code execution</li>
<li><strong>Disadvantages</strong>: Limited compatibility with optimized
inference engines</li>
</ul>
<h3 id="ggml-legacy">GGML (Legacy)</h3>
<ul>
<li><strong>File extension</strong>: <code>.bin</code></li>
<li><strong>Use case</strong>: CPU-only inference on edge devices (e.g.,
FUTO Keyboard)</li>
<li><strong>Compatible with</strong>: <code>whisper.cpp</code> (older
versions)</li>
<li><strong>Advantages</strong>: Enables deployment on
resource-constrained devices, edge computing</li>
<li><strong>Status</strong>: Legacy format, superseded by GGUF</li>
<li><strong>Considerations</strong>: Hardware limitations still apply.
Consider converting Tiny models to GGML for mobile/edge deployment while
keeping Base or larger models in CTranslate2 for desktop
applications.</li>
</ul>
<h3 id="gguf-recommended-for-edgecpu">GGUF (Recommended for
Edge/CPU)</h3>
<ul>
<li><strong>File extension</strong>: <code>.gguf</code></li>
<li><strong>Use case</strong>: CPU-only inference on edge devices, local
deployment</li>
<li><strong>Compatible with</strong>: Modern <code>whisper.cpp</code>,
llama.cpp ecosystem</li>
<li><strong>Advantages</strong>:
<ul>
<li>Improved metadata handling (embedded model info,
hyperparameters)</li>
<li>Better version control and compatibility checking</li>
<li>More efficient storage and loading</li>
<li>Standardized format across llama.cpp ecosystem</li>
<li>Supports quantization for smaller model sizes</li>
</ul></li>
<li><strong>Best for</strong>: Modern edge deployments, CPU inference,
resource-constrained environments</li>
<li><strong>Migration</strong>: GGUF is the successor to GGML and should
be preferred for new projects</li>
</ul>
<h3 id="ctranslate2">CTranslate2</h3>
<ul>
<li><strong>File extension</strong>: <code>.bin</code> (directory with
model files)</li>
<li><strong>Use case</strong>: Optimized inference for desktop
applications</li>
<li><strong>Compatible with</strong>: Faster Whisper and many local STT
applications</li>
<li><strong>Advantages</strong>: Significantly faster inference, reduced
memory usage, optimized for CPU and GPU</li>
<li><strong>Best for</strong>: Production deployments requiring speed
and efficiency</li>
</ul>
<h3 id="onnx">ONNX</h3>
<ul>
<li><strong>File extension</strong>: <code>.onnx</code></li>
<li><strong>Use case</strong>: Cross-platform deployment, inference
optimization</li>
<li><strong>Compatible with</strong>: ONNX Runtime, various inference
engines</li>
<li><strong>Advantages</strong>: Hardware-agnostic, works across
different ML frameworks. Long recording durations / less chunking.</li>
<li><strong>Best for</strong>: Applications requiring maximum
portability across platforms and hardware</li>
</ul>
<h3 id="core-ml-apple-devices">Core ML (Apple Devices)</h3>
<ul>
<li><strong>File extension</strong>: <code>.mlmodel</code> or
<code>.mlpackage</code></li>
<li><strong>Use case</strong>: iOS, macOS, and Apple Silicon
deployment</li>
<li><strong>Advantages</strong>: Native Apple Neural Engine
acceleration, optimized battery usage</li>
<li><strong>Best for</strong>: Native Apple applications</li>
</ul>
<h3 id="tensorflow-lite">TensorFlow Lite</h3>
<ul>
<li><strong>File extension</strong>: <code>.tflite</code></li>
<li><strong>Use case</strong>: Mobile deployment (Android/iOS)</li>
<li><strong>Advantages</strong>: Lightweight, optimized for mobile
inference</li>
<li><strong>Best for</strong>: Mobile applications with size and
performance constraints</li>
</ul>
<h2 id="format-selection-guide">Format Selection Guide</h2>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 29%" />
<col style="width: 25%" />
<col style="width: 29%" />
</colgroup>
<thead>
<tr class="header">
<th>Format</th>
<th>Best Use Case</th>
<th>Performance</th>
<th>Compatibility</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SafeTensors</td>
<td>Training, PyTorch apps</td>
<td>Baseline</td>
<td>PyTorch ecosystem</td>
</tr>
<tr class="even">
<td>GGML</td>
<td>Legacy edge devices</td>
<td>Optimized for CPU</td>
<td>Old whisper.cpp projects</td>
</tr>
<tr class="odd">
<td>GGUF</td>
<td>Modern edge devices, CPU-only</td>
<td>Optimized for CPU</td>
<td>Modern whisper.cpp, llama.cpp</td>
</tr>
<tr class="even">
<td>CTranslate2</td>
<td>Desktop apps, servers</td>
<td>High (GPU/CPU)</td>
<td>Faster Whisper, production apps</td>
</tr>
<tr class="odd">
<td>ONNX</td>
<td>Cross-platform deployment</td>
<td>Good</td>
<td>Wide framework support</td>
</tr>
<tr class="even">
<td>Core ML</td>
<td>Apple devices</td>
<td>Excellent (on Apple HW)</td>
<td>Apple ecosystem only</td>
</tr>
<tr class="odd">
<td>TFLite</td>
<td>Mobile apps</td>
<td>Good</td>
<td>Android/iOS</td>
</tr>
</tbody>
</table>
<h2 id="conversion-considerations">Conversion Considerations</h2>
<ul>
<li><strong>Model size</strong>: Larger models (Base, Small, Medium) may
not be practical for GGML/GGUF on edge devices</li>
<li><strong>Target hardware</strong>: GPU availability, CPU
capabilities, RAM constraints</li>
<li><strong>Use case</strong>: Real-time vs. batch processing, latency
requirements</li>
<li><strong>Deployment environment</strong>: Cloud, edge, mobile,
desktop</li>
</ul>
<h2 id="gguf-vs-ggml">Gguf Vs Ggml</h2>
<h2 id="overview">Overview</h2>
<p>GGML (Georgi Gerganov Machine Learning) was the original quantized
model format created for CPU-based inference in the llama.cpp ecosystem.
GGUF (GGML Universal Format) is its successor, designed to address
limitations and improve the overall user experience.</p>
<h2 id="what-changed">What Changed?</h2>
<h3 id="ggml-legacy-format">GGML (Legacy Format)</h3>
<p><strong>File Extension</strong>: <code>.bin</code></p>
<p><strong>Characteristics</strong>: - Basic binary serialization format
- Minimal metadata embedded in the model file - Version information
stored externally or not at all - Required manual tracking of model
parameters, quantization settings, and architecture details - Prone to
compatibility issues when model formats evolved - Used across early
whisper.cpp and llama.cpp projects</p>
<p><strong>Limitations</strong>: - No standardized way to store metadata
- Difficult to validate model compatibility automatically - Version
mismatches could cause silent failures or crashes - Required users to
manually track model configurations - Limited error messages when
loading incompatible models</p>
<h3 id="gguf-modern-format">GGUF (Modern Format)</h3>
<p><strong>File Extension</strong>: <code>.gguf</code></p>
<p><strong>Improvements</strong>: - <strong>Rich Metadata</strong>:
Embeds comprehensive model information directly in the file - Model
architecture details - Tokenizer information - Quantization parameters -
Version information - Custom metadata fields - <strong>Version
Control</strong>: Built-in versioning system prevents compatibility
issues - <strong>Self-Describing</strong>: Models carry all necessary
information for proper loading - <strong>Better Error Handling</strong>:
Provides clear error messages for incompatible versions -
<strong>Standardization</strong>: Unified format across the entire
llama.cpp ecosystem - <strong>Extensibility</strong>: Designed to
accommodate future format changes without breaking compatibility</p>
<h2 id="technical-comparison">Technical Comparison</h2>
<table>
<thead>
<tr class="header">
<th>Feature</th>
<th>GGML</th>
<th>GGUF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Metadata Storage</td>
<td>Minimal/External</td>
<td>Embedded &amp; Comprehensive</td>
</tr>
<tr class="even">
<td>Version Checking</td>
<td>Manual</td>
<td>Automatic</td>
</tr>
<tr class="odd">
<td>Error Messages</td>
<td>Vague</td>
<td>Detailed &amp; Helpful</td>
</tr>
<tr class="even">
<td>Cross-Tool Compatibility</td>
<td>Limited</td>
<td>Excellent</td>
</tr>
<tr class="odd">
<td>Future-Proofing</td>
<td>Poor</td>
<td>Good</td>
</tr>
<tr class="even">
<td>File Size Overhead</td>
<td>Minimal</td>
<td>Slightly larger (negligible)</td>
</tr>
<tr class="odd">
<td>Loading Speed</td>
<td>Fast</td>
<td>Fast (comparable)</td>
</tr>
</tbody>
</table>
<h2 id="migration-path">Migration Path</h2>
<h3 id="when-to-use-ggml">When to Use GGML</h3>
<ul>
<li><strong>Legacy Systems</strong>: You’re maintaining older
whisper.cpp or llama.cpp deployments</li>
<li><strong>Existing Tooling</strong>: Your production pipeline is built
around GGML and migration isn’t feasible</li>
<li><strong>Compatibility</strong>: You need to support older versions
of tools that don’t support GGUF yet</li>
</ul>
<h3 id="when-to-use-gguf-recommended">When to Use GGUF
(Recommended)</h3>
<ul>
<li><strong>New Projects</strong>: All new fine-tuning and deployment
projects</li>
<li><strong>Modern Tools</strong>: Working with up-to-date whisper.cpp,
llama.cpp, or compatible tools</li>
<li><strong>Better Maintenance</strong>: Want self-documenting models
with clear version information</li>
<li><strong>Long-Term Support</strong>: Building applications that need
to be maintained over time</li>
</ul>
<h2 id="conversion-between-formats">Conversion Between Formats</h2>
<h3 id="ggml-to-gguf">GGML to GGUF</h3>
<p>Most modern versions of whisper.cpp and llama.cpp include conversion
utilities:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> convert-whisper-to-ggml.py <span class="at">--model</span> path/to/model <span class="at">--output-format</span> gguf</span></code></pre></div>
<h3 id="hugging-face-hub">Hugging Face Hub</h3>
<p>Many model repositories now offer both formats: - Look for files
ending in <code>.gguf</code> for the modern format - Older repositories
may only have <code>.bin</code> files (GGML) - Prefer GGUF versions when
available</p>
<h2 id="real-world-impact">Real-World Impact</h2>
<h3 id="for-whisper-fine-tuning">For Whisper Fine-Tuning</h3>
<p><strong>GGML Era Workflow</strong>: 1. Fine-tune model 2. Convert to
GGML <code>.bin</code> 3. Manually document quantization settings 4.
Hope the target device’s whisper.cpp version is compatible 5. Debug
cryptic errors if versions don’t align</p>
<p><strong>GGUF Era Workflow</strong>: 1. Fine-tune model 2. Convert to
GGUF <code>.gguf</code> 3. Metadata automatically embedded 4. Target
device validates compatibility automatically 5. Clear error messages if
there are issues</p>
<h3 id="for-deployment">For Deployment</h3>
<p><strong>Benefits in Production</strong>: - Easier model versioning
and rollback - Better debugging when issues occur - Simplified model
management in multi-model systems - More reliable cross-platform
deployment</p>
<h2 id="recommendations">Recommendations</h2>
<h3 id="for-fine-tuning-projects">For Fine-Tuning Projects</h3>
<p>✅ <strong>Use GGUF</strong> for all new Whisper fine-tuning projects
targeting edge/CPU deployment</p>
<h3 id="for-edge-deployment">For Edge Deployment</h3>
<p>✅ <strong>Migrate to GGUF</strong> if your whisper.cpp version
supports it (most versions since mid-2023)</p>
<h3 id="for-mobileembedded">For Mobile/Embedded</h3>
<p>✅ <strong>GGUF</strong> provides better long-term maintainability,
even if initial setup seems similar</p>
<h3 id="for-legacy-systems">For Legacy Systems</h3>
<p>⚠️ <strong>GGML</strong> may still be necessary for very old
deployment targets, but plan migration</p>
<h2 id="key-takeaway">Key Takeaway</h2>
<p><strong>GGUF is not a different inference engine or a performance
upgrade</strong>—it’s a better packaging format for the same underlying
quantized model technology. Think of it as upgrading from a ZIP file
with a separate README to a self-documenting archive that validates
itself when opened.</p>
<p>For Whisper fine-tuning projects targeting CPU/edge deployment,
<strong>always prefer GGUF unless you have a specific reason to use the
legacy GGML format</strong>.</p>
<h1 id="part-ix-vocabulary-language">Part IX: Vocabulary &amp;
Language</h1>
<p><em>Vocabulary recognition and language considerations</em></p>
<hr />
<h2 id="target-vocabulary-in-training-data">Target Vocabulary In
Training Data</h2>
<h2 id="the-question">The Question</h2>
<p>When recording training data for ASR fine-tuning that includes target
foreign vocabulary (e.g., Hebrew words in English sentences), each
training sample contains:</p>
<ul>
<li><strong>Known words:</strong> Words the model already handles well
(“I’m going to the”)</li>
<li><strong>Target vocabulary:</strong> The new word you want to teach
(“Mekolet” - Hebrew for convenience store)</li>
</ul>
<p><strong>Does the model need to “learn” the entire sentence, or just
the target vocabulary? Should you minimize surrounding context to
increase the signal-to-noise ratio for learning?</strong></p>
<h2 id="short-answer">Short Answer</h2>
<p><strong>No, you should NOT minimize surrounding context.</strong> The
fine-tuning process naturally handles this, and surrounding context
actually <em>improves</em> learning of target vocabulary through:</p>
<ol type="1">
<li>Co-articulation effects (how sounds blend between words)</li>
<li>Contextual embeddings</li>
<li>Statistical regularization</li>
</ol>
<p>Include natural, varied sentences around your target vocabulary—this
helps, not hurts.</p>
<h2 id="how-asr-models-process-training-data">How ASR Models Process
Training Data</h2>
<h3 id="sequence-to-sequence-learning">Sequence-to-Sequence
Learning</h3>
<p>Whisper and similar models use a sequence-to-sequence
architecture:</p>
<pre><code>Audio sequence → Encoder → Context representation → Decoder → Text sequence</code></pre>
<p>During fine-tuning, the model learns:</p>
<ol type="1">
<li><strong>Acoustic patterns:</strong> What does “Mekolet”
<em>sound</em> like?</li>
<li><strong>Phonetic structure:</strong> What phonemes compose it?</li>
<li><strong>Contextual usage:</strong> Where does it appear in
sentences?</li>
<li><strong>Transitions:</strong> How do surrounding words affect its
pronunciation?</li>
</ol>
<h3 id="gradient-based-learning">Gradient-Based Learning</h3>
<p>The loss function compares predicted text to ground truth
<strong>across the entire sequence</strong>:</p>
<pre><code>Loss = sum of prediction errors for each token in the output</code></pre>
<p>However, the <strong>gradient magnitude</strong> (how much the model
adjusts) is automatically higher for tokens where the prediction error
is larger:</p>
<ul>
<li>Words already known well (e.g., “going”, “the”) → Small prediction
error → Small gradient → Minimal learning</li>
<li>Unknown words (e.g., “Mekolet”) → Large prediction error → Large
gradient → Significant learning</li>
</ul>
<p><strong>The model automatically focuses learning where it’s needed
most.</strong> You don’t need to manually increase the signal-to-noise
ratio by removing context.</p>
<h2 id="why-surrounding-context-helps-learning">Why Surrounding Context
Helps Learning</h2>
<h3 id="co-articulation-effects">1. <strong>Co-Articulation
Effects</strong></h3>
<p>Speech is not discrete—sounds blend between words:</p>
<pre><code>&quot;I&#39;m going to the Mekolet&quot;
                  ↓
Pronunciation of &quot;the&quot; affected by following &quot;M&quot;
Pronunciation of &quot;Me-&quot; affected by preceding &quot;the&quot;</code></pre>
<p>If you trained only on isolated “Mekolet” pronunciations, the model
would learn:</p>
<ul>
<li>How “Mekolet” sounds in isolation</li>
<li>But NOT how it sounds after “the”</li>
<li>Or after “to the”</li>
<li>Or how native speakers phonetically reduce preceding words</li>
</ul>
<p><strong>Natural sentence context teaches the model real-world
pronunciation patterns.</strong></p>
<h3 id="contextual-embeddings">2. <strong>Contextual
Embeddings</strong></h3>
<p>Modern transformer-based models use contextual embeddings—the
representation of “Mekolet” is different depending on surrounding
words:</p>
<pre><code>&quot;I&#39;m going to the Mekolet&quot; → Embedding_A for &quot;Mekolet&quot;
&quot;Meet me at Mekolet&quot; → Embedding_B for &quot;Mekolet&quot;</code></pre>
<p>This context helps the model:</p>
<ul>
<li>Disambiguate similar-sounding words</li>
<li>Understand typical usage patterns</li>
<li>Build more robust representations</li>
</ul>
<p><strong>Varied contexts create richer, more generalizable
learning.</strong></p>
<h3 id="statistical-regularization">3. <strong>Statistical
Regularization</strong></h3>
<p>When the model sees:</p>
<pre><code>&quot;I&#39;m going to the Mekolet&quot;
&quot;We stopped at the Mekolet&quot;
&quot;The Mekolet sells groceries&quot;</code></pre>
<p>The <strong>consistent presence of known words</strong> acts as an
anchor:</p>
<ul>
<li>The model is confident about “I’m going to the”</li>
<li>This confidence constrains the solution space for “Mekolet”</li>
<li>Prevents overfitting to spurious patterns</li>
</ul>
<p><strong>Context provides statistical scaffolding that guides
learning.</strong></p>
<h3 id="language-model-priors">4. <strong>Language Model
Priors</strong></h3>
<p>Whisper includes a language model component that predicts likely next
words. During fine-tuning:</p>
<ul>
<li>It learns: “after <em>to the</em>, <em>Mekolet</em> is a plausible
next word”</li>
<li>It learns: <em>Mekolet</em> appears in similar contexts as “store”,
“market”, “shop”</li>
<li>This helps during inference with partial/noisy audio</li>
</ul>
<p><strong>Context teaches the model <em>when</em> to predict your
target vocabulary.</strong></p>
<h2 id="the-isolated-vocabulary-experiment">The “Isolated Vocabulary”
Experiment</h2>
<p>What if you <strong>only</strong> trained on isolated target
words?</p>
<h3 id="approach-a-isolated-words-only">Approach A: Isolated words
only</h3>
<pre><code>Training data:
- &quot;Mekolet&quot; (1 second)
- &quot;Mekolet&quot; (1 second)
- &quot;Mekolet&quot; (1 second)
× 100 samples</code></pre>
<p><strong>Problems:</strong></p>
<ol type="1">
<li><strong>Overfitting:</strong> Model memorizes the specific recording
conditions</li>
<li><strong>Poor generalization:</strong> Doesn’t learn how “Mekolet”
sounds in natural speech</li>
<li><strong>No co-articulation:</strong> Fails when preceded/followed by
other words</li>
<li><strong>Catastrophic forgetting:</strong> May <em>degrade</em>
performance on other words because loss function doesn’t reinforce
them</li>
</ol>
<h3 id="approach-b-natural-sentences-recommended">Approach B: Natural
sentences (recommended)</h3>
<pre><code>Training data:
- &quot;I&#39;m going to the Mekolet&quot;
- &quot;The Mekolet is closed today&quot;
- &quot;She works at the Mekolet&quot;
× 33 samples (same total audio length)</code></pre>
<p><strong>Benefits:</strong></p>
<ol type="1">
<li><strong>Natural co-articulation:</strong> Learns real pronunciation
patterns</li>
<li><strong>Contextual learning:</strong> Understands typical usage</li>
<li><strong>No catastrophic forgetting:</strong> Reinforces known words
too</li>
<li><strong>Better generalization:</strong> More robust to
variations</li>
</ol>
<p><strong>Empirical evidence:</strong> Approach B consistently
outperforms Approach A in ASR fine-tuning.</p>
<h2 id="best-practices-for-training-data-with-target-vocabulary">Best
Practices for Training Data with Target Vocabulary</h2>
<h3 id="use-natural-sentences">1. <strong>Use Natural
Sentences</strong></h3>
<p>✓ “I’m going to the Mekolet to buy milk” ✗ “Mekolet” ✗ “The Mekolet
Mekolet Mekolet”</p>
<h3 id="vary-the-context">2. <strong>Vary the Context</strong></h3>
<p>Include target word in different sentence positions:</p>
<ul>
<li>Beginning: “Mekolet is my favorite store”</li>
<li>Middle: “I shop at the Mekolet daily”</li>
<li>End: “Let’s meet at the Mekolet”</li>
</ul>
<p>Include different preceding/following words:</p>
<ul>
<li>“…to the Mekolet”</li>
<li>“…at the Mekolet”</li>
<li>“…from the Mekolet”</li>
</ul>
<h3 id="balance-target-density">3. <strong>Balance Target
Density</strong></h3>
<p><strong>Good ratio:</strong> 1-3 target words per 10-15 word
sentence</p>
<p>✓ “I’m going to the Mekolet to buy milk” (1 target / 9 words = 11%) ✗
“Mekolet Mekolet Mekolet Mekolet” (4 targets / 4 words = 100%) ✗ “I’m
going to the store today to buy groceries and then heading home” (0
targets / 14 words = 0%)</p>
<h3 id="include-prosodic-variation">4. <strong>Include Prosodic
Variation</strong></h3>
<p>Record with different:</p>
<ul>
<li>Speaking speeds (normal, fast, slow)</li>
<li>Emphasis patterns (“I’m going to the <strong>MEKOLET</strong>”
vs. “I’m <strong>going</strong> to the Mekolet”)</li>
<li>Emotional tone (neutral, excited, tired)</li>
</ul>
<h3 id="dont-artificially-isolate">5. <strong>Don’t Artificially
Isolate</strong></h3>
<p>✗ Don’t insert unnatural pauses: “I’m going to the … MEKOLET” ✓ Speak
naturally: “I’m going to the Mekolet”</p>
<h3 id="quantity-target-words-vs.-total-words">6. <strong>Quantity:
Target Words vs. Total Words</strong></h3>
<p>For effective learning, you need:</p>
<ul>
<li><strong>Absolute target word instances:</strong> 50-100+ instances
of each target word</li>
<li><strong>Total training data:</strong> 30-60 minutes typical for
few-word fine-tuning</li>
</ul>
<p><strong>Example for 10 target words:</strong></p>
<ul>
<li>10 words × 70 instances each = 700 target word instances</li>
<li>In natural sentences (10% density) = 7,000 total words</li>
<li>At ~2 words/second = ~3,500 seconds = ~60 minutes of speech</li>
</ul>
<p>This provides both sufficient target word exposure AND enough
surrounding context.</p>
<h2 id="the-fine-tuning-loss-function-in-practice">The Fine-Tuning Loss
Function in Practice</h2>
<p>Here’s conceptually how the model learns from:</p>
<pre><code>Ground truth: &quot;I&#39;m going to the Mekolet&quot;
Prediction:   &quot;I&#39;m going to the [???]&quot;</code></pre>
<p>Loss computation (simplified):</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">&quot;I&#39;m&quot;</span>, <span class="st">&quot;going&quot;</span>, <span class="st">&quot;to&quot;</span>, <span class="st">&quot;the&quot;</span>, <span class="st">&quot;Mekolet&quot;</span>]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>predicted_probs <span class="op">=</span> model.predict(audio)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    error <span class="op">=</span> <span class="op">-</span>log(predicted_probs[i][token])</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">+=</span> error</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p><strong>The model’s attention automatically focuses on
errors.</strong> Including known words doesn’t dilute learning—it
stabilizes it.</p>
<h2 id="special-case-very-limited-data">Special Case: Very Limited
Data</h2>
<p>If you have <strong>extremely limited data</strong> (&lt; 20 minutes
total), you might consider:</p>
<ol type="1">
<li><strong>Slightly higher target density:</strong> 15-20% instead of
10%</li>
<li><strong>Shorter surrounding sentences:</strong> “Go to the Mekolet”
vs. “I think we should go to the Mekolet tomorrow”</li>
<li><strong>But still include natural context:</strong> Never train on
isolated words</li>
</ol>
<p>Even with limited data, context helps more than it hurts.</p>
<h2 id="what-about-data-augmentation">What About Data Augmentation?</h2>
<p>Rather than removing context, consider <strong>augmenting</strong>
your target vocabulary training:</p>
<h3 id="effective-augmentation">Effective augmentation:</h3>
<ol type="1">
<li><strong>Pitch shifting:</strong> Simulate different speakers</li>
<li><strong>Speed variation:</strong> 0.9x - 1.1x playback speed</li>
<li><strong>Background noise:</strong> Add realistic noise at low
levels</li>
<li><strong>Room reverb:</strong> Simulate different recording
environments</li>
</ol>
<p>These help the model generalize without sacrificing contextual
learning.</p>
<h3 id="ineffective-augmentation">Ineffective augmentation:</h3>
<p>✗ Cutting sentences to isolate target words ✗ Repeating target words
artificially ✗ Removing surrounding words</p>
<h2 id="monitoring-during-fine-tuning">Monitoring During
Fine-Tuning</h2>
<p>You can verify this behavior during training:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>Known word losses:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;I&#39;m&quot;</span>:     <span class="fl">0.01</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;going&quot;</span>:   <span class="fl">0.02</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;to&quot;</span>:      <span class="fl">0.01</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;the&quot;</span>:     <span class="fl">0.02</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>Target word losses:</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">&quot;Mekolet&quot;</span>: <span class="fl">3.45</span>  ← much higher</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p>If you see all losses roughly equal, something is wrong with your
training setup.</p>
<h2 id="conclusion-1">Conclusion</h2>
<p><strong>You should NOT minimize surrounding context in your training
data.</strong> The fine-tuning process naturally handles the
signal-to-noise issue through gradient-based learning that automatically
focuses on high-error tokens (your target vocabulary).</p>
<p><strong>Surrounding context actively helps by:</strong></p>
<ol type="1">
<li>Teaching natural co-articulation patterns</li>
<li>Providing contextual embeddings</li>
<li>Offering statistical regularization</li>
<li>Building language model priors</li>
<li>Preventing catastrophic forgetting</li>
</ol>
<p><strong>Best practice:</strong> Record natural sentences with 10-15%
target vocabulary density, varied contexts, and natural prosody. Let the
model’s learning algorithm handle the focus—it’s designed for exactly
this scenario.</p>
<p>The intuition that “more signal, less noise” would help is
understandable but misapplies concepts from traditional signal
processing. In neural network training, context <em>is</em> signal, not
noise.</p>
<hr />
<p><em>Note: This document was generated by Claude Code, an AI
assistant. Please validate technical details and test recommendations in
your specific environment before implementing.</em></p>
<h2 id="vocabulary-recognition-asr-training-data">Vocabulary Recognition
Asr Training Data</h2>
<h2 id="question-1">Question</h2>
<p>Why does Whisper recognize some Hebrew words (like “Shabbat”) but not
others (like “Macolet”)? Does ASR work at the word level or phonetic
level, and how does training data determine which words make it into the
model?</p>
<h2 id="answer-1">Answer</h2>
<p>Excellent question that touches on fundamental ASR mechanics. Let’s
break down how vocabulary recognition works in models like Whisper.</p>
<hr />
<h2 id="how-asr-models-handle-vocabulary">How ASR Models Handle
Vocabulary</h2>
<h3 id="the-short-answer"><strong>The Short Answer:</strong></h3>
<p><strong>ASR models work at the subword/phoneme level, not the word
level.</strong></p>
<p><strong>Whether a word is recognized depends on:</strong> 1.
<strong>Frequency in training data</strong> (how often it appeared) 2.
<strong>Phonetic similarity to known words</strong> 3.
<strong>Tokenization strategy</strong> (how the model breaks down
sounds) 4. <strong>Language mode</strong> (English vs. Hebrew
vs. multilingual)</p>
<p><strong>Your observation is spot-on:</strong> “Shabbat” is common
enough in English-language audio (especially in North America/UK
content) to be well-represented, while “Macolet” (מכולת) is
Israeli-specific and rare in international English audio.</p>
<hr />
<h2 id="how-whisper-and-modern-asr-works">How Whisper (and Modern ASR)
Works</h2>
<h3 id="phonetic-level-subword-tokens-words"><strong>Phonetic Level →
Subword Tokens → Words</strong></h3>
<p><strong>Architecture Overview:</strong></p>
<pre><code>Audio → Mel-spectrogram → Encoder → Decoder → Subword tokens → Words</code></pre>
<p><strong>Key Insight: Whisper doesn’t have a “vocabulary” like a
dictionary.</strong></p>
<p>Instead: 1. <strong>Audio encoding</strong>: Convert sound waves →
spectral features 2. <strong>Sequence modeling</strong>: Encoder learns
phonetic patterns 3. <strong>Token prediction</strong>: Decoder predicts
subword tokens (BPE - Byte-Pair Encoding) 4. <strong>Token →
Text</strong>: Subword tokens combine into words</p>
<hr />
<h3 id="byte-pair-encoding-bpe-tokenization"><strong>Byte-Pair Encoding
(BPE) Tokenization</strong></h3>
<p><strong>What is BPE?</strong> - Breaks words into frequent subword
units - Common subwords become single tokens - Rare words are split into
smaller pieces</p>
<p><strong>Example:</strong></p>
<pre><code>Common word: &quot;hello&quot; → [hello]  (single token)
Rare word: &quot;Macolet&quot; → [Mac][ol][et]  (multiple tokens)</code></pre>
<p><strong>Whisper’s tokenizer has ~50,000 tokens</strong>: - Common
English words: Single tokens - Common names/terms: Single tokens - Rare
words: Split into subwords</p>
<p><strong>Why This Matters:</strong> If “Shabbat” appears frequently in
training data, it becomes a <strong>single token</strong> in Whisper’s
vocabulary. If “Macolet” doesn’t, it must be constructed from
<strong>phonetic subword tokens</strong>—and this is where errors
happen.</p>
<hr />
<h2 id="why-shabbat-works-but-macolet-doesnt">Why “Shabbat” Works But
“Macolet” Doesn’t</h2>
<h3 id="case-study-shabbat"><strong>Case Study: “Shabbat”</strong></h3>
<p><strong>Frequency in Training Data:</strong> - Whisper trained on
680,000 hours of audio - Sources include: - YouTube subtitles
(religious/cultural content) - Podcasts (Jewish topics, interfaith
discussions) - TV shows/movies (Jewish characters, cultural references)
- News (stories about Israel, Judaism)</p>
<p><strong>“Shabbat” appears in:</strong> - Religious content (sermons,
lectures) - Cultural programming (food shows, travel vlogs) - Mainstream
media (discussions of Jewish holidays)</p>
<p><strong>Result:</strong> - <strong>High frequency</strong> → BPE
tokenizer creates a token <code>[Shabbat]</code> - Whisper learns
acoustic patterns for “Shabbat” - Decoder predicts
<code>[Shabbat]</code> token confidently</p>
<p><strong>Transcription: ✅ “Shabbat”</strong> (correct)</p>
<hr />
<h3 id="case-study-macolet-מכולת"><strong>Case Study: “Macolet”
(מכולת)</strong></h3>
<p><strong>Frequency in Training Data:</strong> - “Macolet” (or
“Makolet”) is <strong>Israeli-specific slang</strong> - Rarely used in
English-language media - Not commonly in international English audio -
Whisper’s training data skews toward: - North American English - British
English - International content (but not hyper-local terms)</p>
<p><strong>Result:</strong> - <strong>Low/zero frequency</strong> → No
<code>[Macolet]</code> token - Whisper must construct from phonetic
subwords - Decoder guesses: <code>[Mac][ol][et]</code> or similar -
Acoustically similar words interfere (e.g., “makeup lot”, “mackerel”,
“macho let”)</p>
<p><strong>Transcription: ❌ “Makeup lot” / “Maco late” /
gibberish</strong> (incorrect)</p>
<hr />
<h2 id="the-phonetic-level-why-errors-happen">The Phonetic Level: Why
Errors Happen</h2>
<h3 id="how-whisper-hears-unknown-words"><strong>How Whisper “Hears”
Unknown Words</strong></h3>
<p>When you say “Macolet” (<code>/ma-ko-let/</code>):</p>
<ol type="1">
<li><strong>Acoustic encoding</strong>: Whisper converts sound →
spectral features
<ul>
<li>Recognizes phonemes: <code>/m/</code>, <code>/a/</code>,
<code>/k/</code>, <code>/o/</code>, <code>/l/</code>, <code>/e/</code>,
<code>/t/</code></li>
</ul></li>
<li><strong>Decoder prediction</strong>: Tries to match phonemes to
known tokens
<ul>
<li>Searches for tokens that match <code>/ma-ko-let/</code>
acoustically</li>
<li>Finds partial matches:
<ul>
<li>“Mac” (common prefix: Macintosh, McDonald’s)</li>
<li>“lot” (common word)</li>
<li>“late” (common word)</li>
</ul></li>
</ul></li>
<li><strong>Decoder outputs best guess</strong>:
<ul>
<li>“Mac lot” (if it parses as two words)</li>
<li>“Macolate” (if it tries to keep as one word)</li>
<li>“Macaulay” (if it finds a similar name)</li>
</ul></li>
</ol>
<p><strong>The problem:</strong> Without seeing “Macolet” in training,
Whisper has no prior to favor the correct spelling.</p>
<hr />
<h2 id="training-data-determines-recognition">Training Data Determines
Recognition</h2>
<h3 id="the-rule"><strong>The Rule:</strong></h3>
<p><strong>If a word appears frequently enough in training data, it will
be recognized reliably.</strong></p>
<p><strong>“Frequently enough” depends on:</strong> - <strong>Raw
count</strong>: How many times it appears - <strong>Acoustic
variability</strong>: Different speakers, accents, contexts -
<strong>Context</strong>: Surrounding words that help disambiguation</p>
<p><strong>Thresholds (Rough Estimates):</strong></p>
<pre><code>&gt;10,000 occurrences: Very likely to be a single token → reliable recognition
1,000-10,000: May be a token or common subword sequence → good recognition
100-1,000: Likely subword split → moderate recognition (context-dependent)
&lt;100: Definitely subword split → poor recognition (often fails)</code></pre>
<p><strong>“Shabbat”</strong>: Likely 10,000+ occurrences in Whisper’s
training data <strong>“Macolet”</strong>: Likely &lt;10 occurrences (if
any)</p>
<hr />
<h2 id="language-mode-and-code-switching">Language Mode and
Code-Switching</h2>
<h3 id="your-use-case-english-hebrew-words"><strong>Your Use Case:
English + Hebrew Words</strong></h3>
<p><strong>Whisper’s multilingual model has language
detection:</strong></p>
<pre><code>Audio → Language detection → Decoder (language-specific mode)</code></pre>
<p><strong>What happens when you speak English with Hebrew
words:</strong></p>
<p><strong>Option 1: Whisper detects English</strong> - Decoder uses
English tokens - Hebrew words must map to English phonetics - Result:
Hebrew words often mis-transcribed</p>
<p><strong>Option 2: Whisper detects Hebrew</strong> - Decoder uses
Hebrew tokens - English words must map to Hebrew phonetics - Result:
English words may be transliterated incorrectly</p>
<p><strong>Option 3: Whisper code-switches (rare)</strong> - Decoder
flips between English and Hebrew tokens - Can work if the model learned
this pattern - But Whisper wasn’t explicitly trained for
code-switching</p>
<p><strong>Your experience:</strong> - When you say “I need to go to the
Macolet,” Whisper stays in English mode - “Macolet” has no English token
→ phonetic guessing → error</p>
<hr />
<h2 id="fine-tuning-to-fix-this">Fine-Tuning to Fix This</h2>
<h3 id="how-fine-tuning-helps"><strong>How Fine-Tuning
Helps:</strong></h3>
<p><strong>Your fine-tuning data:</strong></p>
<pre><code>Audio: &quot;I&#39;m going to the Macolet to buy milk&quot;
Text: &quot;I&#39;m going to the Macolet to buy milk&quot;</code></pre>
<p><strong>What the model learns:</strong> 1. <strong>Phonetic
pattern</strong>: <code>/ma-ko-let/</code> → “Macolet” (consistent
mapping) 2. <strong>Context</strong>: “Macolet” appears after “the”
(like “the store”, “the shop”) 3. <strong>Frequency</strong>: If you
provide 50-100 examples, “Macolet” becomes a learned pattern</p>
<p><strong>Post-fine-tuning:</strong> - Whisper’s decoder learns to
output “Macolet” when it hears <code>/ma-ko-let/</code> - Even if
“Macolet” isn’t a single token, the model learns the subword sequence -
Context helps (e.g., “going to the [Macolet]” vs. “Mac” + “lot”)</p>
<p><strong>Result: ✅ Reliable transcription of “Macolet”</strong></p>
<hr />
<h2 id="vocabulary-expansion-strategies">Vocabulary Expansion
Strategies</h2>
<h3 id="fine-tuning-your-best-option"><strong>1. Fine-Tuning (Your Best
Option)</strong></h3>
<p><strong>Data collection:</strong> - Record yourself using Hebrew
words in English sentences - Transcribe with the correct spelling (e.g.,
“Macolet”) - 2-5 hours of audio with these words</p>
<p><strong>Fine-tuning:</strong> - Train Whisper on your data - Model
learns your code-switching patterns - Hebrew words become consistently
transcribed</p>
<p><strong>Benefit:</strong> - Works for ALL your Hebrew words (Macolet,
misrad, etc.) - Learns your pronunciation patterns</p>
<hr />
<h3 id="custom-tokenizer-advanced-not-recommended"><strong>2. Custom
Tokenizer (Advanced, Not Recommended)</strong></h3>
<p><strong>Concept:</strong> - Retrain Whisper’s BPE tokenizer with your
vocabulary - Add “Macolet”, “misrad”, etc. as explicit tokens</p>
<p><strong>Problems:</strong> - Requires retraining the entire model
(not just fine-tuning) - Extremely compute-intensive - Breaks
compatibility with standard Whisper</p>
<p><strong>Not worth it</strong> for your use case.</p>
<hr />
<h3 id="post-processing-spelling-correction"><strong>3. Post-Processing
(Spelling Correction)</strong></h3>
<p><strong>Concept:</strong> - Let Whisper transcribe (“Mac lot”) -
Apply a spell-checker or LLM to fix known errors</p>
<p><strong>Implementation:</strong></p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> faster_whisper <span class="im">import</span> WhisperModel</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> WhisperModel(<span class="st">&quot;medium&quot;</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>segments, info <span class="op">=</span> model.transcribe(<span class="st">&quot;audio.wav&quot;</span>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot; &quot;</span>.join([seg.text <span class="cf">for</span> seg <span class="kw">in</span> segments])</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>corrections <span class="op">=</span> {</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Mac lot&quot;</span>: <span class="st">&quot;Macolet&quot;</span>,</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;miss rod&quot;</span>: <span class="st">&quot;misrad&quot;</span>,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;to that say hoot&quot;</span>: <span class="st">&quot;te&#39;udat zehut&quot;</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> wrong, right <span class="kw">in</span> corrections.items():</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.replace(wrong, right)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span></code></pre></div>
<p><strong>Pros:</strong> - ✅ Works immediately (no training) - ✅ Easy
to implement</p>
<p><strong>Cons:</strong> - ❌ Manual dictionary maintenance - ❌
Fragile (Whisper might transcribe “Mac lot” differently each time) - ❌
Doesn’t generalize (new words need new rules)</p>
<p><strong>Use case:</strong> Temporary fix while preparing fine-tuning
data.</p>
<hr />
<h3 id="promptinjection-whispers-hidden-feature"><strong>4.
Prompt/Injection (Whisper’s Hidden Feature)</strong></h3>
<p><strong>Whisper supports “initial prompt”</strong> (hint to the
decoder):</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.transcribe(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;audio.wav&quot;</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    initial_prompt<span class="op">=</span><span class="st">&quot;Common Hebrew words: Macolet, misrad, te&#39;udat zehut, Shabbat&quot;</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>How it works:</strong> - Decoder sees these words as context
- Slightly biases output toward these spellings</p>
<p><strong>Effectiveness:</strong> - Modest improvement (not a silver
bullet) - Works best for words that are phonetically close to
transcription errors - Doesn’t add new tokens, just biases existing
ones</p>
<p><strong>Worth trying</strong> as a quick test!</p>
<hr />
<h2 id="linguistic-origin-vs.-training-data">Linguistic Origin
vs. Training Data</h2>
<h3 id="your-question-does-linguistic-origin-matter"><strong>Your
Question: Does Linguistic Origin Matter?</strong></h3>
<p><strong>Short answer: No, training data matters.</strong></p>
<p><strong>Examples:</strong></p>
<table>
<thead>
<tr class="header">
<th>Word</th>
<th>Origin</th>
<th>Whisper Recognition</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>“Shabbat”</td>
<td>Hebrew</td>
<td>✅ Good</td>
<td>High frequency in English audio</td>
</tr>
<tr class="even">
<td>“Macolet”</td>
<td>Hebrew</td>
<td>❌ Poor</td>
<td>Rare in English audio</td>
</tr>
<tr class="odd">
<td>“Schadenfreude”</td>
<td>German</td>
<td>✅ Good</td>
<td>Common in English discourse</td>
</tr>
<tr class="even">
<td>“Fernweh”</td>
<td>German</td>
<td>❌ Poor</td>
<td>Rare in English discourse</td>
</tr>
<tr class="odd">
<td>“Sushi”</td>
<td>Japanese</td>
<td>✅ Excellent</td>
<td>Ubiquitous in English</td>
</tr>
<tr class="even">
<td>“Omakase”</td>
<td>Japanese</td>
<td>⚠️ Mixed</td>
<td>Growing but not universal</td>
</tr>
</tbody>
</table>
<p><strong>What determines recognition:</strong> 1.
<strong>Frequency</strong> in English-language audio (not the word’s
origin) 2. <strong>Cultural integration</strong> (how much the word is
used in English contexts) 3. <strong>Media representation</strong> (how
often it appears in Whisper’s training sources)</p>
<p><strong>Hebrew words in English:</strong> - “Shabbat”, “kosher”,
“Hanukkah” → ✅ Well-known, high frequency - “Macolet”, “misrad”,
“te’udat zehut” → ❌ Israeli-specific, low frequency</p>
<hr />
<h2 id="summary-why-variance-exists">Summary: Why Variance Exists</h2>
<p><strong>Your observation:</strong> &gt; “I encounter variance in what
I find [Whisper recognizing]”</p>
<p><strong>Explanation:</strong></p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="header">
<th>Factor</th>
<th>“Shabbat” (Works)</th>
<th>“Macolet” (Fails)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Training data frequency</strong></td>
<td>High (10k+ examples)</td>
<td>Low/Zero (&lt;10 examples)</td>
</tr>
<tr class="even">
<td><strong>BPE tokenization</strong></td>
<td>Single token <code>[Shabbat]</code></td>
<td>Subword split <code>[Mac][ol][et]</code></td>
</tr>
<tr class="odd">
<td><strong>Phonetic ambiguity</strong></td>
<td>Low (distinct sound)</td>
<td>High (sounds like “Mac lot”)</td>
</tr>
<tr class="even">
<td><strong>Cultural integration</strong></td>
<td>International Jewish culture</td>
<td>Israeli-specific slang</td>
</tr>
<tr class="odd">
<td><strong>Media representation</strong></td>
<td>YouTube, podcasts, TV</td>
<td>Rare outside Israel</td>
</tr>
</tbody>
</table>
<p><strong>The variance is entirely due to training data distribution,
not linguistic origin.</strong></p>
<hr />
<h2 id="practical-recommendations-for-you">Practical Recommendations for
You</h2>
<h3 id="option-1-fine-tune-best-long-term"><strong>Option 1: Fine-Tune
(Best Long-Term)</strong></h3>
<p>Collect 2-5 hours of your speech with Hebrew words, transcribe
carefully, fine-tune Whisper.</p>
<p><strong>Result:</strong> All your Hebrew words (Macolet, misrad,
etc.) recognized correctly.</p>
<h3 id="option-2-initial-prompt-quick-test"><strong>Option 2: Initial
Prompt (Quick Test)</strong></h3>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.transcribe(</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;audio.wav&quot;</span>,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    initial_prompt<span class="op">=</span><span class="st">&quot;Hebrew words used: Macolet (convenience store), misrad (office), te&#39;udat zehut (ID card)&quot;</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>Result:</strong> Modest improvement (worth trying).</p>
<h3 id="option-3-post-processing-interim-fix"><strong>Option 3:
Post-Processing (Interim Fix)</strong></h3>
<p>Maintain a dictionary of corrections, apply after transcription.</p>
<p><strong>Result:</strong> Works but fragile.</p>
<h3 id="recommended-path"><strong>Recommended Path:</strong></h3>
<ol type="1">
<li><strong>Now:</strong> Use initial prompt + post-processing</li>
<li><strong>Short-term:</strong> Collect audio data with Hebrew
words</li>
<li><strong>Long-term:</strong> Fine-tune Whisper (or wait for a
Hebrew-English code-switching dataset to fine-tune on)</li>
</ol>
<hr />
<h2 id="bottom-line">Bottom Line</h2>
<p><strong>ASR works at the phonetic/subword level, but vocabulary
recognition is driven by training data frequency.</strong></p>
<ul>
<li><strong>“Shabbat” works</strong>: High frequency in Whisper’s
training data (English-language audio with Jewish cultural content)</li>
<li><strong>“Macolet” fails</strong>: Low/zero frequency
(Israeli-specific, rare outside Israel)</li>
</ul>
<p><strong>Fine-tuning is the solution</strong>: By providing examples
of your Hebrew words in English contexts, you teach Whisper to recognize
them reliably.</p>
<p><strong>This is exactly the use case where personal fine-tuning
shines.</strong></p>
<hr />
<p><strong>Note</strong>: This explanation was generated by Claude Code
(claude-sonnet-4-5) for Daniel Rosehill’s STT Fine-Tuning Notebook.
Whisper’s vocabulary recognition is probabilistic and depends on
training data distribution. For reliable transcription of code-switched
speech (English + Hebrew), fine-tuning is the most effective solution.
Consider creating a dataset of 2-5 hours with Hebrew words you use
regularly, ensuring diverse contexts and pronunciations. Initial prompts
can provide modest improvements as an interim measure.</p>
</body>
</html>
