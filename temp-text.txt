Welcome to today’s episode! Let’s dive into AMD GPU engines for speech-to-text, or ASR . We’ll discuss the best inference engines that are optimized for AMD graphics cards and provide some practical recommendations along the way. First, let’s ask the question: With an AMD GPU, rather than NVIDIA, what are the best inference engines for ASR that have good AMD or ROCm support? And when converting models from a safe-tensors format after fine-tuning, which target formats should you consider for seamless compatibility with AMD GPUs? Running speech-to-text workloads locally on AMD GPUs presents some unique challenges compared to NVIDIA’s more established CUDA ecosystem. In this guide, we will provide a comprehensive comparison of the most reliable engines for AMD GPU-accelerated STT inference. Let's first explore the current state of AMD GPU support for speech-to-text. AMD GPU support for AI workloads primarily relies on ROCm , or Radeon Open Compute. Historically, this has lagged behind NVIDIA’s CUDA in terms of software support and ecosystem maturity. Many popular inference engines were originally built with CUDA as the main target, with AMD support added later or, in some cases, not at all. Now, let’s compare the engines. Starting with **Faster-Whisper**, which comes highly recommended. **Status**: This is considered the most reliable option for AMD GPU acceleration. **Key Details**: It uses the CTranslate2 backend, which has ROCm support. This engine supports AMD GPUs with a range of architectures, including gfx1100, gfx1101, and gfx1030. It offers three to four times faster inference than OpenAI’s Whisper, all while maintaining similar accuracy, and it requires less VRAM than the original Whisper. **Installation** is simple; you can do it with the command: **ROCm Requirements**: It's recommended to have ROCm version five point four or later. Remember, you may need to set the ` HSA_OVERRIDE_GFX_VERSION ` for certain cards, particularly for the RDNA 3 or RX seven thousand series models. **To verify your GPU usage**, you can run the command: watch -n 1 rocm-smi For more details, consider: watch -n 1 'rocm-smi --showuse --showmeminfo vram' **Pros of Faster-Whisper** include a balance of speed, accuracy, and AMD GPU support, active development and community, along with good documentation for setting up ROCm. However, it does come with a couple of **cons**: the initial setup can be somewhat tricky, and there might be compatibility issues with different ROCm versions. Next up is **whisper.cpp**. **Status**: It has mixed AMD GPU support and is generally considered unreliable for production use. **Key Details**: This engine is primarily optimized for CPU use and utilizes advanced instructions like SIMD and AVX. Although it has experimental HIP support, getting it to perform well on AMD GPUs can be a challenge since it often necessitates compiling from the source with specific flags. Many users find they experience problems with GPU acceleration not engaging properly. Now, you might wonder why your GPU monitoring shows no activity. The app you’re using likely falls into one of these categories: it may be using a pre-compiled binary without ROCm support, reverting to CPU when GPU initialization fails, or not having the right ROCm runtime environment configured. When should you consider using it? Whisper.cpp excels with CPU-only inference, especially for embedded and edge devices where you need minimal dependencies. It has some pros, like outstanding CPU performance and a low memory footprint. But keep in mind its cons include unreliable GPU support and it often falling back to CPU without warning. Then, there’s the original **OpenAI Whisper**. **Status**: There is no direct AMD GPU support through PyTorch. **Key Details**: This version is built on PyTorch using a CUDA backend. While PyTorch has some experimental ROCm support, the original Whisper tends to be slower than optimized alternatives and requires more VRAM. If you still want to try it, the installation for ROCm PyTorch would look like this: **Pros**: It serves as a reliable reference implementation and offers high accuracy. However, be wary of the downsides: it’s the slowest option for inference, ROCm support can be inconsistent, and it uses a lot of VRAM. Next, we have **Whisper-JAX**. **Status**: It has limited AMD support, primarily through experimental ROCm JAX builds. This engine is built on the JAX framework, which is designed for TPU and CUDA primarily. So, unless you have specific expertise with JAX and ROCm, it's best to avoid using this for production workloads on AMD GPUs. Now let’s talk about **Whisper.onnx** and the ONNX Runtime. **Status**: This option shows growing AMD GPU support through DirectML and ROCm. **Key Details**: ONNX Runtime has a ROCm execution provider and can convert Whisper models to ONNX format. However, model conversion is necessary, and this can be more complex than some users might prefer. To install, you can use the command: **Pros**: It’s cross-platform and offers good optimization potential. But it does come with some caveats, including the need for model conversion and potentially less mature ROCm provider compared to CUDA. Now, let’s rank these engines for AMD GPU users! For production-ready choices, we recommend **Faster-Whisper** for its reliability, performance, and setup ease. In the second tier, **OpenAI Whisper with ROCm PyTorch** is workable, yet it has caveats. The ONNX Runtime, while promising, fits here too for specific use cases. Finally, in tier three, we don’t recommend **whisper.cpp** due to its CPU-focused approach and unreliable GPU support, nor **Whisper-JAX**, with its limited compatibility. So, what do you do if your application isn’t showing GPU activity? First, the app you’re using might be employing a CPU-only version. Often, pre-packaged apps fail to include ROCm-compiled versions. Here’s an action plan to consider: **Option A**: Switch to Faster-Whisper. It’s recommended! Install the proper ROCm version and use the following commands to test it out. **Option B**: Verify if your application supports custom builds of whisper.cpp and compile it with ROCm support. **Option C**: If you need something for research development, consider using PyTorch with the original Whisper for a balanced approach. When verifying GPU usage, you can use the ROCm System Management Interface commands to monitor. For detailed GPU usage, tools like `radeontop` can provide insights. Now let’s wrap this up. In conclusion, for AMD GPU users looking to run local speech-to-text workloads, **Faster-Whisper** is currently your best bet. It provides proven support, excellent performance, and is actively being improved. The issues with whisper.cpp are not uncommon, so unless you specifically need that engine, transitioning to Faster-Whisper is your path to better AMD GPU utilization and performance. Thanks for tuning into today’s podcast on AMD GPU engines for speech-to-text! We hope you found this information valuable. Happy coding, and catch you in the next episode!