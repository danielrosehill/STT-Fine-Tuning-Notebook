Something that I've wondered with ASR without talking about fine-tuning is when we talk about sort of what's considered specialist vocabulary and not-specialist vocabulary, living in Israel, I use some words in Hebrew and some words are sort of integrated into the lexicon of English to an extent, like Shabbat is reasonably well known. And there's other words that you probably wouldn't use if you didn't live here, like Macolet, which means a sort of quickie mart, just an example I use. And that isn't in Whisperer when, if I speak that into my standard ASR tool, it's going to not know that word and it'll attempt to transcribe it and often get it laughably wrong. So my question is, I understand that ASR is kind of fundamentally a... doesn't really work at the language level so much as at the fanatic level. And so is it just the case that if a word is commonplace enough that it's going to be in enough training data, regardless of the its linguistic. Origin, it will make it into a model's training. Is that the reason, if I were to say, that I encounter this, uh, variance in, um, in what I find. You.