So, from using a Futo keyboard, an Android app, I've noticed that there is a particular thing around the 30-second mark for transcription, and now that I've begun converting formats, I see that that limitation, if I'm not mistaken, is actually coming from the... Inference format that's being used on phone devices. My question is, for longer form transcription, if you're transcribing, let's say, a note recording, the 30-second thing isn't really that viable. There seem to be some approaches that allow for... Unlimited recording. But even when I go past the 30-second mark, it seems to be, if it's just a chunking approach, that one 30-second chunk is processed automatically after the other, which is how they tend to implement this, seems to be kind of a choppy process that the hardware seems to struggle with when I... just before and after that 30-second limit. For... if I wanted to use a model specifically for note-taking, but really I don't want to use separate ones for voice typing and for what I might call async transcription, where I record a longer note and then send that as a binary after the fact. My question really is, if it's relying on 30-second chunking anyway, does it actually make a meaningful difference to performance, whether I use a front-end tool that supports what's usually called voice typing, that it does it on the fly. Or if I record my file in one go, and then send that in one go for transcription, is it going to be a different processing architecture, and am I going to get different meaningfully different results, depending which approach I use.